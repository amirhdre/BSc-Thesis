{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Implementation (code 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from scipy import signal\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from mne.filter import resample, filter_data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lspopt import spectrogram_lspopt\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n",
    "import logging\n",
    "LOGGING_TYPES = dict(DEBUG=logging.DEBUG, INFO=logging.INFO, WARNING=logging.WARNING,\n",
    "                     ERROR=logging.ERROR, CRITICAL=logging.CRITICAL)\n",
    "logger = logging.getLogger('yasa')\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypno</th>\n",
       "      <th>df_feat</th>\n",
       "      <th>eeg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P18_N3 L</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P18_N3 L.csv</td>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/autoscor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P18_N2 R</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P18_N2 R.csv</td>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/autoscor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P17_N2 L</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P17_N2 L.csv</td>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/autoscor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      hypno  \\\n",
       "name                                                          \n",
       "P18_N3 L  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "P18_N2 R  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "P17_N2 L  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "\n",
       "                       df_feat  \\\n",
       "name                             \n",
       "P18_N3 L  feature/P18_N3 L.csv   \n",
       "P18_N2 R  feature/P18_N2 R.csv   \n",
       "P17_N2 L  feature/P17_N2 L.csv   \n",
       "\n",
       "                                                        eeg  \n",
       "name                                                         \n",
       "P18_N3 L  /Users/amirhosseindaraie/Desktop/data/autoscor...  \n",
       "P18_N2 R  /Users/amirhosseindaraie/Desktop/data/autoscor...  \n",
       "P17_N2 L  /Users/amirhosseindaraie/Desktop/data/autoscor...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load reference_df     \n",
    "reference_df = pd.read_csv(\"reference_df.csv\", index_col=\"name\")\n",
    "reference_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv    \n",
    "rankings_df = pd.read_csv(\"rankings_df aug.csv\", index_col=\"method_name\")\n",
    "rankings_df.head(3)\n",
    "\n",
    "# read top features from file\n",
    "with open('top_features_correlation_90.txt', 'r') as f:\n",
    "    top_feat = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on 60 nights, test of 1 night\n",
    "(1560*60 x 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import kruskal\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[35 50 19 29  0 30 25  9 49 18  7 53 40 58 34 21 10 17 59 57 14 44 15 16\n",
      "  6  1 28  3 31 43 20 37 52 27 11 47 13 36 38  5 42 39 41 60 22 54 32 26\n",
      "  8  2]\n",
      ">>>>>>>> test recordings: \n",
      "[ 4 23 48 55 51 45 24 46 12 33 56]\n"
     ]
    }
   ],
   "source": [
    "idx_all_recordings = np.random.permutation(len(reference_df))\n",
    "idx_train_recordings = idx_all_recordings[:-11]\n",
    "idx_test_recordings = idx_all_recordings[-11:]\n",
    "print(\">>>>>>>> train recordings (index): \")\n",
    "print(idx_train_recordings)\n",
    "print(\">>>>>>>> test recordings: \")\n",
    "print(idx_test_recordings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the dataset to train and test + shuffle each night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[46 14 28 40 44  6 29 27  1 20 53 39 36 22 56  3 57 47 41 30 19 59 50 21\n",
      " 33  9  7 26  8 31 16  2 58 38 15 49 54 10 32 13 55 12 34 51  4 42  0 43\n",
      "  5 37 48 52 35 23 17]\n",
      ">>>>>>>> test recordings: \n",
      "[45 24 18 11 60 25]\n",
      "Train set: X=(108061, 46) y=(108061,)\n",
      "Test set: X=(12211, 46) y=(12211,)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(test_prop=0.2, n_feat=40):\n",
    "\n",
    "    idx_all_recordings = np.random.permutation(len(reference_df))\n",
    "    idx_train_recordings = idx_all_recordings[: -int(test_prop * 61)]\n",
    "    idx_test_recordings = idx_all_recordings[-int(test_prop * 61) :]\n",
    "    print(\">>>>>>>> train recordings (index): \")\n",
    "    print(idx_train_recordings)\n",
    "    print(\">>>>>>>> test recordings: \")\n",
    "    print(idx_test_recordings)\n",
    "\n",
    "    df_feat_X_train = np.array([])\n",
    "    df_feat_X_test = np.array([])\n",
    "    hypno_y_train = np.array([])\n",
    "    hypno_y_test = np.array([])\n",
    "\n",
    "    # columns = rankings_df.columns[:n_feat]  # for selecting top n_feat columns\n",
    "    with open('top_features_correlation_90.txt', 'r') as f:\n",
    "        top_feat = [line.rstrip('\\n') for line in f]\n",
    "    columns = top_feat[:n_feat]\n",
    "\n",
    "    # to loop over all recording files:\n",
    "    for i in idx_train_recordings:\n",
    "        ### to load augmented hypnos for train:\n",
    "        name = reference_df.iloc[i].name\n",
    "        hypno_30s_loc = reference_df.iloc[i].hypno\n",
    "        hypno_30s_loc = hypno_30s_loc.split(\".\")[0] + \" aug.txt\"\n",
    "        hypno_30s = np.loadtxt(hypno_30s_loc, delimiter=\"\\n\")\n",
    "\n",
    "        ### to load features of augmented eeg for train:\n",
    "        df_feat_loc = reference_df.iloc[i].df_feat\n",
    "        df_feat_loc = df_feat_loc.split(\".\")[0] + \" aug.csv\"\n",
    "        df_feat = pd.read_csv(df_feat_loc, index_col=False)\n",
    "\n",
    "        df_feat = df_feat.replace(\n",
    "            [np.inf, -np.inf], 0\n",
    "        )  # Replacing infinite values in features\n",
    "\n",
    "        ### select top n_feat ranks columns\n",
    "        df_feat = df_feat[columns]\n",
    "\n",
    "        ### shuffle X\n",
    "        permut = np.random.permutation(df_feat.shape[0])\n",
    "        df_feat = df_feat.iloc[permut]\n",
    "\n",
    "        ### to load features for train: append df_feat to df_feat_X_train\n",
    "        if i == idx_train_recordings[0]:\n",
    "            df_feat_X_train = df_feat.to_numpy()\n",
    "        else:\n",
    "            df_feat_X_train = np.vstack([df_feat_X_train, df_feat.to_numpy()])\n",
    "\n",
    "        ### shuffle y\n",
    "        hypno_30s = hypno_30s[permut]\n",
    "\n",
    "        ### to load labels for train: append hypno to hypno_y_train\n",
    "        hypno_y_train = np.append(hypno_y_train, hypno_30s)\n",
    "\n",
    "    for i in idx_test_recordings:\n",
    "        ### to load features for test:\n",
    "        df_feat_loc = reference_df.iloc[i].df_feat\n",
    "        df_feat_loc = df_feat_loc.split(\".\")[0] + \" aug.csv\"\n",
    "        df_feat = pd.read_csv(df_feat_loc, index_col=False)\n",
    "\n",
    "        ### to load labels for test:\n",
    "        hypno_30s_loc = reference_df.iloc[i].hypno\n",
    "        hypno_30s_loc = hypno_30s_loc.split(\".\")[0] + \" aug.txt\"\n",
    "        hypno_30s = np.loadtxt(hypno_30s_loc, delimiter=\"\\n\")\n",
    "\n",
    "        df_feat = df_feat.replace(\n",
    "            [np.inf, -np.inf], 0\n",
    "        )  # Replacing infinite values in features\n",
    "\n",
    "        ### select top n_feat ranks columns\n",
    "        df_feat = df_feat[columns].to_numpy()\n",
    "\n",
    "        ### to load features for train: append df_feat to df_feat_X_train\n",
    "        if i == idx_test_recordings[0]:\n",
    "            df_feat_X_test = df_feat\n",
    "        else:\n",
    "            df_feat_X_test = np.vstack([df_feat_X_test, df_feat])\n",
    "\n",
    "        ### to load labels for train: append hypno to hypno_y_train\n",
    "        hypno_y_test = np.append(hypno_y_test, hypno_30s)\n",
    "\n",
    "    print(f\"Train set: X={df_feat_X_train.shape} y={hypno_y_train.shape}\")\n",
    "    print(f\"Test set: X={df_feat_X_test.shape} y={hypno_y_test.shape}\")\n",
    "\n",
    "    ### To standardize all dataset including train and test, after train/test split\n",
    "    # Generate a numpy array including all epochs:\n",
    "    df_feat_all = np.array([])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # we will standardize the columns in dataset before we feed them to a classifier\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(df_feat_X_train)  # first fit all the dataset\n",
    "    X_train_std = sc.transform(df_feat_X_train)  # then transform train\n",
    "    X_test_std = sc.transform(df_feat_X_test)  # and test\n",
    "\n",
    "    return X_train_std, X_test_std, hypno_y_train, hypno_y_test\n",
    "\n",
    "\n",
    "X_train_std, X_test_std, y_train, y_test = train_test_split(0.1, n_feat=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "# svm.fit(X_train_std, y_train)\n",
    "# y_pred = svm.predict(X_test_std)\n",
    "# print(\"Misclassified examples: %d\" % (y_test != y_pred).sum())\n",
    "# print(\"Accuracy: %.3f\" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confmat_f(confmat):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i, j], va=\"center\", ha=\"center\")\n",
    "    ax.set(\n",
    "        xticklabels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"],\n",
    "        xticks=range(5),\n",
    "        yticklabels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"],\n",
    "        yticks=range(5),\n",
    "    )\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.xaxis.labelpad = 15\n",
    "    ax.xaxis.set_tick_params(labeltop=True)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Confusion Matrix\", y=-0.1)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"confmat.png\")\n",
    "    # plt.savefig(\"confmat.svg\")\n",
    "    plt.show()\n",
    "\n",
    "### Using:\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(report)\n",
    "\n",
    "# confmat = confusion_matrix(y_test, y_pred)\n",
    "# confmat_f(confmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with different top feature numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[57 51 52 27 45 54 41 34 10 40 14 32 30  9 17 16  4 29 49 35  8 19 48 15\n",
      " 31 13 23  7 55 42 53  6 50  1  0 44 38  5 56 25 36 39 60 21 11 59 33 18\n",
      " 43  2 22 20 46 58 47]\n",
      ">>>>>>>> test recordings: \n",
      "[37  3 12 26 28 24]\n",
      "Train set: X=(108993, 5) y=(108993,)\n",
      "Test set: X=(11279, 5) y=(11279,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.62      0.65      1486\n",
      "         1.0       0.41      0.15      0.22      1590\n",
      "         2.0       0.64      0.68      0.66      2711\n",
      "         3.0       0.75      0.72      0.74      2767\n",
      "         4.0       0.63      0.87      0.73      2725\n",
      "\n",
      "    accuracy                           0.65     11279\n",
      "   macro avg       0.62      0.61      0.60     11279\n",
      "weighted avg       0.64      0.65      0.63     11279\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[18 17 40 56 54 53 43 19 25 10 15 49 51  1 60  3 24 26 32 21 16 52 11 42\n",
      "  2 14 22 59 12  4 30 55 38  0 36 37 50 28 47 29 13 57 31  6 35 33 45 39\n",
      " 48 34 41 20 23 27 46]\n",
      ">>>>>>>> test recordings: \n",
      "[44 58  8  5  9  7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(107363, 10) y=(107363,)\n",
      "Test set: X=(12909, 10) y=(12909,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.60      0.63      1928\n",
      "         1.0       0.55      0.34      0.42      2408\n",
      "         2.0       0.71      0.59      0.64      2740\n",
      "         3.0       0.70      0.83      0.76      2807\n",
      "         4.0       0.67      0.90      0.77      3026\n",
      "\n",
      "    accuracy                           0.67     12909\n",
      "   macro avg       0.66      0.65      0.64     12909\n",
      "weighted avg       0.66      0.67      0.65     12909\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[32  0 43 47  2 19 17 30 51 60 29 40 14 20 52 26 10 13  4 38 41 42 11 37\n",
      " 23 12  6 35 24 27 34 54 55 22 57 44  1 59  5 31 58 33 53 18  3  9 46 36\n",
      " 48 50 15 16 39 56 45]\n",
      ">>>>>>>> test recordings: \n",
      "[28  7 49  8 21 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(108759, 15) y=(108759,)\n",
      "Test set: X=(11513, 15) y=(11513,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.68      0.68      1912\n",
      "         1.0       0.51      0.42      0.46      2024\n",
      "         2.0       0.72      0.65      0.68      2336\n",
      "         3.0       0.76      0.88      0.81      2439\n",
      "         4.0       0.76      0.83      0.79      2802\n",
      "\n",
      "    accuracy                           0.70     11513\n",
      "   macro avg       0.69      0.69      0.69     11513\n",
      "weighted avg       0.70      0.70      0.70     11513\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[ 4  0  1 31  3 39 25 50 51 20 60 29 35 41 37 33 57 23 47 16 46 30  9 22\n",
      " 59 45 12 24 18 11  2 58 53 55 17  5  7 10 56 26  6 49 44 14 54 40 43 48\n",
      " 13 32 19 42 52  8 21]\n",
      ">>>>>>>> test recordings: \n",
      "[27 36 28 34 38 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(109377, 20) y=(109377,)\n",
      "Test set: X=(10895, 20) y=(10895,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.63      0.68      1411\n",
      "         1.0       0.54      0.52      0.53      1800\n",
      "         2.0       0.75      0.72      0.74      2398\n",
      "         3.0       0.86      0.86      0.86      2572\n",
      "         4.0       0.75      0.87      0.81      2714\n",
      "\n",
      "    accuracy                           0.75     10895\n",
      "   macro avg       0.73      0.72      0.72     10895\n",
      "weighted avg       0.74      0.75      0.74     10895\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[ 1 52 59 56 23  6 22 12 35 32 29 28 10 34 54 14  4 36 51 18 44 30 42 45\n",
      " 39 50 55 21  8 57 37 47 20  3 24 48 33 15  7 13 38  9 27 60 40 19 43 49\n",
      " 16 25  5 46 58 11 31]\n",
      ">>>>>>>> test recordings: \n",
      "[53 17  0 41  2 26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(108040, 25) y=(108040,)\n",
      "Test set: X=(12232, 25) y=(12232,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.58      0.62      1981\n",
      "         1.0       0.56      0.39      0.46      2214\n",
      "         2.0       0.64      0.76      0.70      2581\n",
      "         3.0       0.85      0.94      0.89      2679\n",
      "         4.0       0.72      0.76      0.74      2777\n",
      "\n",
      "    accuracy                           0.71     12232\n",
      "   macro avg       0.69      0.69      0.68     12232\n",
      "weighted avg       0.70      0.71      0.70     12232\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[25 18  7 52 44 36 16  8 57  3 22 46 20 34 47 13 35 11  9 59 39 30 37 33\n",
      " 26 23 38 41  2 17 10 53 49 55 29 48 50 31 51  1  6 42 40 21 19  0 24 14\n",
      " 45 56  5 32 60 43  4]\n",
      ">>>>>>>> test recordings: \n",
      "[58 28 54 12 27 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(108196, 30) y=(108196,)\n",
      "Test set: X=(12076, 30) y=(12076,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.70      0.77      1638\n",
      "         1.0       0.72      0.62      0.67      2048\n",
      "         2.0       0.75      0.80      0.77      2717\n",
      "         3.0       0.88      0.89      0.89      2733\n",
      "         4.0       0.80      0.90      0.85      2940\n",
      "\n",
      "    accuracy                           0.80     12076\n",
      "   macro avg       0.80      0.78      0.79     12076\n",
      "weighted avg       0.80      0.80      0.80     12076\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[55 32 15  3 13  4 30 46 10 26 23 27 31 20 35 37 14 53 52 34 17 47  1  2\n",
      " 59 36 25 12 16 41 57 21 43 39 54  7  9 49 22  8 51  0  5 18 44 29 45 19\n",
      " 42 60 56  6 28 33 40]\n",
      ">>>>>>>> test recordings: \n",
      "[38 11 24 48 58 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(108414, 35) y=(108414,)\n",
      "Test set: X=(11858, 35) y=(11858,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.79      0.75      1330\n",
      "         1.0       0.72      0.54      0.61      2365\n",
      "         2.0       0.75      0.74      0.75      2774\n",
      "         3.0       0.89      0.89      0.89      2677\n",
      "         4.0       0.76      0.90      0.83      2712\n",
      "\n",
      "    accuracy                           0.78     11858\n",
      "   macro avg       0.77      0.77      0.76     11858\n",
      "weighted avg       0.77      0.78      0.77     11858\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[53 31 28 36 10 16 18 30 24 49 41 34  9 40 35 60  5 42 55  1  7  8 21 57\n",
      "  6 58  4 29 54 19 38 52  3 43 11  0 44 20 14 22 15 23 47 26 51 32 45 56\n",
      " 25 33 27 13 46 39 48]\n",
      ">>>>>>>> test recordings: \n",
      "[37 59 17  2 12 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(108844, 40) y=(108844,)\n",
      "Test set: X=(11428, 40) y=(11428,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.56      0.66      1422\n",
      "         1.0       0.55      0.36      0.43      1969\n",
      "         2.0       0.63      0.80      0.70      2736\n",
      "         3.0       0.86      0.94      0.90      2574\n",
      "         4.0       0.76      0.79      0.77      2727\n",
      "\n",
      "    accuracy                           0.72     11428\n",
      "   macro avg       0.72      0.69      0.69     11428\n",
      "weighted avg       0.72      0.72      0.71     11428\n",
      "\n",
      ">>>>>>>> train recordings (index): \n",
      "[11 18 48 13 12  2  7 52 56 25 26 33 19 50 54 43 40 51 44  3 53  6 17 39\n",
      " 47 15 58 46 16 10 32 31  5 34 20 21 38 41 42 55  8 49  0 27 45 35 30 22\n",
      " 14  1 57 60 29 59  9]\n",
      ">>>>>>>> test recordings: \n",
      "[23 36  4 24 28 37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X=(110186, 45) y=(110186,)\n",
      "Test set: X=(10086, 45) y=(10086,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.66      0.66      1282\n",
      "         1.0       0.57      0.43      0.49      1638\n",
      "         2.0       0.67      0.72      0.69      2372\n",
      "         3.0       0.84      0.82      0.83      2350\n",
      "         4.0       0.74      0.83      0.78      2444\n",
      "\n",
      "    accuracy                           0.72     10086\n",
      "   macro avg       0.70      0.69      0.69     10086\n",
      "weighted avg       0.71      0.72      0.71     10086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    }
   ],
   "source": [
    "# This cell took 5 hours to execute \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_feat_arr = [5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "confmat_arr = []\n",
    "report_arr = []\n",
    "accuracy_arr = np.array([])\n",
    "\n",
    "for i, n_feat in enumerate(n_feat_arr):\n",
    "    # To split dataset into train/test set:\n",
    "    X_train_std, X_test_std, y_train, y_test = train_test_split(test_prop=0.1, n_feat=n_feat)\n",
    "    # To initiate model\n",
    "    svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "    # To fit the model to train set:\n",
    "    svm.fit(X_train_std, y_train)\n",
    "    # To predit on the test set:\n",
    "    y_pred = svm.predict(X_test_std)\n",
    "    # To print results\n",
    "    print\n",
    "    (\n",
    "        f\"Fold {i}, {n_feat} features => Misclassified: {(y_test != y_pred).sum()}, Acc.: {accuracy_score(y_test, y_pred)}\"\n",
    "    )\n",
    "    # To append accuracy to array\n",
    "    accuracy_arr = np.append(accuracy_arr, accuracy_score(y_test, y_pred))\n",
    "    # to save report and confmat\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    confmat = confusion_matrix(y_test, y_pred)\n",
    "    report_arr.append(report)\n",
    "    confmat_arr.append(confmat)\n",
    "    print(report)\n",
    "    confmat_f(confmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.62      0.65      1486\n",
      "         1.0       0.41      0.15      0.22      1590\n",
      "         2.0       0.64      0.68      0.66      2711\n",
      "         3.0       0.75      0.72      0.74      2767\n",
      "         4.0       0.63      0.87      0.73      2725\n",
      "\n",
      "    accuracy                           0.65     11279\n",
      "   macro avg       0.62      0.61      0.60     11279\n",
      "weighted avg       0.64      0.65      0.63     11279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.60      0.63      1928\n",
      "         1.0       0.55      0.34      0.42      2408\n",
      "         2.0       0.71      0.59      0.64      2740\n",
      "         3.0       0.70      0.83      0.76      2807\n",
      "         4.0       0.67      0.90      0.77      3026\n",
      "\n",
      "    accuracy                           0.67     12909\n",
      "   macro avg       0.66      0.65      0.64     12909\n",
      "weighted avg       0.66      0.67      0.65     12909\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.68      0.68      1912\n",
      "         1.0       0.51      0.42      0.46      2024\n",
      "         2.0       0.72      0.65      0.68      2336\n",
      "         3.0       0.76      0.88      0.81      2439\n",
      "         4.0       0.76      0.83      0.79      2802\n",
      "\n",
      "    accuracy                           0.70     11513\n",
      "   macro avg       0.69      0.69      0.69     11513\n",
      "weighted avg       0.70      0.70      0.70     11513\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.63      0.68      1411\n",
      "         1.0       0.54      0.52      0.53      1800\n",
      "         2.0       0.75      0.72      0.74      2398\n",
      "         3.0       0.86      0.86      0.86      2572\n",
      "         4.0       0.75      0.87      0.81      2714\n",
      "\n",
      "    accuracy                           0.75     10895\n",
      "   macro avg       0.73      0.72      0.72     10895\n",
      "weighted avg       0.74      0.75      0.74     10895\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.58      0.62      1981\n",
      "         1.0       0.56      0.39      0.46      2214\n",
      "         2.0       0.64      0.76      0.70      2581\n",
      "         3.0       0.85      0.94      0.89      2679\n",
      "         4.0       0.72      0.76      0.74      2777\n",
      "\n",
      "    accuracy                           0.71     12232\n",
      "   macro avg       0.69      0.69      0.68     12232\n",
      "weighted avg       0.70      0.71      0.70     12232\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.70      0.77      1638\n",
      "         1.0       0.72      0.62      0.67      2048\n",
      "         2.0       0.75      0.80      0.77      2717\n",
      "         3.0       0.88      0.89      0.89      2733\n",
      "         4.0       0.80      0.90      0.85      2940\n",
      "\n",
      "    accuracy                           0.80     12076\n",
      "   macro avg       0.80      0.78      0.79     12076\n",
      "weighted avg       0.80      0.80      0.80     12076\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.79      0.75      1330\n",
      "         1.0       0.72      0.54      0.61      2365\n",
      "         2.0       0.75      0.74      0.75      2774\n",
      "         3.0       0.89      0.89      0.89      2677\n",
      "         4.0       0.76      0.90      0.83      2712\n",
      "\n",
      "    accuracy                           0.78     11858\n",
      "   macro avg       0.77      0.77      0.76     11858\n",
      "weighted avg       0.77      0.78      0.77     11858\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.56      0.66      1422\n",
      "         1.0       0.55      0.36      0.43      1969\n",
      "         2.0       0.63      0.80      0.70      2736\n",
      "         3.0       0.86      0.94      0.90      2574\n",
      "         4.0       0.76      0.79      0.77      2727\n",
      "\n",
      "    accuracy                           0.72     11428\n",
      "   macro avg       0.72      0.69      0.69     11428\n",
      "weighted avg       0.72      0.72      0.71     11428\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.66      0.66      1282\n",
      "         1.0       0.57      0.43      0.49      1638\n",
      "         2.0       0.67      0.72      0.69      2372\n",
      "         3.0       0.84      0.82      0.83      2350\n",
      "         4.0       0.74      0.83      0.78      2444\n",
      "\n",
      "    accuracy                           0.72     10086\n",
      "   macro avg       0.70      0.69      0.69     10086\n",
      "weighted avg       0.71      0.72      0.71     10086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write results to a file \n",
    "# textfile = open(\"report_arr hyperparam feat_num top.txt\", \"w\")\n",
    "# for element in report_arr:\n",
    "#     textfile.write(element + \",\\n\")\n",
    "# textfile.close()\n",
    "\n",
    "# load results from that file\n",
    "report_arr = open(\"report_arr hyperparam feat_num top.txt\")\n",
    "report_arr = ('').join(report_arr.readlines()).split(',\\n')[:-1]\n",
    "\n",
    "for el in report_arr:\n",
    "    print(el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to a file \n",
    "# np.savetxt(\n",
    "#     \"confmat_arr hyperparam feat_num top.csv\",\n",
    "#     np.array(confmat_arr).reshape((3, -1)),\n",
    "#     delimiter=\",\",\n",
    "# )\n",
    "\n",
    "# load results from that file\n",
    "confmat_arr = np.loadtxt(\"confmat_arr hyperparam feat_num top.csv\", delimiter=',')\n",
    "confmat_arr = confmat_arr.reshape(9,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_i: [precision  recall   f1-score   support]\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[0].split(\"   \")\n",
    "class_0 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,9):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[0].split(\"   \")\n",
    "    class_0 = np.vstack([class_0,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_0 = class_0.astype('float')\n",
    "class_0 = class_0*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[1].split(\"   \")\n",
    "class_1 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,9):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[1].split(\"   \")\n",
    "    class_1 = np.vstack([class_1,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_1 = class_1[:,1:]\n",
    "class_1 = class_1.astype('float')\n",
    "class_1 = class_1*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[2].split(\"   \")\n",
    "class_2 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,9):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[2].split(\"   \")\n",
    "    class_2 = np.vstack([class_2,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_2 = class_2[:,1:]\n",
    "class_2 = class_2.astype('float')\n",
    "class_2 = class_2*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[3].split(\"   \")\n",
    "class_3 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,9):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[3].split(\"   \")\n",
    "    class_3 = np.vstack([class_3,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_3 = class_3[:,1:]\n",
    "class_3 = class_3.astype('float')\n",
    "class_3 = class_3*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[4].split(\"   \")\n",
    "class_4 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,9):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[4].split(\"   \")\n",
    "    class_4 = np.vstack([class_4,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_4 = class_4[:,1:]\n",
    "class_4 = class_4.astype('float')\n",
    "class_4 = class_4*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 7, 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximums = [\n",
    "    np.argmax(class_0[:, 0] + class_0[:, 1] + class_0[:, 2]),\n",
    "    np.argmax(class_1[:, 0] + class_1[:, 1] + class_1[:, 2]),\n",
    "    np.argmax(class_2[:, 0] + class_2[:, 1] + class_2[:, 2]),\n",
    "    np.argmax(class_3[:, 0] + class_3[:, 1] + class_3[:, 2]),\n",
    "    np.argmax(class_4[:, 0] + class_4[:, 1] + class_4[:, 2]),\n",
    "]\n",
    "\n",
    "maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat_arr = [5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 5), sharex=True)\n",
    "ylabel = [\"Precision\", \"Recall\", \"F1-score\"]\n",
    "for i in range(3):\n",
    "    ax[i].plot(n_feat_arr, class_0[:, i], label=\"Wake\", color=\"tomato\")\n",
    "    ax[i].plot(n_feat_arr, class_0[:, i], \"o\", color=\"red\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[0]], color=\"tomato\", linestyle='--', linewidth=6, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_1[:, i], label=\"N1\", color=\"gold\")\n",
    "    ax[i].plot(n_feat_arr, class_1[:, i], \"o\", color=\"goldenrod\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[1]], color=\"gold\", linestyle='--', linewidth=4, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_2[:, i], label=\"N2\", color=\"limegreen\")\n",
    "    ax[i].plot(n_feat_arr, class_2[:, i], \"o\", color=\"olivedrab\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[2]], color=\"limegreen\", linestyle='--', linewidth=3, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_3[:, i], label=\"N3\", color=\"dodgerblue\")\n",
    "    ax[i].plot(n_feat_arr, class_3[:, i], \"o\", color=\"royalblue\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[3]], color=\"dodgerblue\", linestyle='--', linewidth=4, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_4[:, i], label=\"REM\", color=\"mediumslateblue\")\n",
    "    ax[i].plot(n_feat_arr, class_4[:, i], \"o\", color=\"darkviolet\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[4]],color=\"mediumslateblue\", linestyle='--', linewidth=4, alpha=0.3)\n",
    "    ax[i].set(ylim=[25, 100], xticks=n_feat_arr)\n",
    "    ax[i].grid(alpha=0.4)\n",
    "    ax[i].set(ylabel=ylabel[i])\n",
    "\n",
    "plt.xlabel(\"Number of top features\")\n",
    "ax[1].legend()\n",
    "# ax[1].legend(loc=(1.01,0.05))\n",
    "ax[0].set(\n",
    "    title=\"SVM RBF C=10 performance metrics with different number of top uncorr. features. (train,test)=(55,6) sessions\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm performace metrics plot top.png')\n",
    "plt.savefig('svm performace metrics plot top.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = class_0[:, 0] + class_0[:, 1] + class_0[:, 2]\n",
    "arr = np.vstack([arr, class_1[:, 0] + class_1[:, 1] + class_1[:, 2]])\n",
    "arr = np.vstack([arr, class_2[:, 0] + class_2[:, 1] + class_2[:, 2]])\n",
    "arr = np.vstack([arr, class_3[:, 0] + class_3[:, 1] + class_3[:, 2]])\n",
    "arr = np.vstack([arr, class_4[:, 0] + class_4[:, 1] + class_4[:, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[195., 189., 204., 205., 187., 232., 225., 201., 199.],\n",
       "       [ 78., 131., 139., 159., 141., 201., 187., 134., 149.],\n",
       "       [198., 194., 205., 221., 210., 232., 224., 213., 208.],\n",
       "       [221., 229., 245., 258., 268., 266., 267., 270., 249.],\n",
       "       [223., 234., 238., 243., 222., 255., 249., 232., 235.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(6,7), sharex=True)\n",
    "ax[0].imshow(arr, cmap=\"Blues\")\n",
    "ax[0].set(\n",
    "    xticks=list(range(0, 9)),\n",
    "    xticklabels=n_feat_arr,\n",
    "    yticks=list(range(0, 5)),\n",
    "    yticklabels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"],\n",
    "    ylabel=\"Sleep Stage\",\n",
    "    title=\"Sum of precision, recall, and f1-score\"\n",
    ")\n",
    "\n",
    "array = np.sum(arr, axis=0)\n",
    "temp = array.argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(array))\n",
    "ranks = 8 - ranks\n",
    "\n",
    "ax[1].imshow(np.sum(arr, axis=0).reshape(1,-1), cmap=\"Blues\")\n",
    "ax[1].set(\n",
    "    xticks=list(range(0, 9)),\n",
    "    xticklabels=n_feat_arr,\n",
    "    yticks=list(range(0, 1)),\n",
    "    xlabel=\"Number of top uncorrelated features\",\n",
    "    ylabel=\"Sum of stages\",\n",
    ")\n",
    "ax[1].set_yticklabels([\"$\\Sigma$\"], fontsize=25)\n",
    "for i in range(len(ranks)):\n",
    "    if i == 0:\n",
    "        ax[1].text(i, 0, ranks[i]+1, ha=\"center\", va=\"center\", color=\"black\", fontsize=15)\n",
    "    else:\n",
    "        ax[1].text(i, 0, ranks[i]+1, ha=\"center\", va=\"center\", color=\"white\", fontsize=15)\n",
    "    \n",
    "plt.subplots_adjust(top=1.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sum of metrics matrix top.png\")\n",
    "plt.savefig(\"sum of metrics matrix top.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold CV with C=10, n_feat=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# pipe_lr = make_pipeline(SVC(kernel=\"rbf\", C=10))\n",
    "\n",
    "# X_train_std, X_test_std, y_train, y_test = train_test_split(test_prop=0.05, n_feat=35)\n",
    "# kfold = StratifiedKFold(n_splits=10).split(X_train_std, y_train)\n",
    "\n",
    "# scores = []\n",
    "# for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "#     pipe_lr.fit(X_train_std[train], y_train[train])\n",
    "#     score = pipe_lr.score(X_train_std[test], y_train[test])\n",
    "#     scores.append(score)\n",
    "\n",
    "#     print(\n",
    "#         f\"Fold: {k+1:02d}, Class distr.: {np.bincount(y_train[train].astype(int))}, Acc.: {score:.3f}\"\n",
    "#     )\n",
    "\n",
    "# mean_acc = np.mean(scores)\n",
    "# std_acc = np.std(scores)\n",
    "# print(f\"\\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[59 43 22 27  7 41 46  9 34 47 58 49 60 54 38 56  0 26  4 37 53 10 19 51\n",
      "  2 18 24 14 48 42 17 52 45  8  5 12 28 35 32 13 40 36 23 44 31  3]\n",
      ">>>>>>>> test recordings: \n",
      "[21 25 20 50 11 39 33 29 15 57 30  1 16 55  6]\n",
      "Train set: X=(91400, 30) y=(91400,)\n",
      "Test set: X=(28872, 30) y=(28872,)\n",
      "Misclassified examples: 6873\n",
      "Accuracy: 0.762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_std, X_test_std, y_train, y_test = train_test_split(test_prop=0.25, n_feat=30)\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(\"Misclassified examples: %d\" % (y_test != y_pred).sum())\n",
    "print(\"Accuracy: %.3f\" % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.67      0.74      4267\n",
      "         1.0       0.60      0.50      0.54      5134\n",
      "         2.0       0.74      0.78      0.76      6168\n",
      "         3.0       0.89      0.90      0.89      6410\n",
      "         4.0       0.74      0.87      0.80      6893\n",
      "\n",
      "    accuracy                           0.76     28872\n",
      "   macro avg       0.76      0.74      0.75     28872\n",
      "weighted avg       0.76      0.76      0.76     28872\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_95757/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred)\n",
    "confmat_f(confmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results in hypnogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(\n",
    "    data,\n",
    "    sf,\n",
    "    hypno=None,\n",
    "    hypno_pred=None,\n",
    "    win_sec=30,\n",
    "    fmin=0.5,\n",
    "    fmax=25,\n",
    "    trimperc=2.5,\n",
    "    cmap=\"RdBu_r\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a full-night multi-taper spectrogram, optionally with the hypnogram on top.\n",
    "    For more details, please refer to the `Jupyter notebook\n",
    "    <https://github.com/raphaelvallat/yasa/blob/master/notebooks/10_spectrogram.ipynb>`_\n",
    "    .. versionadded:: 0.1.8\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : :py:class:`numpy.ndarray`\n",
    "        Single-channel EEG data. Must be a 1D NumPy array.\n",
    "    sf : float\n",
    "        The sampling frequency of data AND the hypnogram.\n",
    "    hypno : array_like\n",
    "        Sleep stage (hypnogram), optional.\n",
    "        The hypnogram must have the exact same number of samples as ``data``.\n",
    "        To upsample your hypnogram, please refer to :py:func:`yasa.hypno_upsample_to_data`.\n",
    "        .. note::\n",
    "            The default hypnogram format in YASA is a 1D integer\n",
    "            vector where:\n",
    "            - -2 = Unscored\n",
    "            - -1 = Artefact / Movement\n",
    "            - 0 = Wake\n",
    "            - 1 = N1 sleep\n",
    "            - 2 = N2 sleep\n",
    "            - 3 = N3 sleep\n",
    "            - 4 = REM sleep\n",
    "    win_sec : int or float\n",
    "        The length of the sliding window, in seconds, used for multitaper PSD\n",
    "        calculation. Default is 30 seconds. Note that ``data`` must be at least\n",
    "        twice longer than ``win_sec`` (e.g. 60 seconds).\n",
    "    fmin, fmax : int or float\n",
    "        The lower and upper frequency of the spectrogram. Default 0.5 to 25 Hz.\n",
    "    trimperc : int or float\n",
    "        The amount of data to trim on both ends of the distribution when\n",
    "        normalizing the colormap. This parameter directly impacts the\n",
    "        contrast of the spectrogram plot (higher values = higher contrast).\n",
    "        Default is 2.5, meaning that the min and max of the colormap\n",
    "        are defined as the 2.5 and 97.5 percentiles of the spectrogram.\n",
    "    cmap : str\n",
    "        Colormap. Default to 'RdBu_r'.\n",
    "    Returns\n",
    "    -------\n",
    "    fig : :py:class:`matplotlib.figure.Figure`\n",
    "        Matplotlib Figure\n",
    "    Examples\n",
    "    --------\n",
    "    1. Full-night multitaper spectrogram on Cz, no hypnogram\n",
    "    .. plot::\n",
    "        >>> import yasa\n",
    "        >>> import numpy as np\n",
    "        >>> # In the next 5 lines, we're loading the data from GitHub.\n",
    "        >>> import requests\n",
    "        >>> from io import BytesIO\n",
    "        >>> r = requests.get('https://github.com/raphaelvallat/yasa/raw/master/notebooks/data_full_6hrs_100Hz_Cz%2BFz%2BPz.npz', stream=True)\n",
    "        >>> npz = np.load(BytesIO(r.raw.read()))\n",
    "        >>> data = npz.get('data')[0, :]\n",
    "        >>> sf = 100\n",
    "        >>> fig = yasa.plot_spectrogram(data, sf)\n",
    "    2. Full-night multitaper spectrogram on Cz with the hypnogram on top\n",
    "    .. plot::\n",
    "        >>> import yasa\n",
    "        >>> import numpy as np\n",
    "        >>> # In the next lines, we're loading the data from GitHub.\n",
    "        >>> import requests\n",
    "        >>> from io import BytesIO\n",
    "        >>> r = requests.get('https://github.com/raphaelvallat/yasa/raw/master/notebooks/data_full_6hrs_100Hz_Cz%2BFz%2BPz.npz', stream=True)\n",
    "        >>> npz = np.load(BytesIO(r.raw.read()))\n",
    "        >>> data = npz.get('data')[0, :]\n",
    "        >>> sf = 100\n",
    "        >>> # Load the 30-sec hypnogram and upsample to data\n",
    "        >>> hypno = np.loadtxt('https://raw.githubusercontent.com/raphaelvallat/yasa/master/notebooks/data_full_6hrs_100Hz_hypno_30s.txt')\n",
    "        >>> hypno = yasa.hypno_upsample_to_data(hypno, 1/30, data, sf)\n",
    "        >>> fig = yasa.plot_spectrogram(data, sf, hypno, cmap='Spectral_r')\n",
    "    \"\"\"\n",
    "    # Increase font size while preserving original\n",
    "    old_fontsize = plt.rcParams[\"font.size\"]\n",
    "    plt.rcParams.update({\"font.size\": 13})\n",
    "\n",
    "    # Safety checks\n",
    "    assert isinstance(data, np.ndarray), \"Data must be a 1D NumPy array.\"\n",
    "    assert isinstance(sf, (int, float)), \"sf must be int or float.\"\n",
    "    assert data.ndim == 1, \"Data must be a 1D (single-channel) NumPy array.\"\n",
    "    assert isinstance(win_sec, (int, float)), \"win_sec must be int or float.\"\n",
    "    assert isinstance(fmin, (int, float)), \"fmin must be int or float.\"\n",
    "    assert isinstance(fmax, (int, float)), \"fmax must be int or float.\"\n",
    "    assert fmin < fmax, \"fmin must be strictly inferior to fmax.\"\n",
    "    assert fmax < sf / 2, \"fmax must be less than Nyquist (sf / 2).\"\n",
    "\n",
    "    # Calculate multi-taper spectrogram\n",
    "    nperseg = int(win_sec * sf)\n",
    "    assert data.size > 2 * nperseg, \"Data length must be at least 2 * win_sec.\"\n",
    "    f, t, Sxx = spectrogram_lspopt(data, sf, nperseg=nperseg, noverlap=0)\n",
    "    Sxx = 10 * np.log10(Sxx)  # Convert uV^2 / Hz --> dB / Hz\n",
    "\n",
    "    # Select only relevant frequencies (up to 30 Hz)\n",
    "    good_freqs = np.logical_and(f >= fmin, f <= fmax)\n",
    "    Sxx = Sxx[good_freqs, :]\n",
    "    f = f[good_freqs]\n",
    "    t /= 3600  # Convert t to hours\n",
    "\n",
    "    # Normalization\n",
    "    vmin, vmax = np.percentile(Sxx, [0 + trimperc, 100 - trimperc])\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    if hypno is None:\n",
    "        fig, ax = plt.subplots(nrows=1, figsize=(12, 4))\n",
    "        im = ax.pcolormesh(\n",
    "            t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\"\n",
    "        )\n",
    "        ax.set_xlim(0, t.max())\n",
    "        ax.set_ylabel(\"Frequency [Hz]\")\n",
    "        ax.set_xlabel(\"Time [hrs]\")\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(im, ax=ax, shrink=0.95, fraction=0.1, aspect=25)\n",
    "        cbar.ax.set_ylabel(\"Log Power (dB / Hz)\", rotation=270, labelpad=20)\n",
    "        return fig\n",
    "    elif (hypno is not None) and (hypno_pred is None):\n",
    "        hypno = np.asarray(hypno).astype(int)\n",
    "        assert hypno.ndim == 1, \"Hypno must be 1D.\"\n",
    "        assert hypno.size == data.size, \"Hypno must have the same sf as data.\"\n",
    "        t_hyp = np.arange(hypno.size) / (sf * 3600)\n",
    "        # Make sure that REM is displayed after Wake\n",
    "        hypno = (\n",
    "            pd.Series(hypno).map({-2: -2, -1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1}).values\n",
    "        )\n",
    "        hypno_rem = np.ma.masked_not_equal(hypno, 1)\n",
    "\n",
    "        fig, (ax0, ax1) = plt.subplots(\n",
    "            nrows=2, figsize=(12, 6), gridspec_kw={\"height_ratios\": [1, 2]}\n",
    "        )\n",
    "        plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "        # Hypnogram (top axis)\n",
    "        ax0.step(t_hyp, -1 * hypno, color=\"k\")\n",
    "        ax0.step(t_hyp, -1 * hypno_rem, color=\"r\")\n",
    "        if -2 in hypno and -1 in hypno:\n",
    "            # Both Unscored and Artefacts are present\n",
    "            ax0.set_yticks([2, 1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "        elif -2 in hypno and -1 not in hypno:\n",
    "            # Only Unscored are present\n",
    "            ax0.set_yticks([2, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "\n",
    "        elif -2 not in hypno and -1 in hypno:\n",
    "            # Only Artefacts are present\n",
    "            ax0.set_yticks([1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 1.5)\n",
    "        else:\n",
    "            # No artefacts or Unscored\n",
    "            ax0.set_yticks([0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 0.5)\n",
    "        ax0.set_xlim(0, t_hyp.max())\n",
    "        ax0.set_ylabel(\"Stage\")\n",
    "        ax0.xaxis.set_visible(False)\n",
    "        ax0.spines[\"right\"].set_visible(False)\n",
    "        ax0.spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Spectrogram (bottom axis)\n",
    "        im = ax1.pcolormesh(\n",
    "            t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\"\n",
    "        )\n",
    "        ax1.set_xlim(0, t.max())\n",
    "        ax1.set_ylabel(\"Frequency [Hz]\")\n",
    "        ax1.set_xlabel(\"Time [hrs]\")\n",
    "\n",
    "        # Revert font-size\n",
    "        plt.rcParams.update({\"font.size\": old_fontsize})\n",
    "        return fig\n",
    "    elif (hypno is not None) and (hypno_pred is not None):\n",
    "        hypno_pred = np.asarray(hypno_pred).astype(int)\n",
    "        hypno = np.asarray(hypno).astype(int)\n",
    "        assert hypno.ndim == 1, \"Hypno must be 1D.\"\n",
    "        assert hypno.size == data.size, \"Hypno must have the same sf as data.\"\n",
    "        assert hypno_pred.ndim == 1, \"hypno_pred must be 1D.\"\n",
    "        assert hypno_pred.size == data.size, \"hypno_pred must have the same sf as data.\"\n",
    "        t_hyp = np.arange(hypno.size) / (sf * 3600)\n",
    "        t_hyp_pred = np.arange(hypno_pred.size) / (sf * 3600)\n",
    "        # Make sure that REM is displayed after Wake\n",
    "        hypno = (\n",
    "            pd.Series(hypno).map({-2: -2, -1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1}).values\n",
    "        )\n",
    "        hypno_pred = (\n",
    "            pd.Series(hypno_pred)\n",
    "            .map({-2: -2, -1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1})\n",
    "            .values\n",
    "        )\n",
    "        hypno_rem = np.ma.masked_not_equal(hypno, 1)\n",
    "        hypno_pred_rem = np.ma.masked_not_equal(hypno_pred, 1)\n",
    "\n",
    "        fig, (ax0, ax1, ax2) = plt.subplots(\n",
    "            nrows=3, figsize=(12, 6), gridspec_kw={\"height_ratios\": [1, 1, 2]}\n",
    "        )\n",
    "        plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "        # Hypnogram (top axis)\n",
    "        ax0.step(t_hyp, -1 * hypno, color=\"k\")\n",
    "        ax0.step(t_hyp, -1 * hypno_rem, color=\"r\")\n",
    "        if -2 in hypno and -1 in hypno:\n",
    "            # Both Unscored and Artefacts are present\n",
    "            ax0.set_yticks([2, 1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "        elif -2 in hypno and -1 not in hypno:\n",
    "            # Only Unscored are present\n",
    "            ax0.set_yticks([2, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "\n",
    "        elif -2 not in hypno and -1 in hypno:\n",
    "            # Only Artefacts are present\n",
    "            ax0.set_yticks([1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 1.5)\n",
    "        else:\n",
    "            # No artefacts or Unscored\n",
    "            ax0.set_yticks([0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 0.5)\n",
    "        ax0.set_xlim(0, t_hyp.max())\n",
    "        ax0.set_ylabel(\"Stage\")\n",
    "        ax0.xaxis.set_visible(False)\n",
    "        ax0.spines[\"right\"].set_visible(False)\n",
    "        ax0.spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Hypnogram Pred (middle axis)\n",
    "        ax1.step(t_hyp_pred, -1 * hypno_pred, color=\"k\")\n",
    "        ax1.step(t_hyp_pred, -1 * hypno_pred_rem, color=\"r\")\n",
    "        if -2 in hypno_pred and -1 in hypno_pred:\n",
    "            # Both Unscored and Artefacts are present\n",
    "            ax1.set_yticks([2, 1, 0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"Uns\", \"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 2.5)\n",
    "        elif -2 in hypno_pred and -1 not in hypno_pred:\n",
    "            # Only Unscored are present\n",
    "            ax1.set_yticks([2, 0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"Uns\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 2.5)\n",
    "\n",
    "        elif -2 not in hypno_pred and -1 in hypno_pred:\n",
    "            # Only Artefacts are present\n",
    "            ax1.set_yticks([1, 0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 1.5)\n",
    "        else:\n",
    "            # No artefacts or Unscored\n",
    "            ax1.set_yticks([0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 0.5)\n",
    "        ax1.set_xlim(0, t_hyp_pred.max())\n",
    "        ax1.set_ylabel(\"Stage\")\n",
    "        ax1.xaxis.set_visible(False)\n",
    "        ax1.spines[\"right\"].set_visible(False)\n",
    "        ax1.spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Spectrogram (bottom axis)\n",
    "        im = ax2.pcolormesh(\n",
    "            t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\"\n",
    "        )\n",
    "        ax2.set_xlim(0, t.max())\n",
    "        ax2.set_ylabel(\"Frequency [Hz]\")\n",
    "        ax2.set_xlabel(\"Time [hrs]\")\n",
    "\n",
    "        # Revert font-size\n",
    "        plt.rcParams.update({\"font.size\": old_fontsize})\n",
    "        return fig\n",
    "\n",
    "\n",
    "def format_seconds_to_hhmmss(seconds):\n",
    "    hours = seconds // (60 * 60)\n",
    "    seconds %= 60 * 60\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%02i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "def set_log_level(verbose=None):\n",
    "    \"\"\"Convenience function for setting the logging level.\n",
    "    This function comes from the PySurfer package. See :\n",
    "    https://github.com/nipy/PySurfer/blob/master/surfer/utils.py\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool, str, int, or None\n",
    "        The verbosity of messages to print. If a str, it can be either\n",
    "        PROFILER, DEBUG, INFO, WARNING, ERROR, or CRITICAL.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"yasa\")\n",
    "    if isinstance(verbose, bool):\n",
    "        verbose = \"INFO\" if verbose else \"WARNING\"\n",
    "    if isinstance(verbose, str):\n",
    "        if verbose.upper() in LOGGING_TYPES:\n",
    "            verbose = verbose.upper()\n",
    "            verbose = LOGGING_TYPES[verbose]\n",
    "            logger.setLevel(verbose)\n",
    "        else:\n",
    "            raise ValueError(\"verbose must be in %s\" % \", \".join(LOGGING_TYPES))\n",
    "\n",
    "\n",
    "def hypno_upsample_to_data(hypno, sf_hypno, data, sf_data=None, verbose=True):\n",
    "    \"\"\"Upsample an hypnogram to a given sampling frequency and fit the\n",
    "    resulting hypnogram to corresponding EEG data, such that the hypnogram\n",
    "    and EEG data have the exact same number of samples.\n",
    "    .. versionadded:: 0.1.5\n",
    "    Parameters\n",
    "    ----------\n",
    "    hypno : array_like\n",
    "        The sleep staging (hypnogram) 1D array.\n",
    "    sf_hypno : float\n",
    "        The current sampling frequency of the hypnogram, in Hz, e.g.\n",
    "        * 1/30 = 1 value per each 30 seconds of EEG data,\n",
    "        * 1 = 1 value per second of EEG data\n",
    "    data : array_like or :py:class:`mne.io.BaseRaw`\n",
    "        1D or 2D EEG data. Can also be a :py:class:`mne.io.BaseRaw`, in which\n",
    "        case ``data`` and ``sf_data`` will be automatically extracted.\n",
    "    sf_data : float\n",
    "        The sampling frequency of ``data``, in Hz (e.g. 100 Hz, 256 Hz, ...).\n",
    "        Can be omitted if ``data`` is a :py:class:`mne.io.BaseRaw`.\n",
    "    verbose : bool or str\n",
    "        Verbose level. Default (False) will only print warning and error\n",
    "        messages. The logging levels are 'debug', 'info', 'warning', 'error',\n",
    "        and 'critical'. For most users the choice is between 'info'\n",
    "        (or ``verbose=True``) and warning (``verbose=False``).\n",
    "    Returns\n",
    "    -------\n",
    "    hypno : array_like\n",
    "        The hypnogram, upsampled to ``sf_data`` and cropped/padded to ``max(data.shape)``.\n",
    "    Warns\n",
    "    -----\n",
    "    UserWarning\n",
    "        If the upsampled ``hypno`` is shorter / longer than ``max(data.shape)``\n",
    "        and therefore needs to be padded/cropped respectively. This output can be disabled by\n",
    "        passing ``verbose='ERROR'``.\n",
    "    \"\"\"\n",
    "    set_log_level(verbose)\n",
    "    if isinstance(data, mne.io.BaseRaw):\n",
    "        sf_data = data.info[\"sfreq\"]\n",
    "        data = data.times\n",
    "\n",
    "    # Upsample the hypnogram to a given sampling frequency\n",
    "    repeats = sf_data / sf_hypno\n",
    "    assert sf_hypno <= sf_data, \"sf_hypno must be less than sf_data.\"\n",
    "    assert repeats.is_integer(), \"sf_hypno / sf_data must be a whole number.\"\n",
    "    assert isinstance(hypno, (list, np.ndarray, pd.Series))\n",
    "    hypno_up = np.repeat(np.asarray(hypno), repeats)\n",
    "\n",
    "    # Crop or pad the hypnogram to fit the length of data.\n",
    "    # Check if data is an MNE raw object\n",
    "    hypno = hypno_up\n",
    "    sf = sf_data\n",
    "    if isinstance(data, mne.io.BaseRaw):\n",
    "        sf = data.info[\"sfreq\"]\n",
    "        data = data.times  # 1D array and does not require to preload data\n",
    "    data = np.asarray(data)\n",
    "    hypno = np.asarray(hypno)\n",
    "    assert hypno.ndim == 1, \"Hypno must be 1D.\"\n",
    "    npts_hyp = hypno.size\n",
    "    npts_data = max(data.shape)  # Support for 2D data\n",
    "    if npts_hyp < npts_data:\n",
    "        # Hypnogram is shorter than data\n",
    "        npts_diff = npts_data - npts_hyp\n",
    "        if sf is not None:\n",
    "            dur_diff = npts_diff / sf\n",
    "            logger.warning(\n",
    "                \"Hypnogram is SHORTER than data by %.2f seconds. \"\n",
    "                \"Padding hypnogram with last value to match data.size.\" % dur_diff\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Hypnogram is SHORTER than data by %i samples. \"\n",
    "                \"Padding hypnogram with last value to match data.size.\" % npts_diff\n",
    "            )\n",
    "        hypno = np.pad(hypno, (0, npts_diff), mode=\"edge\")\n",
    "    elif npts_hyp > npts_data:\n",
    "        # Hypnogram is longer than data\n",
    "        npts_diff = npts_hyp - npts_data\n",
    "        if sf is not None:\n",
    "            dur_diff = npts_diff / sf\n",
    "            logger.warning(\n",
    "                \"Hypnogram is LONGER than data by %.2f seconds. \"\n",
    "                \"Cropping hypnogram to match data.size.\" % dur_diff\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Hypnogram is LONGER than data by %i samples. \"\n",
    "                \"Cropping hypnogram to match data.size.\" % npts_diff\n",
    "            )\n",
    "        hypno = hypno[0:npts_data]\n",
    "\n",
    "    return hypno\n",
    "\n",
    "\n",
    "def transition_matrix(hypno):\n",
    "    \"\"\"Create a state-transition matrix from an hypnogram.\n",
    "    .. versionadded:: 0.1.9\n",
    "    Parameters\n",
    "    ----------\n",
    "    hypno : array_like\n",
    "        Hypnogram. The dtype of ``hypno`` must be integer\n",
    "        (e.g. [0, 2, 2, 1, 1, 1, ...]). The sampling frequency must be the\n",
    "        original one, i.e. 1 value per 30 seconds if the staging was done in\n",
    "        30 seconds epochs. Using an upsampled hypnogram will result in an\n",
    "        incorrect transition matrix.\n",
    "        For best results, we recommend using an hypnogram cropped to\n",
    "        either the time in bed (TIB) or the sleep period time (SPT), without\n",
    "        any artefact / unscored epochs.\n",
    "    Returns\n",
    "    -------\n",
    "    counts : :py:class:`pandas.DataFrame`\n",
    "        Counts transition matrix (number of transitions from stage A to\n",
    "        stage B). The pre-transition states are the rows and the\n",
    "        post-transition states are the columns.\n",
    "    probs : :py:class:`pandas.DataFrame`\n",
    "        Conditional probability transition matrix, i.e.\n",
    "        given that current state is A, what is the probability that\n",
    "        the next state is B.\n",
    "        ``probs`` is a `right stochastic matrix\n",
    "        <https://en.wikipedia.org/wiki/Stochastic_matrix>`_,\n",
    "        i.e. each row sums to 1.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from yasa import transition_matrix\n",
    "    >>> a = [0, 0, 0, 1, 1, 0, 1, 2, 2, 3, 3, 2, 3, 3, 0, 2, 2, 1, 2, 2, 3, 3]\n",
    "    >>> counts, probs = transition_matrix(a)\n",
    "    >>> counts\n",
    "           0  1  2  3\n",
    "    Stage\n",
    "    0      2  2  1  0\n",
    "    1      1  1  2  0\n",
    "    2      0  1  3  3\n",
    "    3      1  0  1  3\n",
    "    >>> probs.round(2)\n",
    "              0     1     2     3\n",
    "    Stage\n",
    "    0      0.40  0.40  0.20  0.00\n",
    "    1      0.25  0.25  0.50  0.00\n",
    "    2      0.00  0.14  0.43  0.43\n",
    "    3      0.20  0.00  0.20  0.60\n",
    "    Several metrics of sleep fragmentation can be calculated from the\n",
    "    probability matrix. For example, the stability of sleep stages can be\n",
    "    calculated by taking the average of the diagonal values (excluding Wake\n",
    "    and N1 sleep):\n",
    "    >>> np.diag(probs.loc[2:, 2:]).mean().round(3)\n",
    "    0.514\n",
    "    Finally, we can plot the transition matrix using :py:func:`seaborn.heatmap`\n",
    "    .. plot::\n",
    "        >>> import numpy as np\n",
    "        >>> import seaborn as sns\n",
    "        >>> import matplotlib.pyplot as plt\n",
    "        >>> from yasa import transition_matrix\n",
    "        >>> # Calculate probability matrix\n",
    "        >>> a = [1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0]\n",
    "        >>> _, probs = transition_matrix(a)\n",
    "        >>> # Start the plot\n",
    "        >>> grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .1}\n",
    "        >>> f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws,\n",
    "        ...                                 figsize=(5, 5))\n",
    "        >>> sns.heatmap(probs, ax=ax, square=False, vmin=0, vmax=1, cbar=True,\n",
    "        ...             cbar_ax=cbar_ax, cmap='YlOrRd', annot=True, fmt='.2f',\n",
    "        ...             cbar_kws={\"orientation\": \"horizontal\", \"fraction\": 0.1,\n",
    "        ...                       \"label\": \"Transition probability\"})\n",
    "        >>> ax.set_xlabel(\"To sleep stage\")\n",
    "        >>> ax.xaxis.tick_top()\n",
    "        >>> ax.set_ylabel(\"From sleep stage\")\n",
    "        >>> ax.xaxis.set_label_position('top')\n",
    "    \"\"\"\n",
    "    x = np.asarray(hypno, dtype=int)\n",
    "    unique, inverse = np.unique(x, return_inverse=True)  # unique is sorted\n",
    "    n = unique.size\n",
    "    # Integer transition counts\n",
    "    counts = np.zeros((n, n), dtype=int)\n",
    "    np.add.at(counts, (inverse[:-1], inverse[1:]), 1)\n",
    "    # Conditional probabilities\n",
    "    probs = counts / counts.sum(axis=-1, keepdims=True)\n",
    "    # Convert to a Pandas DataFrame\n",
    "    counts = pd.DataFrame(counts, index=unique, columns=unique)\n",
    "    probs = pd.DataFrame(probs, index=unique, columns=unique)\n",
    "    counts.index.name = \"From Stage\"\n",
    "    probs.index.name = \"From Stage\"\n",
    "    counts.columns.name = \"To Stage\"\n",
    "    probs.columns.name = \"To Stage\"\n",
    "    return counts, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to load augmented hypno:\n",
    "name = reference_df.iloc[33].name\n",
    "hypno_30s_loc = reference_df.iloc[33].hypno\n",
    "hypno_30s_loc = hypno_30s_loc.split(\".\")[0] + \" aug.txt\"\n",
    "hypno_30s = np.loadtxt(hypno_30s_loc, delimiter=\"\\n\")\n",
    "\n",
    "### to load features for augmented eeg:\n",
    "df_feat_loc = reference_df.iloc[33].df_feat\n",
    "df_feat_loc = df_feat_loc.split(\".\")[0] + \" aug.csv\"\n",
    "df_feat = pd.read_csv(df_feat_loc, index_col=False)\n",
    "\n",
    "### to load augmented eeg:\n",
    "eeg_loc = reference_df.iloc[33].eeg\n",
    "eeg_loc = eeg_loc.split(\".\")[0] + \" aug.txt\"\n",
    "data = np.loadtxt(eeg_loc, delimiter=\",\")  # took ~7 seconds # this is filtered data actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypno_30s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 50310.0 (sec) OR 13:58:30\n"
     ]
    }
   ],
   "source": [
    "sf = 256\n",
    "\n",
    "print(\n",
    "    f\"Duration: {data.flatten().shape[0]/sf} (sec) OR {format_seconds_to_hhmmss(data.flatten().shape[0]/sf)}\"\n",
    ")\n",
    "\n",
    "fig = plot_spectrogram(data.flatten(), sf, fmax=45)\n",
    "plt.title(\n",
    "    f\"Spectrogram of {name} - {format_seconds_to_hhmmss(data.shape[1]/sf)}\", fontsize=16\n",
    ")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'spectro QS {folder} {LR}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "hypno = hypno_upsample_to_data(\n",
    "    hypno=hypno_30s, sf_hypno=(1 / 30), data=data.flatten(), sf_data=sf\n",
    ")\n",
    "hypno_pred = hypno_upsample_to_data(\n",
    "    hypno=y_pred[: len(hypno_30s)], sf_hypno=(1 / 30), data=data.flatten(), sf_data=sf\n",
    ")\n",
    "\n",
    "fig = plot_spectrogram(\n",
    "    data.flatten(), sf, hypno=hypno, hypno_pred=hypno_pred, fmax=30, trimperc=5\n",
    ")\n",
    "fig.suptitle(\n",
    "    f\"Spectrogram and Hypnogram of {name} - {format_seconds_to_hhmmss(data.shape[1]/sf)}\",\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'spectro-hypno QS {folder} {LR}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[36  7 25  9 56 41 27  8 13  6 31 43 16 34 20 30 33  2 38 60 39 37 51  3\n",
      " 22 52 24 15 42 58 49 40 53 26  5 18 57 12 35  1 55 21 47  4 44 50]\n",
      ">>>>>>>> test recordings: \n",
      "[29 28 10 59 11 19 46  0 48 54 14 17 32 23 45]\n",
      "Train set: X=(91328, 46) y=(91328,)\n",
      "Test set: X=(28944, 46) y=(28944,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_std2, X_test_std2, y_train2, y_test2 = train_test_split(test_prop=0.25, n_feat=60)\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "svm.fit(X_train_std2, y_train2)\n",
    "y_pred2 = svm.predict(X_test_std2)\n",
    "print(\"Misclassified examples: %d\" % (y_test2 != y_pred2).sum())\n",
    "print(\"Accuracy: %.3f\" % accuracy_score(y_test2, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred)\n",
    "confmat_f(confmat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mne')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9461a3eb0cbef873800a78c94f0b6f375dc71a03d464463d7b611af82b6cd16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
