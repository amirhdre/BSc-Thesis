{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from scipy import signal\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from mne.filter import resample, filter_data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lspopt import spectrogram_lspopt\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n",
    "import logging\n",
    "LOGGING_TYPES = dict(DEBUG=logging.DEBUG, INFO=logging.INFO, WARNING=logging.WARNING,\n",
    "                     ERROR=logging.ERROR, CRITICAL=logging.CRITICAL)\n",
    "logger = logging.getLogger('yasa')\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Setting up band-pass filter from 0.5 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Chan = ['EEG L']\n",
      "Sampling frequency = 256.0 Hz\n",
      "Data shape = (1, 7608320)\n",
      "Duration: 29720.0 (sec) OR 08:15:20\n"
     ]
    }
   ],
   "source": [
    "# Load the EDF file\n",
    "fname = \"P8_N3\"  # define here\n",
    "lr = \"L\"  # define here\n",
    "location = f\"/Users/amirhosseindaraie/Desktop/data/autoscoring-material/data/Zmax Donders/{fname}\"\n",
    "raw = mne.io.read_raw_edf(f\"{location}/EEG {lr}.edf\", preload=True, verbose=0)\n",
    "raw.pick_types(eeg=True)\n",
    "# fig = raw.plot(use_opengl=False)\n",
    "\n",
    "# Apply a zero-phase bandpass filter between 0.5 ~ 45 Hz\n",
    "raw.filter(0.5, 45)\n",
    "\n",
    "# Plot properties of the filter\n",
    "filt = mne.filter.create_filter(raw._data, 256, 0.5, 40)\n",
    "mne.viz.plot_filter(filt, 256)\n",
    "plt.savefig(\"filter shape.png\", dpi=100, bbox_inches=\"tight\")\n",
    "\n",
    "# Extract the data and convert from V to uV\n",
    "data = raw._data * 1e6\n",
    "sf = raw.info[\"sfreq\"]\n",
    "chan = raw.ch_names\n",
    "\n",
    "# Let's have a look at the data\n",
    "print(\"Chan =\", chan)\n",
    "print(\"Sampling frequency =\", sf, \"Hz\")\n",
    "print(\"Data shape =\", data.shape)\n",
    "\n",
    "\n",
    "def format_seconds_to_hhmmss(seconds):\n",
    "    # Return hhmmss of total seconds parameter\n",
    "    hours = seconds // (60 * 60)\n",
    "    seconds %= 60 * 60\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%02i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Duration: {data.shape[1]/sf} (sec) OR {format_seconds_to_hhmmss(data.shape[1]/sf)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antropy as ant\n",
    "import scipy.signal as sp_sig\n",
    "import scipy.stats as sp_stats\n",
    "from numpy import apply_along_axis as apply\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", -1)\n",
    "\n",
    "# Time vector in seconds\n",
    "times = np.arange(data.size) / sf\n",
    "\n",
    "\n",
    "def sliding_window(data, sf, window, step=None, axis=-1):\n",
    "    \"\"\"Calculate a sliding window of a 1D or 2D EEG signal.\n",
    "    .. versionadded:: 0.1.7\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy array\n",
    "        The 1D or 2D EEG data.\n",
    "    sf : float\n",
    "        The sampling frequency of ``data``.\n",
    "    window : int\n",
    "        The sliding window length, in seconds.\n",
    "    step : int\n",
    "        The sliding window step length, in seconds.\n",
    "        If None (default), ``step`` is set to ``window``,\n",
    "        which results in no overlap between the sliding windows.\n",
    "    axis : int\n",
    "        The axis to slide over. Defaults to the last axis.\n",
    "    Returns\n",
    "    -------\n",
    "    times : numpy array\n",
    "        Time vector, in seconds, corresponding to the START of each sliding\n",
    "        epoch in ``strided``.\n",
    "    strided : numpy array\n",
    "        A matrix where row in last dimension consists of one instance\n",
    "        of the sliding window, shape (n_epochs, ..., n_samples).\n",
    "    Notes\n",
    "    -----\n",
    "    This is a wrapper around the\n",
    "    :py:func:`numpy.lib.stride_tricks.as_strided` function.\n",
    "    Examples\n",
    "    --------\n",
    "    With a 1-D array\n",
    "    >>> import numpy as np\n",
    "    >>> from yasa import sliding_window\n",
    "    >>> data = np.arange(20)\n",
    "    >>> times, epochs = sliding_window(data, sf=1, window=5)\n",
    "    >>> times\n",
    "    array([ 0.,  5., 10., 15.])\n",
    "    >>> epochs\n",
    "    array([[ 0,  1,  2,  3,  4],\n",
    "           [ 5,  6,  7,  8,  9],\n",
    "           [10, 11, 12, 13, 14],\n",
    "           [15, 16, 17, 18, 19]])\n",
    "    >>> sliding_window(data, sf=1, window=5, step=1)[1]\n",
    "    array([[ 0,  1,  2,  3,  4],\n",
    "           [ 2,  3,  4,  5,  6],\n",
    "           [ 4,  5,  6,  7,  8],\n",
    "           [ 6,  7,  8,  9, 10],\n",
    "           [ 8,  9, 10, 11, 12],\n",
    "           [10, 11, 12, 13, 14],\n",
    "           [12, 13, 14, 15, 16],\n",
    "           [14, 15, 16, 17, 18]])\n",
    "    >>> sliding_window(data, sf=1, window=11)[1]\n",
    "    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
    "    With a N-D array\n",
    "    >>> np.random.seed(42)\n",
    "    >>> # 4 channels x 20 samples\n",
    "    >>> data = np.random.randint(-100, 100, size=(4, 20))\n",
    "    >>> epochs = sliding_window(data, sf=1, window=10)[1]\n",
    "    >>> epochs.shape  # shape (n_epochs, n_channels, n_samples)\n",
    "    (2, 4, 10)\n",
    "    >>> epochs\n",
    "    array([[[  2,  79,  -8, -86,   6, -29,  88, -80,   2,  21],\n",
    "            [-13,  57, -63,  29,  91,  87, -80,  60, -43, -79],\n",
    "            [-50,   7, -46, -37,  30, -50,  34, -80, -28,  66],\n",
    "            [ -9,  10,  87,  98,  71, -93,  74, -66, -20,  63]],\n",
    "           [[-26, -13,  16,  -1,   3,  51,  30,  49, -48, -99],\n",
    "            [-12, -52, -42,  69,  87, -86,  89,  89,  74,  89],\n",
    "            [-83,  31, -12, -41, -87, -92, -11, -48,  29, -17],\n",
    "            [-51,   3,  31, -99,  33, -47,   5, -97, -47,  90]]])\n",
    "    \"\"\"\n",
    "    from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "    assert axis <= data.ndim, \"Axis value out of range.\"\n",
    "    assert isinstance(sf, (int, float)), \"sf must be int or float\"\n",
    "    assert isinstance(window, (int, float)), \"window must be int or float\"\n",
    "    assert isinstance(step, (int, float, type(None))), (\n",
    "        \"step must be int, \" \"float or None.\"\n",
    "    )\n",
    "    if isinstance(sf, float):\n",
    "        assert sf.is_integer(), \"sf must be a whole number.\"\n",
    "        sf = int(sf)\n",
    "    assert isinstance(axis, int), \"axis must be int.\"\n",
    "\n",
    "    # window and step in samples instead of points\n",
    "    window *= sf\n",
    "    step = window if step is None else step * sf\n",
    "\n",
    "    if isinstance(window, float):\n",
    "        assert window.is_integer(), \"window * sf must be a whole number.\"\n",
    "        window = int(window)\n",
    "\n",
    "    if isinstance(step, float):\n",
    "        assert step.is_integer(), \"step * sf must be a whole number.\"\n",
    "        step = int(step)\n",
    "\n",
    "    assert step >= 1, \"Stepsize may not be zero or negative.\"\n",
    "    assert window < data.shape[axis], (\n",
    "        \"Sliding window size may not exceed \" \"size of selected axis\"\n",
    "    )\n",
    "\n",
    "    # Define output shape\n",
    "    shape = list(data.shape)\n",
    "    shape[axis] = np.floor(data.shape[axis] / step - window / step + 1).astype(int)\n",
    "    shape.append(window)\n",
    "\n",
    "    # Calculate strides and time vector\n",
    "    strides = list(data.strides)\n",
    "    strides[axis] *= step\n",
    "    strides.append(data.strides[axis])\n",
    "    strided = as_strided(data, shape=shape, strides=strides)\n",
    "    t = np.arange(strided.shape[-2]) * (step / sf)\n",
    "\n",
    "    # Swap axis: n_epochs, ..., n_samples\n",
    "    if strided.ndim > 2:\n",
    "        strided = np.rollaxis(strided, -2, 0)\n",
    "    return t, strided\n",
    "\n",
    "\n",
    "# Convert the EEG data to 30-sec data\n",
    "times, data_win = sliding_window(data[0], sf, window=30)\n",
    "\n",
    "# Convert times to minutes\n",
    "times /= 60\n",
    "\n",
    "\n",
    "def lziv(x):\n",
    "    \"\"\"Binarize the EEG signal and calculate the Lempel-Ziv complexity.\"\"\"\n",
    "    return ant.lziv_complexity(x > x.mean(), normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a plot for manuscript\n",
    "\n",
    "begin = 3.5\n",
    "end = 3.6\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "raw = mne.io.read_raw_edf(f\"{location}/EEG L.edf\", preload=True, verbose=0)\n",
    "raw.pick_types(eeg=True)\n",
    "data = raw._data * 1e6\n",
    "y = data[0]\n",
    "t = np.arange(data.size) / sf / 60 / 60\n",
    "plt.plot(\n",
    "    t[np.where(t == begin)[0][0] : np.where(t == end)[0][0]] * 60,\n",
    "    y[np.where(t == begin)[0][0] : np.where(t == end)[0][0]],\n",
    "    c=\"mediumblue\",\n",
    "    label=\"EEG L\"\n",
    ")\n",
    "\n",
    "raw = mne.io.read_raw_edf(f\"{location}/EEG R.edf\", preload=True, verbose=0)\n",
    "raw.pick_types(eeg=True)\n",
    "data = raw._data * 1e6\n",
    "y = data[0]\n",
    "plt.plot(\n",
    "    t[np.where(t == begin)[0][0] : np.where(t == end)[0][0]] * 60,\n",
    "    y[np.where(t == begin)[0][0] : np.where(t == end)[0][0]] + 130,\n",
    "    c=\"dodgerblue\",\n",
    "    label=\"EEG R\"\n",
    ")\n",
    "plt.xlim([t[np.where(t == begin)[0][0]] * 60, t[np.where(t == end)[0][0]] * 60])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time ($min$)\")\n",
    "plt.ylabel(\"EEG ($mV$)\")\n",
    "plt.title(\"Sleep EEG Wave\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sleep_EEG_wave.svg\")\n",
    "plt.show()\n",
    "\n",
    "# raw.filter(0.5, 45)\n",
    "# data = raw._data * 1e6\n",
    "# y = data[0]\n",
    "# plt.plot(\n",
    "#     t[np.where(t == begin)[0][0] : np.where(t == end)[0][0]] * 60,\n",
    "#     y[np.where(t == begin)[0][0] : np.where(t == end)[0][0]],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_win.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard descriptive statistics\n",
    "hmob, hcomp = ant.hjorth_params(data_win, axis=1)\n",
    "\n",
    "# Feature extraction\n",
    "df_feat = {\n",
    "    # Statistical\n",
    "    \"std\": apply(np.std, arr=data_win, axis=1, ddof=1),\n",
    "    \"mean\": apply(np.mean, arr=data_win, axis=1),\n",
    "    \"median\": apply(np.median, arr=data_win, axis=1),\n",
    "    \"iqr\": apply(sp_stats.iqr, arr=data_win, axis=1, rng=(25, 75)),\n",
    "    \"skew\": apply(sp_stats.skew, arr=data_win, axis=1),\n",
    "    \"kurt\": apply(sp_stats.kurtosis, arr=data_win, axis=1),\n",
    "    \"nzc\": apply(ant.num_zerocross, arr=data_win, axis=1),\n",
    "    \"hmob\": hmob,\n",
    "    \"hcomp\": hcomp,\n",
    "    # Entropy\n",
    "    \"perm_entropy\": apply(ant.perm_entropy, axis=1, arr=data_win, normalize=True),\n",
    "    \"svd_entropy\": apply(ant.svd_entropy, 1, data_win, normalize=True),\n",
    "    \"sample_entropy\": apply(ant.sample_entropy, 1, data_win),\n",
    "    \"app_entropy\": apply(ant.app_entropy, 1, data_win, order=2),\n",
    "    \"spec_entropy\": apply(\n",
    "        ant.spectral_entropy,\n",
    "        1,\n",
    "        data_win,\n",
    "        sf,\n",
    "        normalize=True,\n",
    "        method=\"welch\",\n",
    "        nperseg=50,\n",
    "    ),\n",
    "    \"lziv\": apply(ant.lziv_complexity, 1, data_win),\n",
    "    # Fractal dimension\n",
    "    \"dfa\": apply(ant.detrended_fluctuation, 1, data_win),\n",
    "    \"petrosian\": apply(ant.petrosian_fd, 1, data_win),\n",
    "    \"katz\": apply(ant.katz_fd, 1, data_win),\n",
    "    \"higuchi\": apply(ant.higuchi_fd, 1, data_win),\n",
    "}\n",
    "\n",
    "\n",
    "df_feat = pd.DataFrame(df_feat)\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import simps\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Estimate power spectral density using Welch's method\n",
    "freqs, psd = welch(data_win, sf, nperseg=int(4 * sf))\n",
    "\n",
    "\n",
    "def bandpower_from_psd_ndarray(\n",
    "    psd,\n",
    "    freqs,\n",
    "    bands=[\n",
    "        (0.5, 4, \"Delta\"),\n",
    "        (4, 8, \"Theta\"),\n",
    "        (8, 12, \"Alpha\"),\n",
    "        (12, 16, \"Sigma\"),\n",
    "        (16, 30, \"Beta\"),\n",
    "        (30, 40, \"Gamma\"),\n",
    "    ],\n",
    "    relative=True,\n",
    "):\n",
    "    \"\"\"Compute bandpowers in N-dimensional PSD.\n",
    "    This is a np-only implementation of the :py:func:`yasa.bandpower_from_psd` function,\n",
    "    which supports 1-D arrays of shape (n_freqs), or N-dimensional arays (e.g. 2-D (n_chan,\n",
    "    n_freqs) or 3-D (n_chan, n_epochs, n_freqs))\n",
    "    .. versionadded:: 0.2.0\n",
    "    Parameters\n",
    "    ----------\n",
    "    psd : :py:class:`np.ndarray`\n",
    "        Power spectral density of data, in uV^2/Hz. Must be a N-D array of shape (..., n_freqs).\n",
    "        See :py:func:`scipy.signal.welch` for more details.\n",
    "    freqs : :py:class:`np.ndarray`\n",
    "        Array of frequencies. Must be a 1-D array of shape (n_freqs,)\n",
    "    bands : list of tuples\n",
    "        List of frequency bands of interests. Each tuple must contain the lower and upper\n",
    "        frequencies, as well as the band name (e.g. (0.5, 4, 'Delta')).\n",
    "    relative : boolean\n",
    "        If True, bandpower is divided by the total power between the min and\n",
    "        max frequencies defined in ``band`` (default 0.5 to 40 Hz).\n",
    "    Returns\n",
    "    -------\n",
    "    bandpowers : :py:class:`np.ndarray`\n",
    "        Bandpower array of shape *(n_bands, ...)*.\n",
    "    \"\"\"\n",
    "    # Type checks\n",
    "    assert isinstance(bands, list), \"bands must be a list of tuple(s)\"\n",
    "    assert isinstance(relative, bool), \"relative must be a boolean\"\n",
    "\n",
    "    # Safety checks\n",
    "    freqs = np.asarray(freqs)\n",
    "    psd = np.asarray(psd)\n",
    "    assert freqs.ndim == 1, \"freqs must be a 1-D array of shape (n_freqs,)\"\n",
    "    assert psd.shape[-1] == freqs.shape[-1], \"n_freqs must be last axis of psd\"\n",
    "\n",
    "    # Extract frequencies of interest\n",
    "    all_freqs = np.hstack([[b[0], b[1]] for b in bands])\n",
    "    fmin, fmax = min(all_freqs), max(all_freqs)\n",
    "    idx_good_freq = np.logical_and(freqs >= fmin, freqs <= fmax)\n",
    "    freqs = freqs[idx_good_freq]\n",
    "    res = freqs[1] - freqs[0]\n",
    "\n",
    "    # Trim PSD to frequencies of interest\n",
    "    psd = psd[..., idx_good_freq]\n",
    "\n",
    "    # Check if there are negative values in PSD\n",
    "    if (psd < 0).any():\n",
    "        msg = (\n",
    "            \"There are negative values in PSD. This will result in incorrect \"\n",
    "            \"bandpower values. We highly recommend working with an \"\n",
    "            \"all-positive PSD. For more details, please refer to: \"\n",
    "            \"https://github.com/raphaelvallat/yasa/issues/29\"\n",
    "        )\n",
    "        logger.warning(msg)\n",
    "\n",
    "    # Calculate total power\n",
    "    total_power = simps(psd, dx=res, axis=-1)\n",
    "    total_power = total_power[np.newaxis, ...]\n",
    "\n",
    "    # Initialize empty array\n",
    "    bp = np.zeros((len(bands), *psd.shape[:-1]), dtype=np.float64)\n",
    "\n",
    "    # Enumerate over the frequency bands\n",
    "    labels = []\n",
    "    for i, band in enumerate(bands):\n",
    "        b0, b1, la = band\n",
    "        labels.append(la)\n",
    "        idx_band = np.logical_and(freqs >= b0, freqs <= b1)\n",
    "        bp[i] = simps(psd[..., idx_band], dx=res, axis=-1)\n",
    "\n",
    "    if relative:\n",
    "        bp /= total_power\n",
    "    return bp\n",
    "\n",
    "\n",
    "# Compute bandpowers in N-dimensional PSD\n",
    "bp = bandpower_from_psd_ndarray(psd, freqs)\n",
    "bp = pd.DataFrame(bp.T, columns=[\"delta\", \"theta\", \"alpha\", \"sigma\", \"beta\", \"gamma\"])\n",
    "df_feat = pd.concat([df_feat, bp], axis=1)\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of spectral power\n",
    "df_feat.eval(\"dt = delta / theta\", inplace=True)\n",
    "df_feat.eval(\"da = delta / alpha\", inplace=True)\n",
    "df_feat.eval(\"ds = delta / sigma\", inplace=True)\n",
    "df_feat.eval(\"db = delta / beta\", inplace=True)\n",
    "df_feat.eval(\"dg = delta / gamma\", inplace=True)\n",
    "\n",
    "df_feat.eval(\"td = theta / delta\", inplace=True)\n",
    "df_feat.eval(\"ta = theta / alpha\", inplace=True)\n",
    "df_feat.eval(\"ts = theta / sigma\", inplace=True)\n",
    "df_feat.eval(\"tb = theta / beta\", inplace=True)\n",
    "df_feat.eval(\"tg = theta / gamma\", inplace=True)\n",
    "\n",
    "df_feat.eval(\"ad = alpha / delta\", inplace=True)\n",
    "df_feat.eval(\"at = alpha / theta\", inplace=True)\n",
    "df_feat.eval(\"asi = alpha / sigma\", inplace=True)\n",
    "df_feat.eval(\"ab = alpha / beta\", inplace=True)\n",
    "df_feat.eval(\"ag = alpha / gamma\", inplace=True)\n",
    "\n",
    "df_feat.eval(\"sd = sigma / delta\", inplace=True)\n",
    "df_feat.eval(\"st = sigma / theta\", inplace=True)\n",
    "df_feat.eval(\"sa = sigma / alpha\", inplace=True)\n",
    "df_feat.eval(\"sb = sigma / beta\", inplace=True)\n",
    "df_feat.eval(\"sg = sigma / gamma\", inplace=True)\n",
    "\n",
    "df_feat.eval(\"bd = beta / delta\", inplace=True)\n",
    "df_feat.eval(\"bt = beta / theta\", inplace=True)\n",
    "df_feat.eval(\"ba = beta / alpha\", inplace=True)\n",
    "df_feat.eval(\"bs = beta / sigma\", inplace=True)\n",
    "df_feat.eval(\"bg = beta / gamma\", inplace=True)\n",
    "\n",
    "df_feat.eval(\"gd = gamma / delta\", inplace=True)\n",
    "df_feat.eval(\"gt = gamma / theta\", inplace=True)\n",
    "df_feat.eval(\"ga = gamma / alpha\", inplace=True)\n",
    "df_feat.eval(\"gs = gamma / sigma\", inplace=True)\n",
    "df_feat.eval(\"gb = gamma / beta\", inplace=True)\n",
    "\n",
    "df_feat.eval(\"ta_b = (theta + alpha)/beta\", inplace=True)\n",
    "df_feat.eval(\"ta_ab = (theta + alpha)/(alpha + beta)\", inplace=True)\n",
    "df_feat.eval(\"gb_da = (gamma + beta)/(delta + alpha)\", inplace=True)\n",
    "\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the envelope derivative operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute the envelope derivative operator (EDO), as defined in [1].\n",
    "[1] JM O' Toole, A Temko, NJ Stevenson, “Assessing instantaneous energy in the EEG: a non-negative, frequency-weighted energy operator”, IEEE Int. Conf.  on Eng. in Medicine and Biology, Chicago, August 2014\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def discrete_hilbert(x, DBplot=False):\n",
    "    \"\"\"Discrete Hilbert transform\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        input signal\n",
    "    DBplot: bool, optional\n",
    "        plot or not\n",
    "    Returns\n",
    "    -------\n",
    "    x_hilb : ndarray\n",
    "        Hilbert transform of x\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    Nh = np.ceil(N / 2)\n",
    "    k = np.arange(N)\n",
    "\n",
    "    # build the Hilbert transform in the frequency domain:\n",
    "    H = -1j * np.sign(Nh - k) * np.sign(k)\n",
    "    x_hilb = np.fft.ifft(np.fft.fft(x) * H)\n",
    "    x_hilb = np.real(x_hilb)\n",
    "\n",
    "    if DBplot:\n",
    "        plt.figure(10, clear=True)\n",
    "        plt.plot(np.imag(H))\n",
    "\n",
    "    return x_hilb\n",
    "\n",
    "\n",
    "def edo(x, DBplot=False):\n",
    "    \"\"\"Generate Envelope Derivative Operator (EDO) Γ[x(n)] from simple formula in the time domain:\n",
    "    Γ[x(n)] = y(n)² + H[y(n)]²\n",
    "    where y(n) is the derivative of x(n) using the central-finite method and H[.] is the\n",
    "    Hilbert transform.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        input signal\n",
    "    DBplot: bool, optional\n",
    "        plot or not\n",
    "    Returns\n",
    "    -------\n",
    "    x_edo : ndarray\n",
    "        EDO of x\n",
    "    \"\"\"\n",
    "    # 1. check if odd length and if so make even:\n",
    "    N_start = len(x)\n",
    "    if (N_start % 2) != 0:\n",
    "        x = np.hstack((x, 0))\n",
    "\n",
    "    N = len(x)\n",
    "    nl = np.arange(1, N - 1)\n",
    "    xx = np.zeros(N)\n",
    "\n",
    "    # 2. calculate the Hilbert transform\n",
    "    h = discrete_hilbert(x)\n",
    "\n",
    "    # 3. implement with the central finite difference equation\n",
    "    xx[nl] = (\n",
    "        (x[nl + 1] ** 2) + (x[nl - 1] ** 2) + (h[nl + 1] ** 2) + (h[nl - 1] ** 2)\n",
    "    ) / 4 - ((x[nl + 1] * x[nl - 1] + h[nl + 1] * h[nl - 1]) / 2)\n",
    "\n",
    "    # trim and zero-pad and the ends:\n",
    "    x_edo = np.pad(xx[2 : (len(xx) - 2)], (2, 2), \"constant\", constant_values=(0, 0))\n",
    "\n",
    "    if DBplot:\n",
    "        plt.figure(2, clear=True)\n",
    "        (hl1,) = plt.plot(x, label=\"test signal\")\n",
    "        (hl2,) = plt.plot(x_edo, label=\"EDO\")\n",
    "        plt.legend(handles=[hl1, hl2], loc=\"upper right\")\n",
    "        plt.pause(0.0001)\n",
    "\n",
    "    return x_edo[0:N_start]\n",
    "\n",
    "\n",
    "def test_edo_random():\n",
    "    \"\"\"Test EDO with a random signal\"\"\"\n",
    "\n",
    "    DBplot = True\n",
    "    x = np.random.randn(102)\n",
    "    x_e = edo(x)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # plot\n",
    "    # -------------------------------------------------------------------\n",
    "    if DBplot:\n",
    "        plt.figure(2, clear=True)\n",
    "        (hl1,) = plt.plot(x, label=\"test signal\")\n",
    "        (hl2,) = plt.plot(x_e, label=\"EDO\")\n",
    "        plt.legend(handles=[hl1, hl2], loc=\"upper right\")\n",
    "        plt.pause(0.0001)\n",
    "\n",
    "\n",
    "\"\"\" General_nleo: ''General'' Non-Linear Energy Operator (NLEO) expression: \n",
    "Ψ(n)=x(n-l)x(n-p)-x(n-q)x(n-s) for l+p=q+s  (and [l,p]≠[q,s], otherwise Ψ(n)=0)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def gen_nleo(x, l=1, p=2, q=0, s=3):\n",
    "    \"\"\"general form of the nonlinear energy operator (NLEO)\n",
    "    General NLEO expression: Ψ(n) = x(n-l)x(n-p) - x(n-q)x(n-s)\n",
    "    for l+p=q+s  (and [l,p]≠[q,s], otherwise Ψ(n)=0)\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        input signal\n",
    "    l: int, optional\n",
    "        parameter of NLEO expression (see above)\n",
    "    p: int, optional\n",
    "        parameter of NLEO expression (see above)\n",
    "    q: int, optional\n",
    "        parameter of NLEO expression (see above)\n",
    "    s: int, optional\n",
    "        parameter of NLEO expression (see above)\n",
    "    Returns\n",
    "    -------\n",
    "    x_nleo : ndarray\n",
    "        NLEO array\n",
    "    Example\n",
    "    -------\n",
    "    import np as np\n",
    "    # generate test signal\n",
    "    N = 256\n",
    "    n = np.arange(N)\n",
    "    w1 = np.pi / (N / 32)\n",
    "    ph1 = -np.pi + 2 * np.pi * np.random.rand(1)\n",
    "    a1 = 1.3\n",
    "    x1 = a1 * np.cos(w1 * n + ph1)\n",
    "    # compute instantaneous energy:\n",
    "    x_nleo = gen_nleo(x1, 1, 2, 0, 3)\n",
    "    # plot:\n",
    "    plt.figure(1, clear=True)\n",
    "    plt.plot(x1, '-o', label='test signal')\n",
    "    plt.plot(x_nleo, '-o', label='Agarwal-Gotman')\n",
    "    plt.legend(loc='upper left')\n",
    "    \"\"\"\n",
    "    # check parameters:\n",
    "    if (l + p) != (q + s) and any(np.sort((l, p)) != np.sort((q, s))):\n",
    "        warning(\"Incorrect parameters for NLEO. May be zero!\")\n",
    "\n",
    "    N = len(x)\n",
    "    x_nleo = np.zeros(N)\n",
    "\n",
    "    iedges = abs(l) + abs(p) + abs(q) + abs(s)\n",
    "    n = np.arange(iedges + 1, (N - iedges - 1))\n",
    "\n",
    "    x_nleo[n] = x[n - l] * x[n - p] - x[n - q] * x[n - s]\n",
    "\n",
    "    return x_nleo\n",
    "\n",
    "\n",
    "def nleo(x, type=\"teager\"):\n",
    "    \"\"\"generate different NLEOs based on the same operator\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        input signal\n",
    "    type: {'teager', 'agarwal', 'palmu', 'abs_teager', 'env_only'}\n",
    "        which type of NLEO?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_nleo : ndarray\n",
    "        NLEO array\n",
    "\n",
    "\n",
    "    Additional Notes\n",
    "    ----------------\n",
    "    {'teager': 'Teager-Kaiser', 'agarwal': 'Agarwal-Gotman', 'palmu': 'Palmu et.al.'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def teager():\n",
    "        return gen_nleo(x, 0, 0, 1, -1)\n",
    "\n",
    "    def agarwal():\n",
    "        return gen_nleo(x, 1, 2, 0, 3)\n",
    "\n",
    "    def palmu():\n",
    "        return abs(gen_nleo(x, 1, 2, 0, 3))\n",
    "\n",
    "    def abs_teager():\n",
    "        return abs(gen_nleo(x, 0, 0, 1, -1))\n",
    "\n",
    "    def env_only():\n",
    "        return abs(x) ** 2\n",
    "\n",
    "    def default_nleo():\n",
    "        # -------------------------------------------------------------------\n",
    "        # default option\n",
    "        # -------------------------------------------------------------------\n",
    "        print(\"Invalid NLEO name; defaulting to Teager\")\n",
    "        return teager()\n",
    "\n",
    "    # pick which function to execute\n",
    "    which_nleo = {\n",
    "        \"teager\": teager,\n",
    "        \"agarwal\": agarwal,\n",
    "        \"palmu\": palmu,\n",
    "        \"abs_teager\": abs_teager,\n",
    "        \"env_only\": env_only,\n",
    "    }\n",
    "\n",
    "    def get_nleo(name):\n",
    "        return which_nleo.get(name, default_nleo)()\n",
    "\n",
    "    x_nleo = get_nleo(type)\n",
    "    return x_nleo\n",
    "\n",
    "\n",
    "def test_compare_nleos(x=None, DBplot=True):\n",
    "    \"\"\"test all NLEO variants with 1 signal\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray, optional\n",
    "        input signal (defaults to coloured Gaussian noise)\n",
    "    DBplot: bool\n",
    "        plot or not\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        N = 128\n",
    "        x = np.cumsum(np.random.randn(N))\n",
    "\n",
    "    all_methods = [\"teager\", \"agarwal\", \"palmu\"]\n",
    "    all_methods_strs = {\n",
    "        \"teager\": \"Teager-Kaiser\",\n",
    "        \"agarwal\": \"Agarwal-Gotman\",\n",
    "        \"palmu\": \"Palmu et.al.\",\n",
    "    }\n",
    "    x_nleo = dict.fromkeys(all_methods)\n",
    "\n",
    "    for n in all_methods:\n",
    "        x_nleo[n] = nleo(x, n)\n",
    "\n",
    "    if DBplot:\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=1, num=4, clear=True)\n",
    "        ax[0].plot(x, \"-o\", label=\"test signal\")\n",
    "        for n in all_methods:\n",
    "            ax[1].plot(x_nleo[n], \"-o\", label=all_methods_strs[n])\n",
    "        ax[0].legend(loc=\"upper right\")\n",
    "        ax[1].legend(loc=\"upper left\")\n",
    "        plt.pause(0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_operators_from_signal_ndarray(\n",
    "    x, ops=[\"teager\", \"agarwal\", \"palmu\", \"abs_teager\", \"env_only\", \"edo\"]\n",
    "):\n",
    "    def teager():\n",
    "        return nleo(x, \"teager\")\n",
    "\n",
    "    def agarwal():\n",
    "        return nleo(x, \"agarwal\")\n",
    "\n",
    "    def palmu():\n",
    "        return nleo(x, \"palmu\")\n",
    "\n",
    "    def abs_teager():\n",
    "        return nleo(x, \"abs_teager\")\n",
    "\n",
    "    def env_only():\n",
    "        return nleo(x, \"env_only\")\n",
    "\n",
    "    def edo_f():\n",
    "        return edo(x)\n",
    "\n",
    "    def default_eo():\n",
    "        # -------------------------------------------------------------------\n",
    "        # default option\n",
    "        # -------------------------------------------------------------------\n",
    "        print(\"Invalid EO name; defaulting to Teager\")\n",
    "        return teager()\n",
    "\n",
    "    which_energy_operator = {\n",
    "        \"teager\": teager,\n",
    "        \"agarwal\": agarwal,\n",
    "        \"palmu\": palmu,\n",
    "        \"abs_teager\": abs_teager,\n",
    "        \"env_only\": env_only,\n",
    "        \"edo\": edo_f,\n",
    "    }\n",
    "\n",
    "    def get_energy_operator(name):\n",
    "        return which_energy_operator.get(name, default_eo)()\n",
    "\n",
    "    feat = np.zeros((x.shape[0], len(ops)))\n",
    "\n",
    "    for i, op in enumerate(ops):\n",
    "        x_nleo = get_energy_operator(op)  # Function\n",
    "        feat[:, i] = x_nleo\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "data_win_mean = apply(np.mean, axis=1, arr=data_win)\n",
    "featEnergy = energy_operators_from_signal_ndarray(\n",
    "    data_win_mean, ops=[\"teager\", \"agarwal\", \"palmu\", \"abs_teager\", \"env_only\", \"edo\"]\n",
    ")\n",
    "featEnergy = pd.DataFrame(\n",
    "    featEnergy, columns=[\"teager\", \"agarwal\", \"palmu\", \"abs_teager\", \"env_only\", \"edo\"]\n",
    ")\n",
    "df_feat = pd.concat([df_feat, featEnergy], axis=1)\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write feature object to a comma-separated values (csv) file\n",
    "df_feat.to_csv(f\"feature/{fname} {lr}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature object as a dataframe\n",
    "df_feat = pd.read_csv(f\"feature/{fname} {lr}.csv\", index_col=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets add some new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjorth_activity(x):\n",
    "    \"\"\"Column-wise computation of Hjorth activity (variance).\"\"\"\n",
    "    return np.var(x, axis=0)\n",
    "\n",
    "\n",
    "def hjorth_mobility(x):\n",
    "    \"\"\"Column-wise computation of Hjorth mobility\"\"\"\n",
    "    return np.sqrt(np.var(np.gradient(x, axis=0), axis=0) / np.var(x, axis=0))\n",
    "\n",
    "\n",
    "def hjorth_complexity(x):\n",
    "    \"\"\"Column-wise computation of Hjorth complexity\"\"\"\n",
    "    return hjorth_mobility(np.gradient(x, axis=0)) / hjorth_mobility(x)\n",
    "\n",
    "\n",
    "data = data_cosine(N=1024, A=0.1, sampling=1024, freq=200)\n",
    "hjorth_act = hjorth_activity(data)\n",
    "hjorth_com = hjorth_complexity(data)\n",
    "hjorth_mob = hjorth_mobility(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy (E) of the signal is the sum of the squares of amplitude\n",
    "def energy_fn(x):\n",
    "    x /= np.max(x)\n",
    "    return np.mean(x**2)\n",
    "\n",
    "\n",
    "E = np.apply_along_axis(energy_fn, 1, data_win)\n",
    "df_feat[\"E\"] = E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Domain Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpower_from_psd_ndarray(\n",
    "    psd,\n",
    "    freqs,\n",
    "    bands=[\n",
    "        (0.5, 4, \"Delta\"),\n",
    "        (4, 8, \"Theta\"),\n",
    "        (8, 12, \"Alpha\"),\n",
    "        (12, 16, \"Sigma\"),\n",
    "        (16, 30, \"Beta\"),\n",
    "        (30, 40, \"Gamma\"),\n",
    "    ],\n",
    "    relative=True,\n",
    "):\n",
    "    \"\"\"Compute bandpowers in N-dimensional PSD.\n",
    "    This is a np-only implementation of the :py:func:`yasa.bandpower_from_psd` function,\n",
    "    which supports 1-D arrays of shape (n_freqs), or N-dimensional arays (e.g. 2-D (n_chan,\n",
    "    n_freqs) or 3-D (n_chan, n_epochs, n_freqs))\n",
    "    .. versionadded:: 0.2.0\n",
    "    Parameters\n",
    "    ----------\n",
    "    psd : :py:class:`np.ndarray`\n",
    "        Power spectral density of data, in uV^2/Hz. Must be a N-D array of shape (..., n_freqs).\n",
    "        See :py:func:`scipy.signal.welch` for more details.\n",
    "    freqs : :py:class:`numpy.ndarray`\n",
    "        Array of frequencies. Must be a 1-D array of shape (n_freqs,)\n",
    "    bands : list of tuples\n",
    "        List of frequency bands of interests. Each tuple must contain the lower and upper\n",
    "        frequencies, as well as the band name (e.g. (0.5, 4, 'Delta')).\n",
    "    relative : boolean\n",
    "        If True, bandpower is divided by the total power between the min and\n",
    "        max frequencies defined in ``band`` (default 0.5 to 40 Hz).\n",
    "    Returns\n",
    "    -------\n",
    "    bandpowers : :py:class:`numpy.ndarray`\n",
    "        Bandpower array of shape *(n_bands, ...)*.\n",
    "    \"\"\"\n",
    "    # Type checks\n",
    "    assert isinstance(bands, list), \"bands must be a list of tuple(s)\"\n",
    "    assert isinstance(relative, bool), \"relative must be a boolean\"\n",
    "\n",
    "    # Safety checks\n",
    "    freqs = np.asarray(freqs)\n",
    "    psd = np.asarray(psd)\n",
    "    assert freqs.ndim == 1, \"freqs must be a 1-D array of shape (n_freqs,)\"\n",
    "    assert psd.shape[-1] == freqs.shape[-1], \"n_freqs must be last axis of psd\"\n",
    "\n",
    "    # Extract frequencies of interest\n",
    "    all_freqs = np.hstack([[b[0], b[1]] for b in bands])\n",
    "    fmin, fmax = min(all_freqs), max(all_freqs)\n",
    "    idx_good_freq = np.logical_and(freqs >= fmin, freqs <= fmax)\n",
    "    freqs = freqs[idx_good_freq]\n",
    "    res = freqs[1] - freqs[0]\n",
    "\n",
    "    # Trim PSD to frequencies of interest\n",
    "    psd = psd[..., idx_good_freq]\n",
    "\n",
    "    # Check if there are negative values in PSD\n",
    "    if (psd < 0).any():\n",
    "        msg = (\n",
    "            \"There are negative values in PSD. This will result in incorrect \"\n",
    "            \"bandpower values. We highly recommend working with an \"\n",
    "            \"all-positive PSD. For more details, please refer to: \"\n",
    "            \"https://github.com/raphaelvallat/yasa/issues/29\"\n",
    "        )\n",
    "        logger.warning(msg)\n",
    "\n",
    "    # Calculate total power\n",
    "    total_power = simps(psd, dx=res, axis=-1)\n",
    "    total_power = total_power[np.newaxis, ...]\n",
    "\n",
    "    # Initialize empty array\n",
    "    bp = np.zeros((len(bands), *psd.shape[:-1]), dtype=np.float64)\n",
    "\n",
    "    # Enumerate over the frequency bands\n",
    "    labels = []\n",
    "    for i, band in enumerate(bands):\n",
    "        b0, b1, la = band\n",
    "        labels.append(la)\n",
    "        idx_band = np.logical_and(freqs >= b0, freqs <= b1)\n",
    "        bp[i] = simps(psd[..., idx_band], dx=res, axis=-1)\n",
    "\n",
    "    if relative:\n",
    "        bp /= total_power\n",
    "    return bp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(1, 10):\n",
    "#     plt.plot(freqs, psd[int(x), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import simps\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Estimate power spectral density using Welch's method\n",
    "freqs, psd = welch(data_win, sf, nperseg=int(4 * sf))\n",
    "\n",
    "# Compute features\n",
    "## Compute featrues for normal singal (to compare w/ psd later)\n",
    "hmob, hcomp = ant.hjorth_params(data_win, axis=1)\n",
    "std_nor = np.apply_along_axis(np.std, 1, data_win, ddof=1)\n",
    "mean_nor = np.apply_along_axis(np.mean, 1, data_win)\n",
    "median_nor = np.apply_along_axis(np.median, 1, data_win)\n",
    "iqr_nor = np.apply_along_axis(sp_stats.iqr, 1, data_win, rng=(25, 75))\n",
    "skew_nor = np.apply_along_axis(sp_stats.skew, 1, data_win)\n",
    "kurt_nor = np.apply_along_axis(sp_stats.kurtosis, 1, data_win)\n",
    "hmob_nor = hmob\n",
    "hcomp_nor = hcomp\n",
    "\n",
    "## Compute featrues for PSD\n",
    "hmob, hcomp = ant.hjorth_params(psd, axis=1)\n",
    "std_psd = np.apply_along_axis(np.std, 1, psd, ddof=1)\n",
    "mean_psd = np.apply_along_axis(np.mean, 1, psd)\n",
    "median_psd = np.apply_along_axis(np.median, 1, psd)\n",
    "iqr_psd = np.apply_along_axis(sp_stats.iqr, 1, psd, rng=(25, 75))\n",
    "skew_psd = np.apply_along_axis(sp_stats.skew, 1, psd)\n",
    "kurt_psd = np.apply_along_axis(sp_stats.kurtosis, 1, psd)\n",
    "hmob_psd = hmob\n",
    "hcomp_psd = hcomp\n",
    "\n",
    "# Add features to features dataframe\n",
    "df_feat[\"E\"] = E\n",
    "df_feat[\"std_psd\"] = std_psd\n",
    "df_feat[\"mean_psd\"] = mean_psd\n",
    "df_feat[\"iqr_psd\"] = iqr_psd\n",
    "df_feat[\"skew_psd\"] = skew_psd\n",
    "df_feat[\"kurt_psd\"] = kurt_psd\n",
    "df_feat[\"hmob_psd\"] = hmob_psd\n",
    "df_feat[\"hcomp_psd\"] = hcomp_psd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return v\n",
    "    return v / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(std_psd), label=\"PSD STD\")\n",
    "plt.plot(normalize(std_nor), label=\"Signal STD\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"STD\")\n",
    "plt.title(\"Standard Deviation from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"std.png\", format=\"png\")\n",
    "# plt.savefig(\"std.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(mean_psd), label=\"PSD mean\")\n",
    "plt.plot(normalize(mean_nor), label=\"Signal mean\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"mean.svg\", format=\"svg\")\n",
    "# plt.savefig(\"mean.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(median_psd), label=\"PSD median\")\n",
    "plt.plot(normalize(median_nor), label=\"Signal median\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Median\")\n",
    "plt.title(\"Median from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"median.svg\", format=\"svg\")\n",
    "# plt.savefig(\"median.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(iqr_psd), label=\"PSD iqr\")\n",
    "plt.plot(normalize(iqr_nor), label=\"Signal iqr\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"IQR\")\n",
    "plt.title(\"IQR from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"iqr.svg\", format=\"svg\")\n",
    "# plt.savefig(\"iqr.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(skew_psd), label=\"PSD skew\")\n",
    "plt.plot(normalize(skew_nor), label=\"Signal skew\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Skewness\")\n",
    "plt.title(\"Skewness from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"skew.svg\", format=\"svg\")\n",
    "# plt.savefig(\"skew.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(kurt_psd), label=\"PSD kurt\")\n",
    "plt.plot(normalize(kurt_nor), label=\"Signal kurt\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Kurtosis\")\n",
    "plt.title(\"Kurtosis from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"kurt.svg\", format=\"svg\")\n",
    "# plt.savefig(\"kurt.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(hmob_psd), label=\"PSD hmob\")\n",
    "plt.plot(normalize(hmob_nor), label=\"Signal hmob\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Hjorth Mobility\")\n",
    "plt.title(\"Hjorth Mobility from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"hjorth_mob.svg\", format=\"svg\")\n",
    "# plt.savefig(\"hjorth_mob.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(normalize(hcomp_psd), label=\"PSD hcomp\")\n",
    "plt.plot(normalize(hcomp_nor), label=\"Signal hcomp\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Hjorth Complexity\")\n",
    "plt.title(\"Hjorth Complexity from singal and it's PSD [P8_N3 L]\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"hjorth_comp.svg\", format=\"svg\")\n",
    "# plt.savefig(\"hjorth_comp.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Energy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wavelet_energy(data_set):\n",
    "    \"\"\"\n",
    "    Input : 1 * N vector\n",
    "    Output: Float with the wavelet energy of the input vector,\n",
    "    rounded to 3 decimal places.\n",
    "    \"\"\"\n",
    "    # p_sqr = [i ** 2 for i in data_set]\n",
    "    wavelet_energy = np.nansum(np.log2(np.square(data_set)))\n",
    "    return round(wavelet_energy, 3)\n",
    "\n",
    "\n",
    "wavelet_energy = np.apply_along_axis(calc_wavelet_energy, 1, data_win)\n",
    "\n",
    "# Add features to features dataframe\n",
    "df_feat[\"WEn\"] = wavelet_energy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurst Coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, sys\n",
    "\n",
    "\n",
    "def __to_inc(x):\n",
    "    incs = x[1:] - x[:-1]\n",
    "    return incs\n",
    "\n",
    "\n",
    "def __to_pct(x):\n",
    "    pcts = x[1:] / x[:-1] - 1.0\n",
    "    return pcts\n",
    "\n",
    "\n",
    "def __get_RS(series, kind):\n",
    "    \"\"\"\n",
    "    Get rescaled range (using the range of cumulative sum\n",
    "    of deviations instead of the range of a series as in the simplified version\n",
    "    of R/S) from a time-series of values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        (Time-)series\n",
    "    kind : str\n",
    "        The kind of series (refer to compute_Hc docstring)\n",
    "    \"\"\"\n",
    "\n",
    "    if kind == \"random_walk\":\n",
    "        incs = __to_inc(series)\n",
    "        mean_inc = (series[-1] - series[0]) / len(incs)\n",
    "        deviations = incs - mean_inc\n",
    "        Z = np.cumsum(deviations)\n",
    "        R = max(Z) - min(Z)\n",
    "        S = np.std(incs, ddof=1)\n",
    "\n",
    "    elif kind == \"price\":\n",
    "        incs = __to_pct(series)\n",
    "        mean_inc = np.sum(incs) / len(incs)\n",
    "        deviations = incs - mean_inc\n",
    "        Z = np.cumsum(deviations)\n",
    "        R = max(Z) - min(Z)\n",
    "        S = np.std(incs, ddof=1)\n",
    "\n",
    "    elif kind == \"change\":\n",
    "        incs = series\n",
    "        mean_inc = np.sum(incs) / len(incs)\n",
    "        deviations = incs - mean_inc\n",
    "        Z = np.cumsum(deviations)\n",
    "        R = max(Z) - min(Z)\n",
    "        S = np.std(incs, ddof=1)\n",
    "\n",
    "    if R == 0 or S == 0:\n",
    "        return 0  # return 0 to skip this interval due undefined R/S\n",
    "\n",
    "    return R / S\n",
    "\n",
    "\n",
    "def __get_simplified_RS(series, kind):\n",
    "    \"\"\"\n",
    "    Simplified version of rescaled range\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        (Time-)series\n",
    "    kind : str\n",
    "        The kind of series (refer to compute_Hc docstring)\n",
    "    \"\"\"\n",
    "\n",
    "    if kind == \"random_walk\":\n",
    "        incs = __to_inc(series)\n",
    "        R = max(series) - min(series)  # range in absolute values\n",
    "        S = np.std(incs, ddof=1)\n",
    "    elif kind == \"price\":\n",
    "        pcts = __to_pct(series)\n",
    "        R = max(series) / min(series) - 1.0  # range in percent\n",
    "        S = np.std(pcts, ddof=1)\n",
    "    elif kind == \"change\":\n",
    "        incs = series\n",
    "        _series = np.hstack([[0.0], np.cumsum(incs)])\n",
    "        R = max(_series) - min(_series)  # range in absolute values\n",
    "        S = np.std(incs, ddof=1)\n",
    "\n",
    "    if R == 0 or S == 0:\n",
    "        return 0  # return 0 to skip this interval due the undefined R/S ratio\n",
    "\n",
    "    return R / S\n",
    "\n",
    "\n",
    "def compute_Hc(\n",
    "    series, kind=\"random_walk\", min_window=10, max_window=None, simplified=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute H (Hurst exponent) and C according to Hurst equation:\n",
    "    E(R/S) = c * T^H\n",
    "    Refer to:\n",
    "    https://en.wikipedia.org/wiki/Hurst_exponent\n",
    "    https://en.wikipedia.org/wiki/Rescaled_range\n",
    "    https://en.wikipedia.org/wiki/Random_walk\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        (Time-)series\n",
    "    kind : str\n",
    "        Kind of series\n",
    "        possible values are 'random_walk', 'change' and 'price':\n",
    "        - 'random_walk' means that a series is a random walk with random increments;\n",
    "        - 'price' means that a series is a random walk with random multipliers;\n",
    "        - 'change' means that a series consists of random increments\n",
    "            (thus produced random walk is a cumulative sum of increments);\n",
    "    min_window : int, default 10\n",
    "        the minimal window size for R/S calculation\n",
    "    max_window : int, default is the length of series minus 1\n",
    "        the maximal window size for R/S calculation\n",
    "    simplified : bool, default True\n",
    "        whether to use the simplified or the original version of R/S calculation\n",
    "    Returns tuple of\n",
    "        H, c and data\n",
    "        where H and c — parameters or Hurst equation\n",
    "        and data is a list of 2 lists: time intervals and R/S-values for correspoding time interval\n",
    "        for further plotting log(data[0]) on X and log(data[1]) on Y\n",
    "    \"\"\"\n",
    "\n",
    "    if len(series) < 100:\n",
    "        raise ValueError(\"Series length must be greater or equal to 100\")\n",
    "\n",
    "    ndarray_likes = [np.ndarray]\n",
    "    if \"pandas.core.series\" in sys.modules.keys():\n",
    "        ndarray_likes.append(pd.core.series.Series)\n",
    "\n",
    "    # convert series to np array if series is not np array or pandas Series\n",
    "    if type(series) not in ndarray_likes:\n",
    "        series = np.array(series)\n",
    "\n",
    "    if (\n",
    "        \"pandas.core.series\" in sys.modules.keys()\n",
    "        and type(series) == pd.core.series.Series\n",
    "    ):\n",
    "        if series.isnull().values.any():\n",
    "            raise ValueError(\"Series contains NaNs\")\n",
    "        series = series.values  # convert pandas Series to np array\n",
    "    elif np.isnan(np.min(series)):\n",
    "        raise ValueError(\"Series contains NaNs\")\n",
    "\n",
    "    if simplified:\n",
    "        RS_func = __get_simplified_RS\n",
    "    else:\n",
    "        RS_func = __get_RS\n",
    "\n",
    "    err = np.geterr()\n",
    "    np.seterr(all=\"raise\")\n",
    "\n",
    "    max_window = max_window or len(series) - 1\n",
    "    window_sizes = list(\n",
    "        map(\n",
    "            lambda x: int(10**x),\n",
    "            np.arange(math.log10(min_window), math.log10(max_window), 0.25),\n",
    "        )\n",
    "    )\n",
    "    window_sizes.append(len(series))\n",
    "\n",
    "    RS = []\n",
    "    for w in window_sizes:\n",
    "        rs = []\n",
    "        for start in range(0, len(series), w):\n",
    "            if (start + w) > len(series):\n",
    "                break\n",
    "            _ = RS_func(series[start : start + w], kind)\n",
    "            if _ != 0:\n",
    "                rs.append(_)\n",
    "        RS.append(np.mean(rs))\n",
    "\n",
    "    A = np.vstack([np.log10(window_sizes), np.ones(len(RS))]).T\n",
    "    H, c = np.linalg.lstsq(A, np.log10(RS), rcond=-1)[0]\n",
    "    np.seterr(**err)\n",
    "\n",
    "    c = 10**c\n",
    "    return H, c  # , [window_sizes, RS]\n",
    "\n",
    "\n",
    "# H, c, [window_sizes, RS] = compute_Hc(data_win[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hurst_coeffs = np.apply_along_axis(compute_Hc, 1, data_win, kind=\"random_walk\")\n",
    "Hurst_H1 = Hurst_coeffs[:, 0]\n",
    "Hurst_C1 = Hurst_coeffs[:, 1]\n",
    "Hurst_coeffs = np.apply_along_axis(compute_Hc, 1, data_win, kind=\"change\")\n",
    "Hurst_H2 = Hurst_coeffs[:, 0]\n",
    "Hurst_C2 = Hurst_coeffs[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "from scipy.stats import iqr as IQR\n",
    "\n",
    "\n",
    "class Outlier:\n",
    "    \"\"\"\n",
    "    Find outlier in a numerical dataset with two different methods:\n",
    "        - `sd_outlier`: z-score based method\n",
    "        - `IQR_outlier`: IQR based method\n",
    "    Also allows to remove/filter-out the detected outliers with `filter` method.\n",
    "    `plot` method allows you to plot the original and filtered dataset and inspect the performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x=None):\n",
    "        self.x = x\n",
    "        self.outliers = None\n",
    "        self.outliersIndices = np.array([])\n",
    "        self.x_filt = None\n",
    "\n",
    "    def sd_outlier(self=None, x=None, axis=None, bar=3, side=\"both\"):\n",
    "        \"\"\"\n",
    "        z-score based method\n",
    "        This method will test if the numbers falls outside the three standard deviations.\n",
    "        Based on this rule, if the value is outlier, the method will return true, if not, return false.\n",
    "        \"\"\"\n",
    "\n",
    "        assert side in [\"gt\", \"lt\", \"both\"], \"Side should be `gt`, `lt` or `both`.\"\n",
    "\n",
    "        if (x is None) and (self.x is not None):\n",
    "            x = self.x\n",
    "        elif (x is None) and (self.x is None):\n",
    "            raise ValueError(\"Enter x input!\")\n",
    "\n",
    "        d_z = stat.zscore(x, axis=axis)\n",
    "\n",
    "        if side == \"gt\":\n",
    "            self.outliers = d_z > bar\n",
    "            return d_z > bar\n",
    "        elif side == \"lt\":\n",
    "            self.outliers = d_z < -bar\n",
    "            return d_z < -bar\n",
    "        elif side == \"both\":\n",
    "            self.outliers = np.abs(d_z) > bar\n",
    "            return np.abs(d_z) > bar\n",
    "\n",
    "    def __Q1(self, x, axis=None):\n",
    "        if (x is None) and (self.x is not None):\n",
    "            x = self.x\n",
    "        elif (x is None) and (self.x is None):\n",
    "            raise ValueError(\"Enter x input!\")\n",
    "\n",
    "        return np.percentile(x, 25, axis=axis)\n",
    "\n",
    "    def __Q3(self, x, axis=None):\n",
    "        if (x is None) and (self.x is not None):\n",
    "            x = self.x\n",
    "        elif (x is None) and (self.x is None):\n",
    "            raise ValueError(\"Enter x input!\")\n",
    "\n",
    "        return np.percentile(x, 75, axis=axis)\n",
    "\n",
    "    def IQR_outlier(self, x=None, axis=None, bar=1.5, side=\"both\"):\n",
    "        \"\"\"\n",
    "        IQR based method\n",
    "        This method will test if the value is less than q1 - 1.5 * iqr or\n",
    "        greater than q3 + 1.5 * iqr.\n",
    "        \"\"\"\n",
    "        self.method = \"IQR_outlier\"\n",
    "\n",
    "        assert side in [\"gt\", \"lt\", \"both\"], \"Side should be `gt`, `lt` or `both`.\"\n",
    "\n",
    "        if (x is None) and (self.x is not None):\n",
    "            x = self.x\n",
    "        elif (x is None) and (self.x is None):\n",
    "            raise ValueError(\"Enter x input!\")\n",
    "\n",
    "        d_IQR = IQR(x, axis=axis)\n",
    "        d_Q1 = self.__Q1(x, axis=axis)\n",
    "        d_Q3 = self.__Q3(x, axis=axis)\n",
    "        IQR_distance = np.multiply(d_IQR, bar)\n",
    "\n",
    "        stat_shape = list(x.shape)\n",
    "\n",
    "        if isinstance(axis, collections.Iterable):\n",
    "            for single_axis in axis:\n",
    "                stat_shape[single_axis] = 1\n",
    "        else:\n",
    "            stat_shape[axis] = 1\n",
    "\n",
    "        if side in [\"gt\", \"both\"]:\n",
    "            upper_range = d_Q3 + IQR_distance\n",
    "            upper_outlier = np.greater(x - upper_range.reshape(stat_shape), 0)\n",
    "        if side in [\"lt\", \"both\"]:\n",
    "            lower_range = d_Q1 - IQR_distance\n",
    "            lower_outlier = np.less(x - lower_range.reshape(stat_shape), 0)\n",
    "\n",
    "        if side == \"gt\":\n",
    "            self.outliers = upper_outlier\n",
    "            return upper_outlier\n",
    "        if side == \"lt\":\n",
    "            self.outliers = lower_outlier\n",
    "            return lower_outlier\n",
    "        if side == \"both\":\n",
    "            self.outliers = np.logical_or(upper_outlier, lower_outlier)\n",
    "            return np.logical_or(upper_outlier, lower_outlier)\n",
    "\n",
    "    def filter(self, x=None):\n",
    "        if (x is None) and (self.x is not None):\n",
    "            x = self.x\n",
    "        elif (x is None) and (self.x is None):\n",
    "            raise ValueError(\"Enter x input!\")\n",
    "\n",
    "        self.outliersIndices = np.where(self.outliers == True)\n",
    "        print(f\"Outliers are detected in {len(self.outliersIndices[0])} points.\")\n",
    "        self.x_filt = np.copy(x)\n",
    "        self.x_filt[self.outliersIndices] = np.mean(x[~self.outliers])\n",
    "        return self.x_filt, self.outliersIndices[0]\n",
    "\n",
    "    def plot(self, plot_original=False):\n",
    "        # Plot the signal and detected outliers\n",
    "        plt.figure()\n",
    "\n",
    "        if plot_original:\n",
    "            # plt.plot(np.asarray(self.x), \"ok\", label=\"Orginal Signal\")\n",
    "            plt.plot(np.asarray(self.x), \"-k\", linewidth=7, label=\"Orginal Signal\")\n",
    "\n",
    "        for outlier in self.outliersIndices[0]:\n",
    "            plt.axvline(outlier, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=4)\n",
    "\n",
    "        if plot_original:\n",
    "            plt.plot(filtered, \"-\", c=\"cyan\", linewidth=1, label=\"Filtered Signal\")\n",
    "        else:\n",
    "            plt.plot(filtered, \"-\", c=\"blue\", linewidth=1, label=\"Filtered Signal\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Test\n",
    "# Finally, if you want to filter out the outliers, use a numpy selector.\n",
    "x = [2, 3, 1, 4, 2, 3, 4, 5, 2, 3, 3, 3, 3, 4, 3, 2, 50, 60]  # data\n",
    "outlier = Outlier(np.asarray(x))\n",
    "outlier.IQR_outlier(axis=0, bar=1.5, side=\"both\")\n",
    "filtered, outlierIndices = outlier.filter()\n",
    "outlier.plot(plot_original=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and remove outliers from Hurst coefficients\n",
    "outlier = Outlier(np.asarray(Hurst_H1))\n",
    "outlier.IQR_outlier(axis=0, bar=1.5, side=\"both\")\n",
    "filtered, outlierIndices = outlier.filter()\n",
    "outlier.plot(plot_original=True)\n",
    "Hurst_H1 = filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and remove outliers from Hurst coefficients\n",
    "outlier = Outlier(np.asarray(Hurst_C1))\n",
    "outlier.IQR_outlier(axis=0, bar=1.5, side=\"both\")\n",
    "filtered, outlierIndices = outlier.filter()\n",
    "outlier.plot(plot_original=True)\n",
    "Hurst_C1 = filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and remove outliers from Hurst coefficients\n",
    "outlier = Outlier(np.asarray(Hurst_C2))\n",
    "outlier.IQR_outlier(axis=0, bar=1.5, side=\"both\")\n",
    "filtered, outlierIndices = outlier.filter()\n",
    "outlier.plot(plot_original=True)\n",
    "Hurst_C2 = filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features to features dataframe\n",
    "df_feat[\"Hurst_H1\"] = Hurst_H1\n",
    "df_feat[\"Hurst_H2\"] = Hurst_H2\n",
    "df_feat[\"Hurst_C1\"] = Hurst_C1\n",
    "df_feat[\"Hurst_C2\"] = Hurst_C2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_seq(time_series, tau, embedding_dimension):\n",
    "    \"\"\"Build a set of embedding sequences from given time series `time_series`\n",
    "    with lag `tau` and embedding dimension `embedding_dimension`.\n",
    "    Let time_series = [x(1), x(2), ... , x(N)], then for each i such that\n",
    "    1 < i <  N - (embedding_dimension - 1) * tau,\n",
    "    we build an embedding sequence,\n",
    "    Y(i) = [x(i), x(i + tau), ... , x(i + (embedding_dimension - 1) * tau)].\n",
    "    All embedding sequences are placed in a matrix Y.\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series\n",
    "        list or numpy.ndarray\n",
    "        a time series\n",
    "    tau\n",
    "        integer\n",
    "        the lag or delay when building embedding sequence\n",
    "    embedding_dimension\n",
    "        integer\n",
    "        the embedding dimension\n",
    "    Returns\n",
    "    -------\n",
    "    Y\n",
    "        2-embedding_dimension list\n",
    "        embedding matrix built\n",
    "    Examples\n",
    "    ---------------\n",
    "    >>> import pyeeg\n",
    "    >>> a=range(0,9)\n",
    "    >>> pyeeg.embed_seq(a,1,4)\n",
    "    array([[0,  1,  2,  3],\n",
    "           [1,  2,  3,  4],\n",
    "           [2,  3,  4,  5],\n",
    "           [3,  4,  5,  6],\n",
    "           [4,  5,  6,  7],\n",
    "           [5,  6,  7,  8]])\n",
    "    >>> pyeeg.embed_seq(a,2,3)\n",
    "    array([[0,  2,  4],\n",
    "           [1,  3,  5],\n",
    "           [2,  4,  6],\n",
    "           [3,  5,  7],\n",
    "           [4,  6,  8]])\n",
    "    >>> pyeeg.embed_seq(a,4,1)\n",
    "    array([[0],\n",
    "           [1],\n",
    "           [2],\n",
    "           [3],\n",
    "           [4],\n",
    "           [5],\n",
    "           [6],\n",
    "           [7],\n",
    "           [8]])\n",
    "    \"\"\"\n",
    "    if not type(time_series) == np.ndarray:\n",
    "        typed_time_series = np.asarray(time_series)\n",
    "    else:\n",
    "        typed_time_series = time_series\n",
    "\n",
    "    shape = (\n",
    "        typed_time_series.size - tau * (embedding_dimension - 1),\n",
    "        embedding_dimension,\n",
    "    )\n",
    "\n",
    "    strides = (typed_time_series.itemsize, tau * typed_time_series.itemsize)\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        typed_time_series, shape=shape, strides=strides\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Distance (MD) and Central Tendency Measure (CTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_and_ctm(X, Y):\n",
    "    # features = pd.DataFrame(columns=['radius','mean_distance','central_tendency_measure'])\n",
    "    r = 0.5\n",
    "    d = [math.sqrt(X[i] * X[i] + Y[i] * Y[i]) for i in range(0, len(X))]\n",
    "    delta = [1 if i < r else 0 for i in d]\n",
    "    d = [i for i in d if i < r]\n",
    "\n",
    "    ctm = np.sum(delta[:-2]) / (len(delta) - 2)\n",
    "    mean_distance = np.mean(d)\n",
    "\n",
    "    # features.loc[0] = [r] + [ctm] + [mean_distance]\n",
    "    return r, ctm, mean_distance\n",
    "\n",
    "\n",
    "#### Example\n",
    "y = np.random.randn(7680) * 10 + 100\n",
    "# Second Order Difference Plot (SODP)\n",
    "upper_quartile = np.percentile(y, 80)\n",
    "lower_quartile = np.percentile(y, 20)\n",
    "IQR = (upper_quartile - lower_quartile) * 1.5\n",
    "quartileSet = (lower_quartile - IQR, upper_quartile + IQR)\n",
    "y = y[np.where((y >= quartileSet[0]) & (y <= quartileSet[1]))]\n",
    "# Plotting SODP\n",
    "X = np.subtract(y[1:], y[0:-1])  # x(n+1)-x(n)\n",
    "Y = np.subtract(y[2:], y[0:-2]).tolist()  # x(n+2)-x(n-1)\n",
    "Y.extend([0])\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(X, Y, \".\")\n",
    "# Calculate MD and CTM\n",
    "_, mean_distance, central_tendency_measure = calc_mean_and_ctm(X, Y)\n",
    "\n",
    "\n",
    "def mean_ctm_wrapper(x):\n",
    "    \"\"\"\n",
    "    A wrapper function for calc_mean_and_ctm().\n",
    "    This function calculates mean and central tendancy measure for a given time series `x`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : :py:class:`np.ndarray`\n",
    "        Array of time series data. Must be a 1-D array of shape `(dataPoints,)`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of `mean_distance` and `central_tendency_measure`\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        >>> y = np.random.randn(7680)*10 + 100\n",
    "        >>> md, ctm = mean_ctm_wrapper(y)\n",
    "        (0.054281767955801107, 0.33950566436214885)\n",
    "    \"\"\"\n",
    "    upper_quartile = np.percentile(x, 80)\n",
    "    lower_quartile = np.percentile(x, 20)\n",
    "    IQR = (upper_quartile - lower_quartile) * 1.5\n",
    "    quartileSet = (lower_quartile - IQR, upper_quartile + IQR)\n",
    "    x = x[np.where((x >= quartileSet[0]) & (x <= quartileSet[1]))]\n",
    "    # plotting SODP\n",
    "    X = np.subtract(x[1:], x[0:-1])  # x(n+1)-x(n)\n",
    "    Y = np.subtract(x[2:], x[0:-2]).tolist()  # x(n+2)-x(n-1)\n",
    "    Y.extend([0])\n",
    "    # calculate MD and CTM\n",
    "    _, mean_distance, central_tendency_measure = calc_mean_and_ctm(X, Y)\n",
    "    return mean_distance, central_tendency_measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "mean_ctm = np.apply_along_axis(mean_ctm_wrapper, 1, arr=data_win)\n",
    "df_feat[\"mean_distance\"] = mean_ctm[:, 0]\n",
    "df_feat[\"central_tendency_measure\"] = mean_ctm[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# coding: UTF-8\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def rec_plot(s, eps=0.10, steps=10):\n",
    "    d = pdist(s[:, None])\n",
    "    d = np.floor(d / eps)\n",
    "    d[d > steps] = steps\n",
    "    Z = squareform(d)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def moving_average(s, r=5):\n",
    "    return np.convolve(s, np.ones((r,)) / r, mode=\"valid\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate singal\n",
    "    N = 200\n",
    "    eps = 0.1\n",
    "    steps = 10\n",
    "\n",
    "    # Plot normal dist filtered with moving average\n",
    "    rn = np.random.normal(size=N)\n",
    "    rn_filtered = moving_average(rn)\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(rn_filtered)\n",
    "    plt.title(\"Normal\")\n",
    "    plt.subplot(212)\n",
    "    plt.imshow(rec_plot(rn_filtered, eps=eps, steps=steps))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsallis and Renyi Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Counter(Counter):\n",
    "    def prob(self):\n",
    "        return np.array(list(self.values()))\n",
    "\n",
    "\n",
    "def symbols_to_prob(symbols):\n",
    "    \"\"\"\n",
    "    Return a dict mapping symbols to  probability.\n",
    "    input:\n",
    "    -----\n",
    "        symbols:     iterable of hashable items\n",
    "                     works well if symbols is a zip of iterables\n",
    "    \"\"\"\n",
    "    myCounter = Counter(symbols)\n",
    "\n",
    "    N = float(len(list(symbols)))  # symbols might be a zip object in python 3\n",
    "\n",
    "    for k in myCounter:\n",
    "        myCounter[k] /= N\n",
    "\n",
    "    return myCounter\n",
    "\n",
    "\n",
    "def entropy(data=None, prob=None, tol=1e-5):\n",
    "    \"\"\"\n",
    "    given a probability distribution (prob) or an interable of symbols (data) compute and\n",
    "    return its entropy\n",
    "    inputs:\n",
    "    ------\n",
    "        data:       iterable of symbols\n",
    "        prob:       iterable with probabilities\n",
    "        tol:        if prob is given, 'entropy' checks that the sum is about 1.\n",
    "                    It raises an error if abs(sum(prob)-1) >= tol\n",
    "    \"\"\"\n",
    "\n",
    "    if prob is None and data is None:\n",
    "        raise ValueError(\n",
    "            \"%s.entropy requires either 'prob' or 'data' to be defined\" % __name__\n",
    "        )\n",
    "\n",
    "    if prob is not None and data is not None:\n",
    "        raise ValueError(\n",
    "            \"%s.entropy requires only 'prob' or 'data to be given but not both\"\n",
    "            % __name__\n",
    "        )\n",
    "\n",
    "    if prob is not None and not isinstance(prob, np.ndarray):\n",
    "        raise TypeError(\"'entropy' in '%s' needs 'prob' to be an ndarray\" % __name__)\n",
    "\n",
    "    if prob is not None and abs(prob.sum() - 1) > tol:\n",
    "        raise ValueError(\"parameter 'prob' in '%s.entropy' should sum to 1\" % __name__)\n",
    "\n",
    "    if data is not None:\n",
    "        prob = symbols_to_prob(data).prob()\n",
    "\n",
    "    # compute the log2 of the probability and change any -inf by 0s\n",
    "    logProb = np.log2(prob)\n",
    "    logProb[logProb == -np.inf] = 0\n",
    "\n",
    "    # return dot product of logProb and prob\n",
    "    return -float(np.dot(prob, logProb))\n",
    "\n",
    "\n",
    "def renyi(data=None, a=2):\n",
    "    if data is not None:\n",
    "        prob = symbols_to_prob(data).prob()\n",
    "\n",
    "    # compute the log2 of the probability and change any -inf by 0s\n",
    "    powerProb = prob ** int(a)\n",
    "    logProb = np.log(powerProb)\n",
    "    # return dot product of logProb and prob\n",
    "    return -(a / (1 - a)) * (np.sum(logProb))\n",
    "\n",
    "\n",
    "def tsallis(data=None, q=2):\n",
    "    if data is not None:\n",
    "        prob = symbols_to_prob(data).prob()\n",
    "\n",
    "    # compute the log2 of the probability and change any -inf by 0s\n",
    "    powerProb = prob ** int(q)\n",
    "    # return dot product of logProb and prob\n",
    "    return (1 / (q - 1)) * (1 - np.sum(powerProb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "data_win_rnd3 = np.around(data_win, decimals=3)\n",
    "tsallisEnt = np.apply_along_axis(tsallis, 1, arr=data_win_rnd3)\n",
    "df_feat[\"tsallisEnt\"] = tsallisEnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a figure compraing tsallis and shannon entropy\n",
    "Ent = np.apply_along_axis(entropy, 1, arr=data_win_rnd3)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(\n",
    "    moving_average(normalize(tsallisEnt - np.mean(tsallisEnt)), 2),\n",
    "    label=\"tsallis entropy\",\n",
    "    color=\"black\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.plot(\n",
    "    moving_average(normalize(Ent - np.mean(Ent)), 2),\n",
    "    label=\"shannon entropy\",\n",
    "    color=\"lightgreen\",\n",
    "    linewidth=0.8,\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Compraing Tsallis and Shannon Entropies\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tsallis_compare.svg\", format=\"svg\")\n",
    "plt.savefig(\"tsallis_compare.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "data_win_rnd3 = np.around(data_win, decimals=3)\n",
    "renyiEnt = np.apply_along_axis(renyi, 1, arr=data_win_rnd3)\n",
    "df_feat[\"renyi\"] = renyiEnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a figure compraing renyi and shannon entropy\n",
    "Ent = np.apply_along_axis(entropy, 1, arr=data_win_rnd3)\n",
    "renyiEnt = -renyiEnt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(\n",
    "    moving_average(normalize(renyiEnt - np.mean(renyiEnt)), 2),\n",
    "    label=\"renyi entropy\",\n",
    "    color=\"black\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.plot(\n",
    "    moving_average(normalize(Ent - np.mean(Ent)), 2),\n",
    "    label=\"shannon entropy\",\n",
    "    color=\"cyan\",\n",
    "    linewidth=1,\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Compraing Renyi and Shannon Entropies\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"renyi_compare.svg\", format=\"svg\")\n",
    "plt.savefig(\"renyi_compare.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bubble Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manis and Sassi, “A Python Library with Fast Algorithms for Popular Entropy Definitions.”\n",
    "\n",
    "from numpy import histogram, log\n",
    "\n",
    "\n",
    "def bubble_count(x):\n",
    "    \"\"\"\n",
    "    counts the number of swaps when sorting\n",
    "    :param x: the input vector\n",
    "    :return: the total number of swaps\n",
    "    \"\"\"\n",
    "    y = 0\n",
    "    for i in range(len(x) - 1, 0, -1):\n",
    "        for j in range(i):\n",
    "            if x[j] > x[j + 1]:\n",
    "                x[j], x[j + 1] = x[j + 1], x[j]\n",
    "                y += 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def complexity_count_fast(x, m):\n",
    "    \"\"\"\n",
    "    :param x: the input series\n",
    "    :param m: the dimension of the space\n",
    "    :return: the series of complexities for total number of swaps\n",
    "    \"\"\"\n",
    "\n",
    "    if len(x) < m:\n",
    "        return []\n",
    "\n",
    "    y = [bubble_count(x[:m])]\n",
    "    v = sorted(x[:m])\n",
    "\n",
    "    for i in range(m, len(x)):\n",
    "        steps = y[i - m]\n",
    "        steps -= v.index(x[i - m])\n",
    "        v.pop(v.index(x[i - m]))\n",
    "        v.append(x[i])\n",
    "        j = m - 1\n",
    "        while j > 0 and v[j] < v[j - 1]:\n",
    "            v[j], v[j - 1] = v[j - 1], v[j]\n",
    "            steps += 1\n",
    "            j -= 1\n",
    "        y.append(steps)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def renyi_int(data):\n",
    "    \"\"\"\n",
    "    returns renyi entropy (order 2) of an integer series and bin_size=1\n",
    "    (specified for the needs of bubble entropy)\n",
    "    :param data: the input series\n",
    "    :return: metric\n",
    "    \"\"\"\n",
    "    counter = [0] * (max(data) + 1)\n",
    "    for x in data:\n",
    "        counter[x] += 1\n",
    "    r = 0\n",
    "    for c in counter:\n",
    "        p = c / len(data)\n",
    "        r += p * p\n",
    "    return -log(r)\n",
    "\n",
    "\n",
    "def bubble_entropy(x, m=10):\n",
    "    \"\"\"\n",
    "    computes bubble entropy following the definition\n",
    "    :param x: the input signal\n",
    "    :param m: the dimension of the embedding space\n",
    "    :return: metric\n",
    "    \"\"\"\n",
    "    complexity = complexity_count_fast(x, m)\n",
    "    B = renyi_int(complexity) / log(1 + m * (m - 1) / 2)\n",
    "\n",
    "    complexity = complexity_count_fast(x, m + 1)\n",
    "    A = renyi_int(complexity) / log(1 + (m + 1) * m / 2)\n",
    "\n",
    "    return A - B\n",
    "\n",
    "\n",
    "def bubble_entropy_2(x, m=10):\n",
    "    \"\"\"\n",
    "    computes bubble entropy following the definition\n",
    "    :param x: the input signal\n",
    "    :param m: the dimension of the embedding space\n",
    "    :return: metric\n",
    "    \"\"\"\n",
    "    complexity = complexity_count_fast(x, m)\n",
    "    B = renyi_int(complexity) / log(1 + m * (m - 1) / 2)\n",
    "\n",
    "    complexity = complexity_count_fast(x, m + 2)\n",
    "    A = renyi_int(complexity) / log(1 + (m + 2) * (m + 1) / 2)\n",
    "\n",
    "    return A - B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "data_win_rnd3 = np.around(data_win, decimals=3)\n",
    "bubbleEnt1 = np.apply_along_axis(bubble_entropy, 1, arr=data_win_rnd3)\n",
    "df_feat[\"bubbleEnt1\"] = bubbleEnt1\n",
    "\n",
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "data_win_rnd3 = np.around(data_win, decimals=3)\n",
    "bubbleEnt2 = np.apply_along_axis(bubble_entropy_2, 1, arr=data_win_rnd3)\n",
    "df_feat[\"bubbleEnt2\"] = bubbleEnt2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    moving_average(normalize(bubbleEnt1 - np.mean(bubbleEnt1)), 2),\n",
    "    label=\"Bubble entropy 1\",\n",
    "    color=\"dodgerblue\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "fig.suptitle(\"Compraing Bubble Entropy 1 and 2 (Normalized)\")\n",
    "plt.plot(\n",
    "    moving_average(normalize(bubbleEnt2 - np.mean(bubbleEnt2)), 2),\n",
    "    label=\"Bubble entropy 2\",\n",
    "    color=\"darkorchid\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bubble_entropy.svg\", format=\"svg\")\n",
    "plt.savefig(\"bubble_entropy.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import differential_entropy\n",
    "\n",
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "data_win_rnd3 = np.around(data_win, decimals=3)\n",
    "diffEnt = np.apply_along_axis(differential_entropy, 1, arr=data_win_rnd3)\n",
    "diffEntMean = np.mean(diffEnt[~(diffEnt == -np.inf)])\n",
    "diffEnt[diffEnt == -np.inf] = diffEntMean\n",
    "df_feat[\"diffEnt\"] = diffEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    moving_average(normalize(diffEnt - np.mean(diffEnt)), 2),\n",
    "    label=\"Differential Entropy\",\n",
    "    color=\"darkgreen\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "plt.title(\"Differential Entropy (Normalized)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"differential_entropy.svg\", format=\"svg\")\n",
    "plt.savefig(\"differential_entropy.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell took ~11 min to execute\n",
    "\n",
    "# pip install EntropyHub\n",
    "import EntropyHub as enth\n",
    "\n",
    "\n",
    "def fuzzEnt_f(x, m=1, tau=1):\n",
    "    \"\"\"\n",
    "    A wrapper function for EntropyHub.FuzzEn() Function\n",
    "\n",
    "    Input\n",
    "    ------\n",
    "    `sig`: Time series signal, a vector of length > 10.\n",
    "    `m`: Embedding dimension, a positive integer (for embbeding dim).\n",
    "    `tau`: Time delay, a positive integer (for embbeding dim).\n",
    "    `Fx`: Type of fuzzy function for distance transformation, one of the following strings\n",
    "    Return\n",
    "    ------\n",
    "    `Fuzz`: Fuzzy entropy estimates for each embedding dimension 1:m.\n",
    "    `Ps1`: The average fuzzy distances for embedding dimensions 1:m.\n",
    "    `Ps2`: The average fuzzy distances for embedding dimensions 2:m+1.\n",
    "    Example\n",
    "    -------\n",
    "    >>> [Fuzz, Ps1, Ps2] = enth.FuzzEn(x, m=1, tau=1)\n",
    "    Source\n",
    "    ------\n",
    "    https://github.com/MattWillFlood/EntropyHub/blob/main/Guide/EntropyHub%20Guide.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    [Fuzz, Ps1, Ps2] = enth.FuzzEn(x, m=m, tau=tau)\n",
    "    return Fuzz[0]\n",
    "\n",
    "\n",
    "# Calculate feature for all epochs. Then add them to FeaturesDataFrame\n",
    "data_win_rnd3 = np.around(data_win, decimals=3)\n",
    "fuzzEnt = np.apply_along_axis(fuzzEnt_f, 1, arr=data_win_rnd3)\n",
    "df_feat[\"fuzzEnt\"] = fuzzEnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    moving_average(normalize(fuzzEnt - np.mean(fuzzEnt)), 2),\n",
    "    label=\"Fuzzy Entropy\",\n",
    "    color=\"mediumblue\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "plt.title(\"Fuzzy Entropy (Normalized)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fuzzy_entropy.svg\", format=\"svg\")\n",
    "plt.savefig(\"fuzzy_entropy.png\", format=\"png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write feature object to a comma-separated values (csv) file\n",
    "df_feat.to_csv(f\"feature/{fname} {lr}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Load hypnogram\n",
    "location_hypno = \"/Users/amirhosseindaraie/Desktop/data/synced-hypnos\"\n",
    "hypno_30s = np.loadtxt(f\"{location_hypno}/p8n3_synced.txt\")[:, 0]\n",
    "\n",
    "# Extract sorted F-values\n",
    "# Compute the ANOVA F-value for the provided sample.\n",
    "fvals = pd.Series(\n",
    "    f_classif(X=df_feat, y=hypno_30s)[0], index=df_feat.columns\n",
    ").sort_values()\n",
    "\n",
    "# Plot features ranking\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.barplot(y=fvals.index, x=fvals, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"F-values\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hypnogram and a feature\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "hypno = pd.Series(hypno_30s).map({-1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1}).values\n",
    "hypno_rem = np.ma.masked_not_equal(hypno, 1)\n",
    "\n",
    "# Plot the hypnogram\n",
    "ax1.step(times, -1 * hypno, color=\"k\", lw=1.5)\n",
    "ax1.step(times, -1 * hypno_rem, color=\"r\", lw=2.5)\n",
    "ax1.set_yticks([0, -1, -2, -3, -4])\n",
    "ax1.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "ax1.set_ylim(-4.5, 0.5)\n",
    "ax1.set_ylabel(\"Sleep stage\")\n",
    "\n",
    "# Plot the non-linear feature\n",
    "ax2.plot(times, df_feat[\"perm_entropy\"])\n",
    "ax2.set_ylabel(\"Permutation Entropy\")\n",
    "# ax2.set_ylabel('Higuchi Fractal Dimension')\n",
    "ax2.set_xlabel(\"Time [minutes]\")\n",
    "\n",
    "ax2.set_xlim(0, times[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_feature_detect(data, threshold=0.98):\n",
    "    \"\"\"detect features that show the same value for the\n",
    "    majority/all of the observations (constant/quasi-constant features)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Dataframe\n",
    "    threshold : threshold to identify the variable as constant\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of variables names\n",
    "    \"\"\"\n",
    "\n",
    "    data_copy = data.copy(deep=True)\n",
    "    quasi_constant_feature = []\n",
    "    for feature in data_copy.columns:\n",
    "        predominant = (\n",
    "            (data_copy[feature].value_counts() / np.float(len(data_copy)))\n",
    "            .sort_values(ascending=False)\n",
    "            .values[0]\n",
    "        )\n",
    "        if predominant >= threshold:\n",
    "            quasi_constant_feature.append(feature)\n",
    "    print(len(quasi_constant_feature), \" variables are found to be almost constant\")\n",
    "    return quasi_constant_feature\n",
    "\n",
    "\n",
    "def corr_feature_detect(data, threshold=0.8):\n",
    "    \"\"\"detect highly-correlated features of a Dataframe\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Dataframe\n",
    "    threshold : threshold to identify the variable correlated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pairs of correlated variables\n",
    "    \"\"\"\n",
    "\n",
    "    corrmat = data.corr()\n",
    "    corrmat = corrmat.abs().unstack()  # absolute value of corr coef\n",
    "    corrmat = corrmat.sort_values(ascending=False)\n",
    "    corrmat = corrmat[corrmat >= threshold]\n",
    "    corrmat = corrmat[corrmat < 1]  # remove the digonal\n",
    "    corrmat = pd.DataFrame(corrmat).reset_index()\n",
    "    corrmat.columns = [\"feature1\", \"feature2\", \"corr\"]\n",
    "\n",
    "    grouped_feature_ls = []\n",
    "    correlated_groups = []\n",
    "\n",
    "    for feature in corrmat.feature1.unique():\n",
    "        if feature not in grouped_feature_ls:\n",
    "\n",
    "            # find all features correlated to a single feature\n",
    "            correlated_block = corrmat[corrmat.feature1 == feature]\n",
    "            grouped_feature_ls = (\n",
    "                grouped_feature_ls\n",
    "                + list(correlated_block.feature2.unique())\n",
    "                + [feature]\n",
    "            )\n",
    "\n",
    "            # append the block of features to the list\n",
    "            correlated_groups.append(correlated_block)\n",
    "    return correlated_groups\n",
    "\n",
    "\n",
    "# Code by Eamon.Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation method\n",
    "Remove features that are highly correlated with each other. We can decide which ones to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_feature_detect(data=df_feat, threshold=0.9)\n",
    "# print all the correlated feature groups!\n",
    "for i in corr:\n",
    "    print(i, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feature scaling \n",
    "Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "\n",
    "Why Feature Scaling Matters?\n",
    "\n",
    "If range of inputs varies, in some algorithms, object functions will not work properly.\n",
    "\n",
    "`Gradient descent` converges much faster with feature scaling done. Gradient descent is a common optimization algorithm used in `logistic regression`, `SVMs`, `neural networks` etc.\n",
    "\n",
    "Algorithms that involve distance calculation like `KNN`, `Clustering` are also affected by the magnitude of the feature. Just consider how Euclidean distance is calculated: taking the square root of the sum of the squared differences between observations. This distance can be greatly affected by differences in scale among the variables. Variables with large variances have a larger effect on this measure than variables with small variances.\n",
    "\n",
    "Note: `Tree-based algorithms` are almost the only algorithms that are not affected by the magnitude of the input, as we can easily see from how trees are built. When deciding how to make a split, tree algorithm look for decisions like \"whether feature value X>3.0\" and compute the purity of the child node after the split, so the scale of the feature does not count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization - Standardization (Z-score scaling)\t\n",
    "Removes the mean and scales the data to unit variance.\n",
    "$$z = (X - X.mean) / std$$\n",
    "- `pros (+)`: feature is rescaled to have a standard normal distribution that centered around 0 with SD of 1\t\n",
    "- `cons (-)`: compress the observations in the narrow range if the variable is skewed or has outliers, thus impair the predictive power.\n",
    "\n",
    "🚨 If your feature is not Gaussian like, say, has a skewed distribution or has outliers, Normalization - Standardization is not a good choice as it will compress most data to a narrow range. However, we can transform the feature into Gaussian like and then use Normalization - Standardization.\n",
    "\n",
    "💡 When performing distance or covariance calculation (algorithm like Clustering, PCA and LDA), it is better to use Normalization - Standardization as it will remove the effect of scales on variance and covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization_f(df):\n",
    "    \"\"\"\n",
    "    Example\n",
    "    -------\n",
    "    >>> df = pd.DataFrame( {\"col1\": [1, 3, 5, 7, 9], \"col2\": [7, 4, 35, 14, 56]} )\n",
    "    >>> df = standardization_f(df)\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # define standard scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # transform data\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max scaling\t\n",
    "Transforms features by scaling each feature to a given range. Default is `feature_range` to [0,1].\n",
    "$$X_{scaled} = (X - X.min) / (X.max - X.min)$$\n",
    "- `cons (-)`: Compress the observations in the narrow range if the variable is skewed or has outliers, thus impair the predictive power.\n",
    "\n",
    "🚨 Min-Max scaling has the same drawbacks as Normalization - Standardization, and also new data may not be bounded to [0,1] as they can be out of the original range. Some algorithms, for example some deep learning network prefer input on a 0-1 scale so this is a good choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxScaler_f(df, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Example\n",
    "    -------\n",
    "    >>> df = pd.DataFrame( {\"col1\": [1, 3, 5, 7, 9], \"col2\": [7, 4, 35, 14, 56]} )\n",
    "    >>> df = minMaxScaler_f(df)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range)\n",
    "\n",
    "    df_scaled = scaler.fit_transform(df.to_numpy())\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust scaling\t\n",
    "Removes the median and scales the data according to the quantile range (defaults to `IQR`)\n",
    "$$X_{scaled} = (X - X.median) / IQR$$\n",
    "- `cons (+)`: Better at preserving the spread of the variable after transformation for skewed variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustScaler_f(df, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Example\n",
    "    -------\n",
    "    >>> df = pd.DataFrame( {\"col1\": [1, 3, 5, 7, 9], \"col2\": [7, 4, 35, 14, 56]} )\n",
    "    >>> df = robustScaler_f(df)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    df_scaled = scaler.fit_transform(df.to_numpy())\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import kruskal\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_f_classif(x, y):\n",
    "    \"\"\"\n",
    "    Compute the ANOVA F-value for the provided sample.\n",
    "    Example\n",
    "    -------\n",
    "    >>> fvals = feature_selection_f_classif(df_feat, hypno_30s)\n",
    "    \"\"\"\n",
    "    k_best = SelectKBest(f_classif, k=\"all\")\n",
    "    fit = k_best.fit(x, y)\n",
    "    # print(\"Scores: \", fit.scores_)\n",
    "    # ranking = fit.get_support()\n",
    "    # print(\"Ranking: \", ranking)\n",
    "    # features = fit.transform(x)\n",
    "    fvals = pd.Series(fit.scores_, index=df_feat.columns).sort_values()\n",
    "    return fvals\n",
    "\n",
    "\n",
    "# Compute the ANOVA F-value for the provided sample\n",
    "fvals = feature_selection_f_classif(df_feat, hypno_30s)\n",
    "# Plot features ranking\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle('ANOVA F-value for features')\n",
    "sns.barplot(y=fvals.index, x=fvals, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"ANOVA F-value\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_fclassif.png\", format=\"png\")\n",
    "plt.savefig(\"fs_fclassif.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_chi_squared(x, y):\n",
    "    # Chi Squared\n",
    "    # Feature extraction\n",
    "    test = SelectKBest(score_func=chi2, k=10)\n",
    "    fit = test.fit(x, y)\n",
    "    # Summarize scores\n",
    "    # np.set_printoptions(precision=3)\n",
    "    # print(fit.scores_)\n",
    "    # ranking = fit.get_support()\n",
    "    # print(\"Ranking: \", ranking)\n",
    "    # Summarize selected features\n",
    "    # features = fit.transform(x)\n",
    "    fvals = pd.Series(fit.scores_, index=df_feat.columns).sort_values()\n",
    "    return fvals\n",
    "\n",
    "\n",
    "# Compute the Chi-squared F-value for the provided sample\n",
    "fvals = feature_selection_chi_squared(minMaxScaler_f(df_feat), hypno_30s)\n",
    "# Plot features ranking\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle('Chi-squared stats of non-negative features')\n",
    "sns.barplot(y=fvals.index, x=fvals, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"F-values\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_chi_squared.png\", format=\"png\")\n",
    "plt.savefig(\"fs_chi_squared.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_kruskal_wallis(x, y):\n",
    "    \"\"\"\n",
    "    The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.\n",
    "    \"\"\"\n",
    "    rank = []\n",
    "    for i in df_feat.columns:\n",
    "        setX = df_feat[i].values\n",
    "        rank.append(kruskal(setX, hypno_30s)[0])\n",
    "\n",
    "    ranks = pd.Series(rank, index=df_feat.columns).sort_values()\n",
    "    return ranks\n",
    "\n",
    "\n",
    "# Compute the Kruskal-Wallis H statistic for the provided sample\n",
    "hvals = feature_selection_kruskal_wallis(df_feat, hypno_30s)\n",
    "# Plot features ranking\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle(\"Kruskal-Wallis statistics for features\")\n",
    "sns.barplot(y=hvals.index, x=hvals, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"Kruskal-Wallis H statistic\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_kruskal_wallis.png\", format=\"png\")\n",
    "plt.savefig(\"fs_kruskal_wallis.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_mutual_info(x, y, k=10):\n",
    "    \"\"\"\n",
    "    Preforms feature selection using the select_k_best with mutual_info_classif.\n",
    "    Mutual information measures how much information the presence/absence of a feature contributes to making the correct prediction on Y.\n",
    "    Recieves\n",
    "    --------\n",
    "        x -> feature dataframe\n",
    "        y -> labels\n",
    "        k -> number of remaining features\n",
    "    Returns:\n",
    "        keep_features -> list with the naime of the choosen features\n",
    "    \"\"\"\n",
    "    clf = SelectKBest(mutual_info_classif, k=k)\n",
    "    fit = clf.fit(x, y)\n",
    "    fvals = pd.Series(fit.scores_, index=df_feat.columns).sort_values()\n",
    "    return fvals\n",
    "\n",
    "\n",
    "# Compute the estimated mutual information between each feature and the target for the provided sample\n",
    "MI = feature_selection_mutual_info(df_feat, hypno_30s, 10)\n",
    "# Plot features ranking\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle(\"Mutual Information between each feature and sleep stage\")\n",
    "sns.barplot(y=MI.index, x=MI, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"Estimated mutual information\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_mutual_info.png\", format=\"png\")\n",
    "plt.savefig(\"fs_mutual_info.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_ROC(x, y):\n",
    "    \"\"\"\n",
    "    Preforms feature selection using the ROC with LDA classifier\n",
    "    Recieves:\n",
    "        data -> data frame\n",
    "    Returns:\n",
    "        keep_features -> list with the naime of the choosen features\n",
    "    \"\"\"\n",
    "\n",
    "    rank = []\n",
    "    classifier = LinearDiscriminantAnalysis()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.25, random_state=None, shuffle=False\n",
    "    )\n",
    "\n",
    "    for i in X_train.columns:\n",
    "        x_train_2d_array = X_train[i].to_frame()\n",
    "        y_score = classifier.fit(x_train_2d_array, y_train)\n",
    "        x_test_2d_array = X_test[i].to_frame()\n",
    "        y_score = y_score.decision_function(x_test_2d_array)\n",
    "        # Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "        rank.append([i, roc_auc_score(y_test, y_score)])\n",
    "\n",
    "    rank = sorted(rank, key=lambda x: x[1])\n",
    "    rank = list(\n",
    "        zip(*rank)\n",
    "    )  # python trick: transform list to select 1st and 2nd columns of it with indexing\n",
    "    fvals = pd.Series(rank[1], index=rank[0]).sort_values()\n",
    "    return fvals\n",
    "\n",
    "\n",
    "# Binazrize multiclass hypnogram to two (REM/NREM or 1/0) for ROC with LDA classifier to work\n",
    "hypno_30s_bin = hypno_30s.copy()\n",
    "hypno_30s_bin[~(hypno_30s == 4)] = 0\n",
    "hypno_30s_bin[(hypno_30s == 4)] = 1\n",
    "# Compute the ranks for the provided sample\n",
    "aucVal = feature_selection_ROC(df_feat, hypno_30s_bin)\n",
    "# Plot features ranking\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle(\"Feature selection using the ROC with LDA classifier\")\n",
    "sns.barplot(y=fvals.index, x=fvals, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"Area Under the Receiver Operating Characteristic Curve\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_auc_lda.png\", format=\"png\")\n",
    "plt.savefig(\"fs_auc_lda.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def feature_selection_kernel_density(x, y):\n",
    "    \"\"\"\n",
    "    Preforms feature selection using the KernelDensity with gaussian kernel\n",
    "    Recieves:\n",
    "        data -> data frame\n",
    "        n_features -> number of remaining features\n",
    "    Returns:\n",
    "        keep_features -> list with the naime of the choosen features\n",
    "    \"\"\"\n",
    "    rank = []\n",
    "    for column in x:\n",
    "        df = x[column].values[..., np.newaxis]\n",
    "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(df)\n",
    "        values = kde.score(df, y)  # Compute the total log-likelihood under the model.\n",
    "        rank.append((column, values))\n",
    "\n",
    "    rank = sorted(rank, key=lambda x: x[1], reverse=True)\n",
    "    rank = list(\n",
    "        zip(*rank)\n",
    "    )  # python trick: transform list to select 1st and 2nd columns of it with indexing\n",
    "    fvals = pd.Series(rank[1], index=rank[0]).sort_values()\n",
    "    return fvals\n",
    "\n",
    "\n",
    "# total log-likelihood under the mode\n",
    "likeliUmodel = feature_selection_kernel_density(df_feat, hypno_30s)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "likeliUmodel_scaled = scaler.fit_transform(likeliUmodel.values.reshape(-1, 1))\n",
    "likeliUmodel_scaled = pd.Series(\n",
    "    likeliUmodel_scaled.squeeze(), index=likeliUmodel.keys()\n",
    ")\n",
    "\n",
    "\n",
    "# Plot features ranking - unscaled\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle(\"Feature selection using the Kernel Density Estimation with gaussian kernel\")\n",
    "sns.barplot(y=likeliUmodel.index, x=fvals, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"Total log-likelihood under the model\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_KernelDensity_unscaled.png\", format=\"png\")\n",
    "plt.savefig(\"fs_KernelDensity_unscaled.svg\", format=\"svg\")\n",
    "plt.show()\n",
    "\n",
    "# Plot features ranking - scaled\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle(\"Feature selection using the Kernel Density Estimation with gaussian kernel\")\n",
    "sns.barplot(y=likeliUmodel_scaled.index, x=likeliUmodel_scaled, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"Total log-likelihood under the model ( scaled to [0,1] )\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_KernelDensity_scaled.png\", format=\"png\")\n",
    "plt.savefig(\"fs_KernelDensity_scaled.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_RFE(x, y):\n",
    "    \"\"\"\n",
    "    Preforms feature selection using the RFE with LDA.\n",
    "    Feature ranking with recursive feature elimination.\n",
    "    Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "    \"\"\"\n",
    "    estimator = LinearDiscriminantAnalysis()\n",
    "    selector = RFE(estimator, n_features_to_select=1)\n",
    "    selector = selector.fit(x, y)\n",
    "    ranks = pd.Series(selector.ranking_, index=df_feat.columns).sort_values()\n",
    "    # Sort ascending\n",
    "    ranks = ranks.max() - ranks + 1\n",
    "    ranks = ranks.sort_values()\n",
    "    return ranks\n",
    "\n",
    "\n",
    "ranks = feature_selection_RFE(df_feat, hypno_30s)\n",
    "# Plot features ranking - scaled\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle(\"Feature selection using the RFE with LDA\")\n",
    "sns.barplot(y=ranks.index, x=ranks, palette=\"RdYlGn\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(size=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fs_RFE_LDA.png\", format=\"png\")\n",
    "plt.savefig(\"fs_RFE_LDA.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_print(df):\n",
    "    # Using Pearson Correlation\n",
    "\n",
    "    # plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = df.corr()\n",
    "    # sns.heatmap(corr_matrix, annot=True, cmap=plt.cm.Reds)\n",
    "    # plt.show()\n",
    "\n",
    "    outputVariables = [\"hmob\"]\n",
    "    for outputVariable in outputVariables:\n",
    "        # Correlation with output variable\n",
    "        cor_target = abs(corr_matrix[outputVariable])\n",
    "        # Selecting highly correlated features\n",
    "        relevant_features = cor_target[cor_target >= 0.8]\n",
    "        print(f\"RELEVANT ONES to {outputVariable}:\")\n",
    "        print(relevant_features)\n",
    "        print()\n",
    "\n",
    "\n",
    "def pearson_correlation_plot(df, thresh=0.95):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.suptitle(f'Pearson Correlation Plot for Features (thresh = {thresh})')\n",
    "    corr_matrix = df.corr()\n",
    "    corr_matrix[abs(corr_matrix) >= thresh] = abs(corr_matrix)\n",
    "    corr_matrix[abs(corr_matrix) < thresh] = 0\n",
    "    # print(corr_matrix.shape)\n",
    "    g = sns.heatmap(abs(corr_matrix), cmap=plt.cm.Reds, xticklabels=True, yticklabels=True)\n",
    "    g.set_xticklabels(g.get_xmajorticklabels(), fontsize=7)\n",
    "    g.set_yticklabels(g.get_ymajorticklabels(), fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"fs_pearson_correlation_thresh_{thresh}.png\", format=\"png\")\n",
    "    plt.savefig(f\"fs_pearson_correlation_thresh_{thresh}.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pearson_correlation_print(df_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hypnogram and a feature\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "hypno = pd.Series(hypno_30s).map({-1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1}).values\n",
    "hypno_rem = np.ma.masked_not_equal(hypno, 1)\n",
    "\n",
    "# Plot the hypnogram\n",
    "ax1.step(times, -1 * hypno, color=\"k\", lw=1.5)\n",
    "ax1.step(times, -1 * hypno_rem, color=\"r\", lw=2.5)\n",
    "ax1.set_yticks([0, -1, -2, -3, -4])\n",
    "ax1.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "ax1.set_ylim(-4.5, 0.5)\n",
    "ax1.set_ylabel(\"Sleep stage\")\n",
    "\n",
    "# Plot the non-linear feature\n",
    "ax2.plot(times, df_feat[\"bubbleEnt1\"])\n",
    "ax2.set_ylabel(\"Bubble Entropy\")\n",
    "# ax2.set_ylabel('Higuchi Fractal Dimension')\n",
    "ax2.set_xlabel(\"Time [minutes]\")\n",
    "\n",
    "ax2.set_xlim(0, times[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hypno_30s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9461a3eb0cbef873800a78c94f0b6f375dc71a03d464463d7b611af82b6cd16"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mne')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
