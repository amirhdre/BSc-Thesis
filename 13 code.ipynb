{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from scipy import signal\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from mne.filter import resample, filter_data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lspopt import spectrogram_lspopt\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n",
    "import logging\n",
    "LOGGING_TYPES = dict(DEBUG=logging.DEBUG, INFO=logging.INFO, WARNING=logging.WARNING,\n",
    "                     ERROR=logging.ERROR, CRITICAL=logging.CRITICAL)\n",
    "logger = logging.getLogger('yasa')\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypno</th>\n",
       "      <th>df_feat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P18_N3 L</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P18_N3 L.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P18_N2 R</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P18_N2 R.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P17_N2 L</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P17_N2 L.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      hypno  \\\n",
       "name                                                          \n",
       "P18_N3 L  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "P18_N2 R  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "P17_N2 L  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "\n",
       "                       df_feat  \n",
       "name                            \n",
       "P18_N3 L  feature/P18_N3 L.csv  \n",
       "P18_N2 R  feature/P18_N2 R.csv  \n",
       "P17_N2 L  feature/P17_N2 L.csv  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load reference_df     \n",
    "reference_df = pd.read_csv(\"reference_df.csv\", index_col=\"name\")\n",
    "reference_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sb</th>\n",
       "      <th>ab</th>\n",
       "      <th>bs</th>\n",
       "      <th>ag</th>\n",
       "      <th>sg</th>\n",
       "      <th>gs</th>\n",
       "      <th>ba</th>\n",
       "      <th>ta_b</th>\n",
       "      <th>alpha</th>\n",
       "      <th>ga</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_psd</th>\n",
       "      <th>kurt</th>\n",
       "      <th>E</th>\n",
       "      <th>WEn</th>\n",
       "      <th>renyi</th>\n",
       "      <th>mean_distance</th>\n",
       "      <th>diffEnt</th>\n",
       "      <th>skew</th>\n",
       "      <th>tsallisEnt</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f_classif</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MI</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chiSqr</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sb   ab   bs   ag   sg    gs    ba  ta_b  alpha    ga  ...  \\\n",
       "method_name                                                          ...   \n",
       "f_classif    1.0  2.0  3.0  4.0  5.0  10.0  11.0   6.0    7.0  22.0  ...   \n",
       "MI           1.0  2.0  3.0  4.0  8.0   9.0   5.0   7.0   17.0   6.0  ...   \n",
       "chiSqr       2.0  1.0  3.0  6.0  9.0   4.0   8.0  16.0    5.0   7.0  ...   \n",
       "\n",
       "             mean_psd  kurt     E   WEn  renyi  mean_distance  diffEnt  skew  \\\n",
       "method_name                                                                    \n",
       "f_classif        66.0  72.0  67.0  69.0   70.0           65.0     71.0  73.0   \n",
       "MI               70.0  67.0  73.0  68.0   71.0           74.0     72.0  66.0   \n",
       "chiSqr           66.0  64.0  67.0  71.0   68.0           72.0     69.0  73.0   \n",
       "\n",
       "             tsallisEnt  mean  \n",
       "method_name                    \n",
       "f_classif          74.0  75.0  \n",
       "MI                 69.0  75.0  \n",
       "chiSqr             74.0  75.0  \n",
       "\n",
       "[3 rows x 75 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv    \n",
    "rankings_df = pd.read_csv(\"rankings_df.csv\", index_col=\"method_name\")\n",
    "rankings_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see id's\n",
    "idx = reference_df.index.to_list()\n",
    "\n",
    "# to load hypno:\n",
    "hypno_loc = reference_df.loc[idx[10], \"hypno\"]\n",
    "hypno_30s = np.loadtxt(hypno_loc, dtype ='int')[:, 0]\n",
    "\n",
    "# # to load features:\n",
    "df_feat_loc = reference_df.loc[idx[10], \"df_feat\"]\n",
    "df_feat = pd.read_csv(df_feat_loc, index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypno_30s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822, 75)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omitting artifcat epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_feat.loc[hypno_30s!=-1]\n",
    "hypno_30s = hypno_30s[hypno_30s!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: [0 1 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "print('Class labels:', np.unique(hypno_30s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_feat, hypno_30s, test_size=0.2, random_state=1, stratify=hypno_30s\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [ 24  12 343 131   0 149]\n",
      "Labels counts in y_train: [ 19  10 274 105   0 119]\n",
      "Labels counts in y_test: [ 5  2 69 26  0 30]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels counts in y:\", np.bincount(hypno_30s))\n",
    "print(\"Labels counts in y_train:\", np.bincount(y_train))\n",
    "print(\"Labels counts in y_test:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(df_feat)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(eta0=0.1, random_state=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 14\n"
     ]
    }
   ],
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % ppn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()  # C=100.0, solver='lbfgs', multi_class='ovr')\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "print(lr.predict_proba(X_test_std[:3, :]).argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 6\n",
      "Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test_std)\n",
    "print(\"Misclassified examples: %d\" % (y_test != y_pred).sum())\n",
    "\n",
    "print(\"Accuracy: %.3f\" % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "weights, params = [], []\n",
    "for c in np.arange(-5, 5):\n",
    "    lr = LogisticRegression(C=10.0**c, multi_class=\"ovr\")\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.0**c)\n",
    "\n",
    "weights = np.array(weights)\n",
    "params = np.array(params)\n",
    "plt.figure()\n",
    "plt.plot(params, weights[:, 0], label=\"std\")\n",
    "plt.plot(params, weights[:, 1], linestyle=\"--\", label=\"mean\")\n",
    "plt.ylabel(\"Weight coefficient\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xscale(\"log\")\n",
    "# plt.savefig('figures/03_08.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum margin classification with support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "svm.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 5\n",
      "Accuracy: 0.962\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppn Misclassified examples: 8\n",
      "ppn Accuracy: 0.939\n",
      "lr Misclassified examples: 9\n",
      "lr Accuracy: 0.932\n",
      "svm Misclassified examples: 6\n",
      "svm Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "ppn = SGDClassifier(loss='perceptron')\n",
    "ppn.fit(X_train_std, y_train)\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('ppn Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('ppn Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "lr = SGDClassifier(loss='log')\n",
    "lr.fit(X_train_std, y_train)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print('lr Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('lr Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('svm Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('svm Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting feature space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = \"sb\"\n",
    "feat2 = \"alpha\"\n",
    "\n",
    "df_feat1 = df_feat.copy()\n",
    "df_feat1 = df_feat1[[feat1, feat2]]\n",
    "\n",
    "sc1 = StandardScaler()\n",
    "sc1.fit(df_feat1)\n",
    "X = sc1.transform(df_feat1)\n",
    "y = hypno_30s\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], marker=\"s\", label=\"Class 0\")\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], marker=\"o\", label=\"Class 1\")\n",
    "plt.scatter(X[y == 2, 0], X[y == 2, 1], marker=\"o\", label=\"Class 2\")\n",
    "plt.scatter(X[y == 3, 0], X[y == 3, 1], marker=\"o\", label=\"Class 3\")\n",
    "plt.scatter(X[y == 4, 0], X[y == 4, 1], marker=\"o\", label=\"Class 4\")\n",
    "\n",
    "plt.xlabel(f\"Feature 1 - {feat1}\")\n",
    "plt.ylabel(f\"Feature 2 - {feat2}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{feat1}_vs_{feat2} features.png\", dpi=300)\n",
    "# plt.savefig(f\"{feat1}_vs_{feat2} features.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    feat1 = rankings_df.columns.values.tolist()[int(i // 5 + 7)]\n",
    "    feat2 = rankings_df.columns.values.tolist()[int(np.random.randint(1, 65))]\n",
    "\n",
    "    df_feat1 = df_feat.copy()\n",
    "    df_feat1 = df_feat1[[feat1, feat2]]\n",
    "\n",
    "    sc1 = StandardScaler()\n",
    "    sc1.fit(df_feat1)\n",
    "    X = sc1.transform(df_feat1)\n",
    "    y = hypno_30s\n",
    "\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], marker=\"o\", label=\"Class 0\")\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], marker=\"v\", label=\"Class 1\")\n",
    "    ax.scatter(X[y == 2, 0], X[y == 2, 1], marker=\"s\", label=\"Class 2\")\n",
    "    ax.scatter(X[y == 3, 0], X[y == 3, 1], marker=\"*\", label=\"Class 3\")\n",
    "    ax.scatter(X[y == 4, 0], X[y == 4, 1], marker=\"p\", label=\"Class 4\")\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    ax.set_xlabel(f\"{feat1}\")\n",
    "    ax.set_ylabel(f\"{feat2}\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"2D features space.png\", dpi=300)\n",
    "# plt.savefig(f\"2D features space.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "#     # setup marker generator and color map\n",
    "#     markers = ('o', 's', '^', 'v', '<')\n",
    "#     colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "#     cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "#     # plot the decision surface\n",
    "#     x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#     x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#     xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "#                            np.arange(x2_min, x2_max, resolution))\n",
    "#     lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "#     lab = lab.reshape(xx1.shape)\n",
    "#     plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "#     plt.xlim(xx1.min(), xx1.max())\n",
    "#     plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "#     # plot class examples\n",
    "#     for idx, cl in enumerate(np.unique(y)):\n",
    "#         plt.scatter(x=X[y == cl, 0], \n",
    "#                     y=X[y == cl, 1],\n",
    "#                     alpha=0.8, \n",
    "#                     c=colors[idx],\n",
    "#                     marker=markers[idx], \n",
    "#                     label=f'Class {cl}', \n",
    "#                     edgecolor='black')\n",
    "\n",
    "#     # highlight test examples\n",
    "#     if test_idx:\n",
    "#         # plot all examples\n",
    "#         X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "#         plt.scatter(X_test[:, 0],\n",
    "#                     X_test[:, 1],\n",
    "#                     c='none',\n",
    "#                     edgecolor='black',\n",
    "#                     alpha=1.0,\n",
    "#                     linewidth=1,\n",
    "#                     marker='o',\n",
    "#                     s=100, \n",
    "#                     label='Test set')     \n",
    "\n",
    "# svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)\n",
    "# svm.fit(X, y)\n",
    "# plot_decision_regions(X, y,\n",
    "#                       classifier=svm)\n",
    "\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.tight_layout()\n",
    "# #plt.savefig('figures/03_14.png', dpi=300)\n",
    "# plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving non-linear problems using a kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = rankings_df.columns[:20]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_feat[columns], hypno_30s, test_size=0.2, random_state=1, stratify=hypno_30s\n",
    ")\n",
    "\n",
    "X_train, X_test = X_train[columns], X_test[columns]\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(df_feat[columns])\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sb</th>\n",
       "      <th>ab</th>\n",
       "      <th>bs</th>\n",
       "      <th>ag</th>\n",
       "      <th>sg</th>\n",
       "      <th>gs</th>\n",
       "      <th>ba</th>\n",
       "      <th>ta_b</th>\n",
       "      <th>alpha</th>\n",
       "      <th>ga</th>\n",
       "      <th>at</th>\n",
       "      <th>tb</th>\n",
       "      <th>ad</th>\n",
       "      <th>iqr</th>\n",
       "      <th>higuchi</th>\n",
       "      <th>bubbleEnt1</th>\n",
       "      <th>sigma</th>\n",
       "      <th>bubbleEnt2</th>\n",
       "      <th>lziv</th>\n",
       "      <th>ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.622033</td>\n",
       "      <td>1.000696</td>\n",
       "      <td>1.607631</td>\n",
       "      <td>4.626121</td>\n",
       "      <td>2.875599</td>\n",
       "      <td>0.347754</td>\n",
       "      <td>0.999304</td>\n",
       "      <td>4.260840</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.216164</td>\n",
       "      <td>0.306948</td>\n",
       "      <td>3.260144</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>9.844530</td>\n",
       "      <td>1.476796</td>\n",
       "      <td>-0.006400</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>-0.010514</td>\n",
       "      <td>836</td>\n",
       "      <td>3.257876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1.141445</td>\n",
       "      <td>3.360351</td>\n",
       "      <td>0.876083</td>\n",
       "      <td>14.167325</td>\n",
       "      <td>4.812359</td>\n",
       "      <td>0.207798</td>\n",
       "      <td>0.297588</td>\n",
       "      <td>9.550430</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.070585</td>\n",
       "      <td>0.542861</td>\n",
       "      <td>6.190079</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>11.814632</td>\n",
       "      <td>1.282473</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.011808</td>\n",
       "      <td>774</td>\n",
       "      <td>1.842093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1.571452</td>\n",
       "      <td>4.727456</td>\n",
       "      <td>0.636354</td>\n",
       "      <td>14.600504</td>\n",
       "      <td>4.853347</td>\n",
       "      <td>0.206043</td>\n",
       "      <td>0.211530</td>\n",
       "      <td>11.205612</td>\n",
       "      <td>0.070165</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.729753</td>\n",
       "      <td>6.478156</td>\n",
       "      <td>0.088736</td>\n",
       "      <td>9.942860</td>\n",
       "      <td>1.312383</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.023323</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>734</td>\n",
       "      <td>1.370326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sb        ab        bs         ag        sg        gs        ba  \\\n",
       "272  0.622033  1.000696  1.607631   4.626121  2.875599  0.347754  0.999304   \n",
       "262  1.141445  3.360351  0.876083  14.167325  4.812359  0.207798  0.297588   \n",
       "397  1.571452  4.727456  0.636354  14.600504  4.853347  0.206043  0.211530   \n",
       "\n",
       "          ta_b     alpha        ga        at        tb        ad        iqr  \\\n",
       "272   4.260840  0.019391  0.216164  0.306948  3.260144  0.021990   9.844530   \n",
       "262   9.550430  0.018938  0.070585  0.542861  6.190079  0.020303  11.814632   \n",
       "397  11.205612  0.070165  0.068491  0.729753  6.478156  0.088736   9.942860   \n",
       "\n",
       "      higuchi  bubbleEnt1     sigma  bubbleEnt2  lziv        ta  \n",
       "272  1.476796   -0.006400  0.012053   -0.010514   836  3.257876  \n",
       "262  1.282473    0.009255  0.006433    0.011808   774  1.842093  \n",
       "397  1.312383    0.003868  0.023323    0.004823   734  1.370326  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 6\n",
      "Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 11\n",
      "Accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion='gini', \n",
    "                                    max_depth=5, \n",
    "                                    random_state=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.plot_tree(tree_model, filled=True)\n",
    "\n",
    "# plt.savefig('figures/03_21_1.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining weak to strong learners via random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 11\n",
      "Accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors - a lazy learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 8\n",
      "Accuracy: 0.939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           p=2, \n",
    "                           metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing feature importance with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) ab                             0.067876\n",
      " 2) ba                             0.066054\n",
      " 3) spec_entropy                   0.048812\n",
      " 4) ta                             0.045599\n",
      " 5) at                             0.045500\n",
      " 6) bs                             0.042016\n",
      " 7) sb                             0.041040\n",
      " 8) beta                           0.040812\n",
      " 9) nzc                            0.038547\n",
      "10) bd                             0.034809\n",
      "11) ga                             0.033013\n",
      "12) gb_da                          0.029125\n",
      "13) ag                             0.028449\n",
      "14) db                             0.028225\n",
      "15) ta_b                           0.023249\n",
      "16) gb                             0.020812\n",
      "17) alpha                          0.020804\n",
      "18) ad                             0.019912\n",
      "19) gs                             0.018976\n",
      "20) bg                             0.018525\n",
      "21) lziv                           0.017723\n",
      "22) higuchi                        0.017459\n",
      "23) sd                             0.015383\n",
      "24) da                             0.015269\n",
      "25) sg                             0.014704\n",
      "26) bt                             0.013897\n",
      "27) tb                             0.013845\n",
      "28) sigma                          0.012685\n",
      "29) gt                             0.010339\n",
      "30) svd_entropy                    0.009903\n",
      "31) gamma                          0.009883\n",
      "32) iqr                            0.009780\n",
      "33) gd                             0.009146\n",
      "34) bubbleEnt1                     0.009132\n",
      "35) hmob                           0.009083\n",
      "36) st                             0.009023\n",
      "37) ta_ab                          0.008383\n",
      "38) bubbleEnt2                     0.008078\n",
      "39) tg                             0.007724\n",
      "40) perm_entropy                   0.007169\n",
      "41) std                            0.007001\n",
      "42) sample_entropy                 0.006984\n",
      "43) petrosian                      0.006935\n",
      "44) delta                          0.006579\n",
      "45) app_entropy                    0.006258\n",
      "46) asi                            0.005975\n",
      "47) theta                          0.005533\n",
      "48) sa                             0.005194\n",
      "49) td                             0.004699\n",
      "50) iqr_psd                        0.004080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "columns = rankings_df.columns[:50]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_feat[columns], hypno_30s, test_size=0.2, random_state=1, stratify=hypno_30s\n",
    ")\n",
    "\n",
    "X_train, X_test = X_train[columns], X_test[columns]\n",
    "\n",
    "feat_labels = df_feat[columns].columns[:]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\n",
    "        \"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]])\n",
    "    )\n",
    "\n",
    "plt.title(\"Feature importance\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "\n",
    "plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/04_10.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features that meet this threshold criterion: 17\n",
      " 1) ab                             0.067876\n",
      " 2) ba                             0.066054\n",
      " 3) spec_entropy                   0.048812\n",
      " 4) ta                             0.045599\n",
      " 5) at                             0.045500\n",
      " 6) bs                             0.042016\n",
      " 7) sb                             0.041040\n",
      " 8) beta                           0.040812\n",
      " 9) nzc                            0.038547\n",
      "10) bd                             0.034809\n",
      "11) ga                             0.033013\n",
      "12) gb_da                          0.029125\n",
      "13) ag                             0.028449\n",
      "14) db                             0.028225\n",
      "15) ta_b                           0.023249\n",
      "16) gb                             0.020812\n",
      "17) alpha                          0.020804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirhosseindaraie/miniconda3/envs/mne/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(forest, threshold=0.02, prefit=True)\n",
    "X_selected = sfm.transform(X_train)\n",
    "print('Number of features that meet this threshold criterion:', \n",
    "      X_selected.shape[1])\n",
    "\n",
    "for f in range(X_selected.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mne')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9461a3eb0cbef873800a78c94f0b6f375dc71a03d464463d7b611af82b6cd16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
