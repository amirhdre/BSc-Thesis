{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from scipy import signal\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from mne.filter import resample, filter_data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lspopt import spectrogram_lspopt\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n",
    "import logging\n",
    "LOGGING_TYPES = dict(DEBUG=logging.DEBUG, INFO=logging.INFO, WARNING=logging.WARNING,\n",
    "                     ERROR=logging.ERROR, CRITICAL=logging.CRITICAL)\n",
    "logger = logging.getLogger('yasa')\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypno</th>\n",
       "      <th>df_feat</th>\n",
       "      <th>eeg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P18_N3 L</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P18_N3 L.csv</td>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/autoscor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P18_N2 R</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P18_N2 R.csv</td>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/autoscor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P17_N2 L</th>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/synced-h...</td>\n",
       "      <td>feature/P17_N2 L.csv</td>\n",
       "      <td>/Users/amirhosseindaraie/Desktop/data/autoscor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      hypno  \\\n",
       "name                                                          \n",
       "P18_N3 L  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "P18_N2 R  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "P17_N2 L  /Users/amirhosseindaraie/Desktop/data/synced-h...   \n",
       "\n",
       "                       df_feat  \\\n",
       "name                             \n",
       "P18_N3 L  feature/P18_N3 L.csv   \n",
       "P18_N2 R  feature/P18_N2 R.csv   \n",
       "P17_N2 L  feature/P17_N2 L.csv   \n",
       "\n",
       "                                                        eeg  \n",
       "name                                                         \n",
       "P18_N3 L  /Users/amirhosseindaraie/Desktop/data/autoscor...  \n",
       "P18_N2 R  /Users/amirhosseindaraie/Desktop/data/autoscor...  \n",
       "P17_N2 L  /Users/amirhosseindaraie/Desktop/data/autoscor...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load reference_df     \n",
    "reference_df = pd.read_csv(\"reference_df.csv\", index_col=\"name\")\n",
    "reference_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>sb</th>\n",
       "      <th>ag</th>\n",
       "      <th>sg</th>\n",
       "      <th>lziv</th>\n",
       "      <th>iqr</th>\n",
       "      <th>bs</th>\n",
       "      <th>ta_b</th>\n",
       "      <th>gs</th>\n",
       "      <th>alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>median</th>\n",
       "      <th>mean_psd</th>\n",
       "      <th>E</th>\n",
       "      <th>WEn</th>\n",
       "      <th>ds</th>\n",
       "      <th>mean_distance</th>\n",
       "      <th>diffEnt</th>\n",
       "      <th>renyi</th>\n",
       "      <th>skew</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f_classif</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chiSqr</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ab   sb   ag   sg  lziv  iqr   bs  ta_b    gs  alpha  ...  \\\n",
       "method_name                                                         ...   \n",
       "f_classif    2.0  1.0  3.0  4.0   6.0  9.0  7.0   5.0  15.0    8.0  ...   \n",
       "chiSqr       1.0  2.0  4.0  8.0   7.0  6.0  9.0  12.0   3.0   10.0  ...   \n",
       "\n",
       "             median  mean_psd     E   WEn    ds  mean_distance  diffEnt  \\\n",
       "method_name                                                               \n",
       "f_classif      61.0      64.0  66.0  62.0  71.0           63.0     67.0   \n",
       "chiSqr         69.0      66.0  65.0  70.0  62.0           71.0     68.0   \n",
       "\n",
       "             renyi  skew  mean  \n",
       "method_name                     \n",
       "f_classif     69.0  72.0  73.0  \n",
       "chiSqr        67.0  72.0  73.0  \n",
       "\n",
       "[2 rows x 73 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv    \n",
    "rankings_df = pd.read_csv(\"rankings_df aug.csv\", index_col=\"method_name\")\n",
    "rankings_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on 60 nights, test of 1 night\n",
    "(1560*60 x 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import kruskal\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[39 28 24 17 41  5 10 60 20 14 42 49  0 30 35 54 11 23 43 45 50  7 40 32\n",
      " 18 15 52 13 59 48  8 47 56 27  9 55 25 53 37 46 16 38 58 26 22 36  6  4\n",
      " 21 44]\n",
      ">>>>>>>> test recordings: \n",
      "[33  3 31 57  2  1 19 12 51 34 29]\n"
     ]
    }
   ],
   "source": [
    "idx_all_recordings = np.random.permutation(len(reference_df))\n",
    "idx_train_recordings = idx_all_recordings[:-11]\n",
    "idx_test_recordings = idx_all_recordings[-11:]\n",
    "print(\">>>>>>>> train recordings (index): \")\n",
    "print(idx_train_recordings)\n",
    "print(\">>>>>>>> test recordings: \")\n",
    "print(idx_test_recordings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the dataset to train and test + shuffle each night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[14 35 16 47 44 38 33 58  8 34  4 59 60 41  1 43 50 40 15  0 54 55 36 21\n",
      " 45  7 46 17 51 25  9 18 39 12  3 22 49 29 31  2 20  5 52 37 42 48  6 57\n",
      " 24 28 56 53 32 26 13]\n",
      ">>>>>>>> test recordings: \n",
      "[10 30 27 19 23 11]\n",
      "Train set: X=(108915, 50) y=(108915,)\n",
      "Test set: X=(11357, 50) y=(11357,)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(test_prop=0.2, n_feat=40):\n",
    "\n",
    "    idx_all_recordings = np.random.permutation(len(reference_df))\n",
    "    idx_train_recordings = idx_all_recordings[: -int(test_prop * 61)]\n",
    "    idx_test_recordings = idx_all_recordings[-int(test_prop * 61) :]\n",
    "    print(\">>>>>>>> train recordings (index): \")\n",
    "    print(idx_train_recordings)\n",
    "    print(\">>>>>>>> test recordings: \")\n",
    "    print(idx_test_recordings)\n",
    "\n",
    "    df_feat_X_train = np.array([])\n",
    "    df_feat_X_test = np.array([])\n",
    "    hypno_y_train = np.array([])\n",
    "    hypno_y_test = np.array([])\n",
    "\n",
    "    columns = rankings_df.columns[:n_feat]  # for selecting top n_feat columns\n",
    "\n",
    "    # to loop over all recording files:\n",
    "    for i in idx_train_recordings:\n",
    "        ### to load augmented hypnos for train:\n",
    "        name = reference_df.iloc[i].name\n",
    "        hypno_30s_loc = reference_df.iloc[i].hypno\n",
    "        hypno_30s_loc = hypno_30s_loc.split(\".\")[0] + \" aug.txt\"\n",
    "        hypno_30s = np.loadtxt(hypno_30s_loc, delimiter=\"\\n\")\n",
    "\n",
    "        ### to load features of augmented eeg for train:\n",
    "        df_feat_loc = reference_df.iloc[i].df_feat\n",
    "        df_feat_loc = df_feat_loc.split(\".\")[0] + \" aug.csv\"\n",
    "        df_feat = pd.read_csv(df_feat_loc, index_col=False)\n",
    "\n",
    "        df_feat = df_feat.replace(\n",
    "            [np.inf, -np.inf], 0\n",
    "        )  # Replacing infinite values in features\n",
    "\n",
    "        ### select top n_feat ranks columns\n",
    "        df_feat = df_feat[columns]\n",
    "\n",
    "        ### shuffle X\n",
    "        permut = np.random.permutation(df_feat.shape[0])\n",
    "        df_feat = df_feat.iloc[permut]\n",
    "\n",
    "        ### to load features for train: append df_feat to df_feat_X_train\n",
    "        if i == idx_train_recordings[0]:\n",
    "            df_feat_X_train = df_feat.to_numpy()\n",
    "        else:\n",
    "            df_feat_X_train = np.vstack([df_feat_X_train, df_feat.to_numpy()])\n",
    "\n",
    "        ### shuffle y\n",
    "        hypno_30s = hypno_30s[permut]\n",
    "\n",
    "        ### to load labels for train: append hypno to hypno_y_train\n",
    "        hypno_y_train = np.append(hypno_y_train, hypno_30s)\n",
    "\n",
    "    for i in idx_test_recordings:\n",
    "        ### to load features for test:\n",
    "        df_feat_loc = reference_df.iloc[i].df_feat\n",
    "        df_feat_loc = df_feat_loc.split(\".\")[0] + \" aug.csv\"\n",
    "        df_feat = pd.read_csv(df_feat_loc, index_col=False)\n",
    "\n",
    "        ### to load labels for test:\n",
    "        hypno_30s_loc = reference_df.iloc[i].hypno\n",
    "        hypno_30s_loc = hypno_30s_loc.split(\".\")[0] + \" aug.txt\"\n",
    "        hypno_30s = np.loadtxt(hypno_30s_loc, delimiter=\"\\n\")\n",
    "\n",
    "        df_feat = df_feat.replace(\n",
    "            [np.inf, -np.inf], 0\n",
    "        )  # Replacing infinite values in features\n",
    "\n",
    "        ### select top n_feat ranks columns\n",
    "        df_feat = df_feat[columns].to_numpy()\n",
    "\n",
    "        ### to load features for train: append df_feat to df_feat_X_train\n",
    "        if i == idx_test_recordings[0]:\n",
    "            df_feat_X_test = df_feat\n",
    "        else:\n",
    "            df_feat_X_test = np.vstack([df_feat_X_test, df_feat])\n",
    "\n",
    "        ### to load labels for train: append hypno to hypno_y_train\n",
    "        hypno_y_test = np.append(hypno_y_test, hypno_30s)\n",
    "\n",
    "    print(f\"Train set: X={df_feat_X_train.shape} y={hypno_y_train.shape}\")\n",
    "    print(f\"Test set: X={df_feat_X_test.shape} y={hypno_y_test.shape}\")\n",
    "\n",
    "    ### To standardize all dataset including train and test, after train/test split\n",
    "    # Generate a numpy array including all epochs:\n",
    "    df_feat_all = np.array([])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # we will standardize the columns in dataset before we feed them to a classifier\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(df_feat_X_train)  # first fit all the dataset\n",
    "    X_train_std = sc.transform(df_feat_X_train)  # then transform train\n",
    "    X_test_std = sc.transform(df_feat_X_test)  # and test\n",
    "\n",
    "    return X_train_std, X_test_std, hypno_y_train, hypno_y_test\n",
    "\n",
    "\n",
    "X_train_std, X_test_std, y_train, y_test = train_test_split(0.1, n_feat=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "# svm.fit(X_train_std, y_train)\n",
    "# y_pred = svm.predict(X_test_std)\n",
    "# print(\"Misclassified examples: %d\" % (y_test != y_pred).sum())\n",
    "# print(\"Accuracy: %.3f\" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confmat_f(confmat):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i, j], va=\"center\", ha=\"center\")\n",
    "    ax.set(\n",
    "        xticklabels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"],\n",
    "        xticks=range(5),\n",
    "        yticklabels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"],\n",
    "        yticks=range(5),\n",
    "    )\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.xaxis.labelpad = 15\n",
    "    ax.xaxis.set_tick_params(labeltop=True)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Confusion Matrix\", y=-0.1)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"confmat.png\")\n",
    "    # plt.savefig(\"confmat.svg\")\n",
    "    plt.show()\n",
    "\n",
    "### Using:\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(report)\n",
    "\n",
    "# confmat = confusion_matrix(y_test, y_pred)\n",
    "# confmat_f(confmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with different top feature numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell took 5 hours to execute \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_feat_arr = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 72]\n",
    "accuracy_arr = np.array([])\n",
    "confmat_arr = []\n",
    "report_arr = []\n",
    "\n",
    "# for i, n_feat in enumerate(n_feat_arr):\n",
    "#     # To split dataset into train/test set:\n",
    "#     X_train_std, X_test_std, y_train, y_test = train_test_split(test_prop=0.1, n_feat=n_feat)\n",
    "#     # To initiate model\n",
    "#     svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "#     # To fit the model to train set:\n",
    "#     svm.fit(X_train_std, y_train)\n",
    "#     # To predit on the test set:\n",
    "#     y_pred = svm.predict(X_test_std)\n",
    "#     # To print results\n",
    "#     print\n",
    "#     (\n",
    "#         f\"Fold {i}, {n_feat} features => Misclassified: {(y_test != y_pred).sum()}, Acc.: {accuracy_score(y_test, y_pred)}\"\n",
    "#     )\n",
    "#     # To append accuracy to array\n",
    "#     accuracy_arr = np.append(accuracy_arr, accuracy_score(y_test, y_pred))\n",
    "#     # to save report and confmat\n",
    "#     report = classification_report(y_test, y_pred)\n",
    "#     confmat = confusion_matrix(y_test, y_pred)\n",
    "#     report_arr.append(report)\n",
    "#     confmat_arr.append(confmat)\n",
    "#     print(report)\n",
    "#     confmat_f(confmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.69      0.62      1733\n",
      "         1.0       0.45      0.18      0.26      2623\n",
      "         2.0       0.62      0.47      0.53      2861\n",
      "         3.0       0.59      0.75      0.66      2941\n",
      "         4.0       0.59      0.80      0.68      3120\n",
      "\n",
      "    accuracy                           0.58     13278\n",
      "   macro avg       0.56      0.58      0.55     13278\n",
      "weighted avg       0.57      0.58      0.55     13278\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.71      0.75      1978\n",
      "         1.0       0.51      0.49      0.50      1954\n",
      "         2.0       0.70      0.70      0.70      2411\n",
      "         3.0       0.85      0.90      0.88      2588\n",
      "         4.0       0.79      0.83      0.81      2796\n",
      "\n",
      "    accuracy                           0.74     11727\n",
      "   macro avg       0.73      0.73      0.73     11727\n",
      "weighted avg       0.74      0.74      0.74     11727\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.75      0.67      1218\n",
      "         1.0       0.58      0.51      0.54      1965\n",
      "         2.0       0.84      0.69      0.76      2814\n",
      "         3.0       0.83      0.90      0.86      2826\n",
      "         4.0       0.82      0.88      0.85      3151\n",
      "\n",
      "    accuracy                           0.77     11974\n",
      "   macro avg       0.74      0.75      0.74     11974\n",
      "weighted avg       0.77      0.77      0.76     11974\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.70      0.73      1890\n",
      "         1.0       0.56      0.39      0.46      2200\n",
      "         2.0       0.74      0.78      0.76      2717\n",
      "         3.0       0.88      0.91      0.89      2856\n",
      "         4.0       0.75      0.89      0.81      2922\n",
      "\n",
      "    accuracy                           0.75     12585\n",
      "   macro avg       0.74      0.73      0.73     12585\n",
      "weighted avg       0.74      0.75      0.75     12585\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.68      0.71      1684\n",
      "         1.0       0.58      0.55      0.56      1978\n",
      "         2.0       0.77      0.71      0.74      2580\n",
      "         3.0       0.86      0.90      0.88      2714\n",
      "         4.0       0.80      0.90      0.85      2770\n",
      "\n",
      "    accuracy                           0.77     11726\n",
      "   macro avg       0.75      0.75      0.75     11726\n",
      "weighted avg       0.76      0.77      0.76     11726\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.71      0.72      2019\n",
      "         1.0       0.60      0.44      0.51      2302\n",
      "         2.0       0.72      0.66      0.69      2804\n",
      "         3.0       0.79      0.87      0.83      3020\n",
      "         4.0       0.75      0.89      0.82      3192\n",
      "\n",
      "    accuracy                           0.73     13337\n",
      "   macro avg       0.72      0.72      0.71     13337\n",
      "weighted avg       0.73      0.73      0.73     13337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.73      0.75      1396\n",
      "         1.0       0.65      0.53      0.59      2322\n",
      "         2.0       0.79      0.76      0.78      2834\n",
      "         3.0       0.89      0.89      0.89      2860\n",
      "         4.0       0.75      0.90      0.82      2832\n",
      "\n",
      "    accuracy                           0.78     12244\n",
      "   macro avg       0.77      0.76      0.76     12244\n",
      "weighted avg       0.77      0.78      0.77     12244\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.58      0.67      1891\n",
      "         1.0       0.59      0.54      0.56      1836\n",
      "         2.0       0.73      0.78      0.76      2371\n",
      "         3.0       0.88      0.84      0.86      2595\n",
      "         4.0       0.70      0.87      0.78      2549\n",
      "\n",
      "    accuracy                           0.74     11242\n",
      "   macro avg       0.74      0.72      0.73     11242\n",
      "weighted avg       0.75      0.74      0.74     11242\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.67      0.73      2025\n",
      "         1.0       0.57      0.51      0.54      2294\n",
      "         2.0       0.72      0.80      0.76      2566\n",
      "         3.0       0.91      0.90      0.90      2860\n",
      "         4.0       0.80      0.89      0.85      2982\n",
      "\n",
      "    accuracy                           0.77     12727\n",
      "   macro avg       0.76      0.75      0.76     12727\n",
      "weighted avg       0.77      0.77      0.77     12727\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.65      0.71      1445\n",
      "         1.0       0.56      0.46      0.50      2066\n",
      "         2.0       0.74      0.76      0.75      2516\n",
      "         3.0       0.89      0.87      0.88      2722\n",
      "         4.0       0.73      0.89      0.80      2802\n",
      "\n",
      "    accuracy                           0.75     11551\n",
      "   macro avg       0.74      0.73      0.73     11551\n",
      "weighted avg       0.75      0.75      0.74     11551\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.74      0.76      1777\n",
      "         1.0       0.66      0.45      0.53      2408\n",
      "         2.0       0.78      0.75      0.76      2787\n",
      "         3.0       0.90      0.92      0.91      2890\n",
      "         4.0       0.71      0.93      0.80      2865\n",
      "\n",
      "    accuracy                           0.77     12727\n",
      "   macro avg       0.77      0.76      0.75     12727\n",
      "weighted avg       0.77      0.77      0.76     12727\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.73      0.77      2229\n",
      "         1.0       0.61      0.52      0.56      2088\n",
      "         2.0       0.78      0.77      0.77      2603\n",
      "         3.0       0.89      0.94      0.91      2681\n",
      "         4.0       0.78      0.88      0.82      2963\n",
      "\n",
      "    accuracy                           0.78     12564\n",
      "   macro avg       0.77      0.77      0.77     12564\n",
      "weighted avg       0.78      0.78      0.78     12564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.58      0.67      1570\n",
      "         1.0       0.64      0.63      0.63      1796\n",
      "         2.0       0.71      0.79      0.75      2605\n",
      "         3.0       0.89      0.82      0.85      2858\n",
      "         4.0       0.77      0.88      0.82      2693\n",
      "\n",
      "    accuracy                           0.76     11522\n",
      "   macro avg       0.76      0.74      0.74     11522\n",
      "weighted avg       0.77      0.76      0.76     11522\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.69      0.73      1973\n",
      "         1.0       0.63      0.60      0.62      2147\n",
      "         2.0       0.76      0.69      0.73      2485\n",
      "         3.0       0.81      0.85      0.83      2577\n",
      "         4.0       0.77      0.88      0.82      2760\n",
      "\n",
      "    accuracy                           0.75     11942\n",
      "   macro avg       0.75      0.74      0.74     11942\n",
      "weighted avg       0.75      0.75      0.75     11942\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.66      0.71      2112\n",
      "         1.0       0.61      0.44      0.51      2533\n",
      "         2.0       0.74      0.78      0.76      2891\n",
      "         3.0       0.88      0.92      0.90      3004\n",
      "         4.0       0.74      0.90      0.81      3152\n",
      "\n",
      "    accuracy                           0.76     13692\n",
      "   macro avg       0.75      0.74      0.74     13692\n",
      "weighted avg       0.75      0.76      0.75     13692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write results to a file \n",
    "# textfile = open(\"report_arr hyperparam feat_num.txt\", \"w\")\n",
    "# for element in report_arr:\n",
    "#     textfile.write(element + \",\\n\")\n",
    "# textfile.close()\n",
    "\n",
    "# load results from that file\n",
    "report_arr = open(\"report_arr hyperparam feat_num.txt\")\n",
    "report_arr = ('').join(report_arr.readlines()).split(',\\n')[:-1]\n",
    "\n",
    "for el in report_arr:\n",
    "    print(el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to a file \n",
    "# np.savetxt(\n",
    "#     \"confmat_arr hyperparam feat_num.csv\",\n",
    "#     np.array(confmat_arr).reshape((3, -1)),\n",
    "#     delimiter=\",\",\n",
    "# )\n",
    "\n",
    "# load results from that file\n",
    "confmat_arr = np.loadtxt(\"confmat_arr hyperparam feat_num.csv\", delimiter=',')\n",
    "confmat_arr = confmat_arr.reshape(15,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_i: [precision  recall   f1-score   support]\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[0].split(\"   \")\n",
    "class_0 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,15):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[0].split(\"   \")\n",
    "    class_0 = np.vstack([class_0,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_0 = class_0.astype('float')\n",
    "class_0 = class_0*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[1].split(\"   \")\n",
    "class_1 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,15):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[1].split(\"   \")\n",
    "    class_1 = np.vstack([class_1,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_1 = class_1[:,1:]\n",
    "class_1 = class_1.astype('float')\n",
    "class_1 = class_1*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[2].split(\"   \")\n",
    "class_2 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,15):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[2].split(\"   \")\n",
    "    class_2 = np.vstack([class_2,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_2 = class_2[:,1:]\n",
    "class_2 = class_2.astype('float')\n",
    "class_2 = class_2*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[3].split(\"   \")\n",
    "class_3 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,15):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[3].split(\"   \")\n",
    "    class_3 = np.vstack([class_3,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_3 = class_3[:,1:]\n",
    "class_3 = class_3.astype('float')\n",
    "class_3 = class_3*100\n",
    "\n",
    "x = report_arr[0].split('0.0')[1].split(\"\\n\")[4].split(\"   \")\n",
    "class_4 = np.array([i.strip() for i in list(filter(None, x))])\n",
    "for j in range(1,15):\n",
    "    x = report_arr[j].split('0.0')[1].split(\"\\n\")[4].split(\"   \")\n",
    "    class_4 = np.vstack([class_4,np.array([i.strip() for i in list(filter(None, x))])])\n",
    "class_4 = class_4[:,1:]\n",
    "class_4 = class_4.astype('float')\n",
    "class_4 = class_4*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 6, 11, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximums = [\n",
    "    np.argmax(class_0[:, 0] + class_0[:, 1] + class_0[:, 2]),\n",
    "    np.argmax(class_1[:, 0] + class_1[:, 1] + class_1[:, 2]),\n",
    "    np.argmax(class_2[:, 0] + class_2[:, 1] + class_2[:, 2]),\n",
    "    np.argmax(class_3[:, 0] + class_3[:, 1] + class_3[:, 2]),\n",
    "    np.argmax(class_4[:, 0] + class_4[:, 1] + class_4[:, 2]),\n",
    "]\n",
    "\n",
    "maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(10, 5), sharex=True)\n",
    "ylabel = [\"Precision\", \"Recall\", \"F1-score\"]\n",
    "for i in range(3):\n",
    "    ax[i].plot(n_feat_arr, class_0[:, i], label=\"Wake\", color=\"tomato\")\n",
    "    ax[i].plot(n_feat_arr, class_0[:, i], \"o\", color=\"red\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[0]], color=\"tomato\", linestyle='--', linewidth=6, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_1[:, i], label=\"N1\", color=\"gold\")\n",
    "    ax[i].plot(n_feat_arr, class_1[:, i], \"o\", color=\"goldenrod\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[1]], color=\"gold\", linestyle='--', linewidth=4, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_2[:, i], label=\"N2\", color=\"limegreen\")\n",
    "    ax[i].plot(n_feat_arr, class_2[:, i], \"o\", color=\"olivedrab\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[2]], color=\"limegreen\", linestyle='--', linewidth=3, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_3[:, i], label=\"N3\", color=\"dodgerblue\")\n",
    "    ax[i].plot(n_feat_arr, class_3[:, i], \"o\", color=\"royalblue\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[3]], color=\"dodgerblue\", linestyle='--', linewidth=4, alpha=0.3)\n",
    "    ax[i].plot(n_feat_arr, class_4[:, i], label=\"REM\", color=\"mediumslateblue\")\n",
    "    ax[i].plot(n_feat_arr, class_4[:, i], \"o\", color=\"darkviolet\")\n",
    "    ax[i].axvline(x=n_feat_arr[maximums[4]],color=\"mediumslateblue\", linestyle='--', linewidth=4, alpha=0.3)\n",
    "    ax[i].set(ylim=[25, 100], xticks=n_feat_arr)\n",
    "    ax[i].grid(alpha=0.4)\n",
    "    ax[i].set(ylabel=ylabel[i])\n",
    "\n",
    "plt.xlabel(\"Number of top features\")\n",
    "ax[1].legend()\n",
    "# ax[1].legend(loc=(1.01,0.05))\n",
    "ax[0].set(\n",
    "    title=\"SVM RBF C=10 performance metrics with different number of features. (train,test)=(55,6) sessions\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('svm performace metrics plot.png')\n",
    "# plt.savefig('svm performace metrics plot.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = class_0[:, 0] + class_0[:, 1] + class_0[:, 2]\n",
    "arr = np.vstack([arr, class_1[:, 0] + class_1[:, 1] + class_1[:, 2]])\n",
    "arr = np.vstack([arr, class_2[:, 0] + class_2[:, 1] + class_2[:, 2]])\n",
    "arr = np.vstack([arr, class_3[:, 0] + class_3[:, 1] + class_3[:, 2]])\n",
    "arr = np.vstack([arr, class_4[:, 0] + class_4[:, 1] + class_4[:, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[187., 225., 204., 219., 214., 215., 226., 205., 221., 213., 229.,\n",
       "        232., 206., 218., 213.],\n",
       "       [ 89., 150., 163., 141., 169., 155., 177., 169., 162., 152., 164.,\n",
       "        169., 190., 185., 156.],\n",
       "       [162., 210., 229., 228., 222., 207., 233., 227., 228., 225., 229.,\n",
       "        232., 225., 218., 228.],\n",
       "       [200., 263., 259., 268., 264., 249., 267., 258., 271., 264., 273.,\n",
       "        274., 256., 249., 270.],\n",
       "       [207., 243., 255., 245., 255., 246., 247., 235., 254., 242., 244.,\n",
       "        248., 247., 247., 245.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(10,7), sharex=True)\n",
    "ax[0].imshow(arr, cmap=\"Blues\")\n",
    "ax[0].set(\n",
    "    xticks=list(range(0, 15)),\n",
    "    xticklabels=n_feat_arr,\n",
    "    yticks=list(range(0, 5)),\n",
    "    yticklabels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"],\n",
    "    ylabel=\"Sleep Stage\",\n",
    "    title=\"Sum of precision, recall, and f1-score\"\n",
    ")\n",
    "\n",
    "array = np.sum(arr, axis=0)\n",
    "temp = array.argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(array))\n",
    "ranks = 14 - ranks\n",
    "\n",
    "ax[1].imshow(np.sum(arr, axis=0).reshape(1,-1), cmap=\"Blues\")\n",
    "ax[1].set(\n",
    "    xticks=list(range(0, 15)),\n",
    "    xticklabels=n_feat_arr,\n",
    "    yticks=list(range(0, 1)),\n",
    "    xlabel=\"Number of top features\",\n",
    "    ylabel=\"Sum of stages\",\n",
    ")\n",
    "ax[1].set_yticklabels([\"$\\Sigma$\"], fontsize=25)\n",
    "for i in range(len(ranks)):\n",
    "    if i == 0:\n",
    "        ax[1].text(i, 0, ranks[i]+1, ha=\"center\", va=\"center\", color=\"black\", fontsize=15)\n",
    "    else:\n",
    "        ax[1].text(i, 0, ranks[i]+1, ha=\"center\", va=\"center\", color=\"white\", fontsize=15)\n",
    "    \n",
    "plt.subplots_adjust(top=1.4)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"sum of metrics matrix.png\")\n",
    "# plt.savefig(\"sum of metrics matrix.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold CV with C=10, n_feat=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# pipe_lr = make_pipeline(SVC(kernel=\"rbf\", C=10))\n",
    "\n",
    "# X_train_std, X_test_std, y_train, y_test = train_test_split(test_prop=0.05, n_feat=35)\n",
    "# kfold = StratifiedKFold(n_splits=10).split(X_train_std, y_train)\n",
    "\n",
    "# scores = []\n",
    "# for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "#     pipe_lr.fit(X_train_std[train], y_train[train])\n",
    "#     score = pipe_lr.score(X_train_std[test], y_train[test])\n",
    "#     scores.append(score)\n",
    "\n",
    "#     print(\n",
    "#         f\"Fold: {k+1:02d}, Class distr.: {np.bincount(y_train[train].astype(int))}, Acc.: {score:.3f}\"\n",
    "#     )\n",
    "\n",
    "# mean_acc = np.mean(scores)\n",
    "# std_acc = np.std(scores)\n",
    "# print(f\"\\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> train recordings (index): \n",
      "[14 21 37 22  8 15 48 60 23 40 32 25  6 45 57 28 47 11 10 35 27  3 20 52\n",
      " 36 29 56 51 39  5 53 59 18  9 54 55 41  1 17 42 19 24 38 50 12 16]\n",
      ">>>>>>>> test recordings: \n",
      "[33 31  4  0 26 13  2 44  7 49 46 34 30 58 43]\n",
      "Train set: X=(90044, 60) y=(90044,)\n",
      "Test set: X=(30228, 60) y=(30228,)\n",
      "Misclassified examples: 7792\n",
      "Accuracy: 0.742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_std, X_test_std, y_train, y_test = train_test_split(test_prop=0.25, n_feat=60)\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(\"Misclassified examples: %d\" % (y_test != y_pred).sum())\n",
    "print(\"Accuracy: %.3f\" % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.67      0.70      4485\n",
      "         1.0       0.63      0.51      0.56      5326\n",
      "         2.0       0.68      0.75      0.71      6441\n",
      "         3.0       0.83      0.84      0.83      6850\n",
      "         4.0       0.80      0.85      0.82      7126\n",
      "\n",
      "    accuracy                           0.74     30228\n",
      "   macro avg       0.73      0.73      0.73     30228\n",
      "weighted avg       0.74      0.74      0.74     30228\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/x03my2dj70s46m109bn58l2m0000gn/T/ipykernel_68504/520181548.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set(\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred)\n",
    "confmat_f(confmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results in hypnogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(\n",
    "    data,\n",
    "    sf,\n",
    "    hypno=None,\n",
    "    hypno_pred=None,\n",
    "    win_sec=30,\n",
    "    fmin=0.5,\n",
    "    fmax=25,\n",
    "    trimperc=2.5,\n",
    "    cmap=\"RdBu_r\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a full-night multi-taper spectrogram, optionally with the hypnogram on top.\n",
    "    For more details, please refer to the `Jupyter notebook\n",
    "    <https://github.com/raphaelvallat/yasa/blob/master/notebooks/10_spectrogram.ipynb>`_\n",
    "    .. versionadded:: 0.1.8\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : :py:class:`numpy.ndarray`\n",
    "        Single-channel EEG data. Must be a 1D NumPy array.\n",
    "    sf : float\n",
    "        The sampling frequency of data AND the hypnogram.\n",
    "    hypno : array_like\n",
    "        Sleep stage (hypnogram), optional.\n",
    "        The hypnogram must have the exact same number of samples as ``data``.\n",
    "        To upsample your hypnogram, please refer to :py:func:`yasa.hypno_upsample_to_data`.\n",
    "        .. note::\n",
    "            The default hypnogram format in YASA is a 1D integer\n",
    "            vector where:\n",
    "            - -2 = Unscored\n",
    "            - -1 = Artefact / Movement\n",
    "            - 0 = Wake\n",
    "            - 1 = N1 sleep\n",
    "            - 2 = N2 sleep\n",
    "            - 3 = N3 sleep\n",
    "            - 4 = REM sleep\n",
    "    win_sec : int or float\n",
    "        The length of the sliding window, in seconds, used for multitaper PSD\n",
    "        calculation. Default is 30 seconds. Note that ``data`` must be at least\n",
    "        twice longer than ``win_sec`` (e.g. 60 seconds).\n",
    "    fmin, fmax : int or float\n",
    "        The lower and upper frequency of the spectrogram. Default 0.5 to 25 Hz.\n",
    "    trimperc : int or float\n",
    "        The amount of data to trim on both ends of the distribution when\n",
    "        normalizing the colormap. This parameter directly impacts the\n",
    "        contrast of the spectrogram plot (higher values = higher contrast).\n",
    "        Default is 2.5, meaning that the min and max of the colormap\n",
    "        are defined as the 2.5 and 97.5 percentiles of the spectrogram.\n",
    "    cmap : str\n",
    "        Colormap. Default to 'RdBu_r'.\n",
    "    Returns\n",
    "    -------\n",
    "    fig : :py:class:`matplotlib.figure.Figure`\n",
    "        Matplotlib Figure\n",
    "    Examples\n",
    "    --------\n",
    "    1. Full-night multitaper spectrogram on Cz, no hypnogram\n",
    "    .. plot::\n",
    "        >>> import yasa\n",
    "        >>> import numpy as np\n",
    "        >>> # In the next 5 lines, we're loading the data from GitHub.\n",
    "        >>> import requests\n",
    "        >>> from io import BytesIO\n",
    "        >>> r = requests.get('https://github.com/raphaelvallat/yasa/raw/master/notebooks/data_full_6hrs_100Hz_Cz%2BFz%2BPz.npz', stream=True)\n",
    "        >>> npz = np.load(BytesIO(r.raw.read()))\n",
    "        >>> data = npz.get('data')[0, :]\n",
    "        >>> sf = 100\n",
    "        >>> fig = yasa.plot_spectrogram(data, sf)\n",
    "    2. Full-night multitaper spectrogram on Cz with the hypnogram on top\n",
    "    .. plot::\n",
    "        >>> import yasa\n",
    "        >>> import numpy as np\n",
    "        >>> # In the next lines, we're loading the data from GitHub.\n",
    "        >>> import requests\n",
    "        >>> from io import BytesIO\n",
    "        >>> r = requests.get('https://github.com/raphaelvallat/yasa/raw/master/notebooks/data_full_6hrs_100Hz_Cz%2BFz%2BPz.npz', stream=True)\n",
    "        >>> npz = np.load(BytesIO(r.raw.read()))\n",
    "        >>> data = npz.get('data')[0, :]\n",
    "        >>> sf = 100\n",
    "        >>> # Load the 30-sec hypnogram and upsample to data\n",
    "        >>> hypno = np.loadtxt('https://raw.githubusercontent.com/raphaelvallat/yasa/master/notebooks/data_full_6hrs_100Hz_hypno_30s.txt')\n",
    "        >>> hypno = yasa.hypno_upsample_to_data(hypno, 1/30, data, sf)\n",
    "        >>> fig = yasa.plot_spectrogram(data, sf, hypno, cmap='Spectral_r')\n",
    "    \"\"\"\n",
    "    # Increase font size while preserving original\n",
    "    old_fontsize = plt.rcParams[\"font.size\"]\n",
    "    plt.rcParams.update({\"font.size\": 13})\n",
    "\n",
    "    # Safety checks\n",
    "    assert isinstance(data, np.ndarray), \"Data must be a 1D NumPy array.\"\n",
    "    assert isinstance(sf, (int, float)), \"sf must be int or float.\"\n",
    "    assert data.ndim == 1, \"Data must be a 1D (single-channel) NumPy array.\"\n",
    "    assert isinstance(win_sec, (int, float)), \"win_sec must be int or float.\"\n",
    "    assert isinstance(fmin, (int, float)), \"fmin must be int or float.\"\n",
    "    assert isinstance(fmax, (int, float)), \"fmax must be int or float.\"\n",
    "    assert fmin < fmax, \"fmin must be strictly inferior to fmax.\"\n",
    "    assert fmax < sf / 2, \"fmax must be less than Nyquist (sf / 2).\"\n",
    "\n",
    "    # Calculate multi-taper spectrogram\n",
    "    nperseg = int(win_sec * sf)\n",
    "    assert data.size > 2 * nperseg, \"Data length must be at least 2 * win_sec.\"\n",
    "    f, t, Sxx = spectrogram_lspopt(data, sf, nperseg=nperseg, noverlap=0)\n",
    "    Sxx = 10 * np.log10(Sxx)  # Convert uV^2 / Hz --> dB / Hz\n",
    "\n",
    "    # Select only relevant frequencies (up to 30 Hz)\n",
    "    good_freqs = np.logical_and(f >= fmin, f <= fmax)\n",
    "    Sxx = Sxx[good_freqs, :]\n",
    "    f = f[good_freqs]\n",
    "    t /= 3600  # Convert t to hours\n",
    "\n",
    "    # Normalization\n",
    "    vmin, vmax = np.percentile(Sxx, [0 + trimperc, 100 - trimperc])\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    if hypno is None:\n",
    "        fig, ax = plt.subplots(nrows=1, figsize=(12, 4))\n",
    "        im = ax.pcolormesh(\n",
    "            t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\"\n",
    "        )\n",
    "        ax.set_xlim(0, t.max())\n",
    "        ax.set_ylabel(\"Frequency [Hz]\")\n",
    "        ax.set_xlabel(\"Time [hrs]\")\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(im, ax=ax, shrink=0.95, fraction=0.1, aspect=25)\n",
    "        cbar.ax.set_ylabel(\"Log Power (dB / Hz)\", rotation=270, labelpad=20)\n",
    "        return fig\n",
    "    elif (hypno is not None) and (hypno_pred is None):\n",
    "        hypno = np.asarray(hypno).astype(int)\n",
    "        assert hypno.ndim == 1, \"Hypno must be 1D.\"\n",
    "        assert hypno.size == data.size, \"Hypno must have the same sf as data.\"\n",
    "        t_hyp = np.arange(hypno.size) / (sf * 3600)\n",
    "        # Make sure that REM is displayed after Wake\n",
    "        hypno = (\n",
    "            pd.Series(hypno).map({-2: -2, -1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1}).values\n",
    "        )\n",
    "        hypno_rem = np.ma.masked_not_equal(hypno, 1)\n",
    "\n",
    "        fig, (ax0, ax1) = plt.subplots(\n",
    "            nrows=2, figsize=(12, 6), gridspec_kw={\"height_ratios\": [1, 2]}\n",
    "        )\n",
    "        plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "        # Hypnogram (top axis)\n",
    "        ax0.step(t_hyp, -1 * hypno, color=\"k\")\n",
    "        ax0.step(t_hyp, -1 * hypno_rem, color=\"r\")\n",
    "        if -2 in hypno and -1 in hypno:\n",
    "            # Both Unscored and Artefacts are present\n",
    "            ax0.set_yticks([2, 1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "        elif -2 in hypno and -1 not in hypno:\n",
    "            # Only Unscored are present\n",
    "            ax0.set_yticks([2, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "\n",
    "        elif -2 not in hypno and -1 in hypno:\n",
    "            # Only Artefacts are present\n",
    "            ax0.set_yticks([1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 1.5)\n",
    "        else:\n",
    "            # No artefacts or Unscored\n",
    "            ax0.set_yticks([0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 0.5)\n",
    "        ax0.set_xlim(0, t_hyp.max())\n",
    "        ax0.set_ylabel(\"Stage\")\n",
    "        ax0.xaxis.set_visible(False)\n",
    "        ax0.spines[\"right\"].set_visible(False)\n",
    "        ax0.spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Spectrogram (bottom axis)\n",
    "        im = ax1.pcolormesh(\n",
    "            t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\"\n",
    "        )\n",
    "        ax1.set_xlim(0, t.max())\n",
    "        ax1.set_ylabel(\"Frequency [Hz]\")\n",
    "        ax1.set_xlabel(\"Time [hrs]\")\n",
    "\n",
    "        # Revert font-size\n",
    "        plt.rcParams.update({\"font.size\": old_fontsize})\n",
    "        return fig\n",
    "    elif (hypno is not None) and (hypno_pred is not None):\n",
    "        hypno_pred = np.asarray(hypno_pred).astype(int)\n",
    "        hypno = np.asarray(hypno).astype(int)\n",
    "        assert hypno.ndim == 1, \"Hypno must be 1D.\"\n",
    "        assert hypno.size == data.size, \"Hypno must have the same sf as data.\"\n",
    "        assert hypno_pred.ndim == 1, \"hypno_pred must be 1D.\"\n",
    "        assert hypno_pred.size == data.size, \"hypno_pred must have the same sf as data.\"\n",
    "        t_hyp = np.arange(hypno.size) / (sf * 3600)\n",
    "        t_hyp_pred = np.arange(hypno_pred.size) / (sf * 3600)\n",
    "        # Make sure that REM is displayed after Wake\n",
    "        hypno = (\n",
    "            pd.Series(hypno).map({-2: -2, -1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1}).values\n",
    "        )\n",
    "        hypno_pred = (\n",
    "            pd.Series(hypno_pred)\n",
    "            .map({-2: -2, -1: -1, 0: 0, 1: 2, 2: 3, 3: 4, 4: 1})\n",
    "            .values\n",
    "        )\n",
    "        hypno_rem = np.ma.masked_not_equal(hypno, 1)\n",
    "        hypno_pred_rem = np.ma.masked_not_equal(hypno_pred, 1)\n",
    "\n",
    "        fig, (ax0, ax1, ax2) = plt.subplots(\n",
    "            nrows=3, figsize=(12, 6), gridspec_kw={\"height_ratios\": [1, 1, 2]}\n",
    "        )\n",
    "        plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "        # Hypnogram (top axis)\n",
    "        ax0.step(t_hyp, -1 * hypno, color=\"k\")\n",
    "        ax0.step(t_hyp, -1 * hypno_rem, color=\"r\")\n",
    "        if -2 in hypno and -1 in hypno:\n",
    "            # Both Unscored and Artefacts are present\n",
    "            ax0.set_yticks([2, 1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "        elif -2 in hypno and -1 not in hypno:\n",
    "            # Only Unscored are present\n",
    "            ax0.set_yticks([2, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Uns\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 2.5)\n",
    "\n",
    "        elif -2 not in hypno and -1 in hypno:\n",
    "            # Only Artefacts are present\n",
    "            ax0.set_yticks([1, 0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 1.5)\n",
    "        else:\n",
    "            # No artefacts or Unscored\n",
    "            ax0.set_yticks([0, -1, -2, -3, -4])\n",
    "            ax0.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax0.set_ylim(-4.5, 0.5)\n",
    "        ax0.set_xlim(0, t_hyp.max())\n",
    "        ax0.set_ylabel(\"Stage\")\n",
    "        ax0.xaxis.set_visible(False)\n",
    "        ax0.spines[\"right\"].set_visible(False)\n",
    "        ax0.spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Hypnogram Pred (middle axis)\n",
    "        ax1.step(t_hyp_pred, -1 * hypno_pred, color=\"k\")\n",
    "        ax1.step(t_hyp_pred, -1 * hypno_pred_rem, color=\"r\")\n",
    "        if -2 in hypno_pred and -1 in hypno_pred:\n",
    "            # Both Unscored and Artefacts are present\n",
    "            ax1.set_yticks([2, 1, 0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"Uns\", \"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 2.5)\n",
    "        elif -2 in hypno_pred and -1 not in hypno_pred:\n",
    "            # Only Unscored are present\n",
    "            ax1.set_yticks([2, 0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"Uns\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 2.5)\n",
    "\n",
    "        elif -2 not in hypno_pred and -1 in hypno_pred:\n",
    "            # Only Artefacts are present\n",
    "            ax1.set_yticks([1, 0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"Art\", \"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 1.5)\n",
    "        else:\n",
    "            # No artefacts or Unscored\n",
    "            ax1.set_yticks([0, -1, -2, -3, -4])\n",
    "            ax1.set_yticklabels([\"W\", \"R\", \"N1\", \"N2\", \"N3\"])\n",
    "            ax1.set_ylim(-4.5, 0.5)\n",
    "        ax1.set_xlim(0, t_hyp_pred.max())\n",
    "        ax1.set_ylabel(\"Stage\")\n",
    "        ax1.xaxis.set_visible(False)\n",
    "        ax1.spines[\"right\"].set_visible(False)\n",
    "        ax1.spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Spectrogram (bottom axis)\n",
    "        im = ax2.pcolormesh(\n",
    "            t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\"\n",
    "        )\n",
    "        ax2.set_xlim(0, t.max())\n",
    "        ax2.set_ylabel(\"Frequency [Hz]\")\n",
    "        ax2.set_xlabel(\"Time [hrs]\")\n",
    "\n",
    "        # Revert font-size\n",
    "        plt.rcParams.update({\"font.size\": old_fontsize})\n",
    "        return fig\n",
    "\n",
    "\n",
    "def format_seconds_to_hhmmss(seconds):\n",
    "    hours = seconds // (60 * 60)\n",
    "    seconds %= 60 * 60\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%02i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "def set_log_level(verbose=None):\n",
    "    \"\"\"Convenience function for setting the logging level.\n",
    "    This function comes from the PySurfer package. See :\n",
    "    https://github.com/nipy/PySurfer/blob/master/surfer/utils.py\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool, str, int, or None\n",
    "        The verbosity of messages to print. If a str, it can be either\n",
    "        PROFILER, DEBUG, INFO, WARNING, ERROR, or CRITICAL.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"yasa\")\n",
    "    if isinstance(verbose, bool):\n",
    "        verbose = \"INFO\" if verbose else \"WARNING\"\n",
    "    if isinstance(verbose, str):\n",
    "        if verbose.upper() in LOGGING_TYPES:\n",
    "            verbose = verbose.upper()\n",
    "            verbose = LOGGING_TYPES[verbose]\n",
    "            logger.setLevel(verbose)\n",
    "        else:\n",
    "            raise ValueError(\"verbose must be in %s\" % \", \".join(LOGGING_TYPES))\n",
    "\n",
    "\n",
    "def hypno_upsample_to_data(hypno, sf_hypno, data, sf_data=None, verbose=True):\n",
    "    \"\"\"Upsample an hypnogram to a given sampling frequency and fit the\n",
    "    resulting hypnogram to corresponding EEG data, such that the hypnogram\n",
    "    and EEG data have the exact same number of samples.\n",
    "    .. versionadded:: 0.1.5\n",
    "    Parameters\n",
    "    ----------\n",
    "    hypno : array_like\n",
    "        The sleep staging (hypnogram) 1D array.\n",
    "    sf_hypno : float\n",
    "        The current sampling frequency of the hypnogram, in Hz, e.g.\n",
    "        * 1/30 = 1 value per each 30 seconds of EEG data,\n",
    "        * 1 = 1 value per second of EEG data\n",
    "    data : array_like or :py:class:`mne.io.BaseRaw`\n",
    "        1D or 2D EEG data. Can also be a :py:class:`mne.io.BaseRaw`, in which\n",
    "        case ``data`` and ``sf_data`` will be automatically extracted.\n",
    "    sf_data : float\n",
    "        The sampling frequency of ``data``, in Hz (e.g. 100 Hz, 256 Hz, ...).\n",
    "        Can be omitted if ``data`` is a :py:class:`mne.io.BaseRaw`.\n",
    "    verbose : bool or str\n",
    "        Verbose level. Default (False) will only print warning and error\n",
    "        messages. The logging levels are 'debug', 'info', 'warning', 'error',\n",
    "        and 'critical'. For most users the choice is between 'info'\n",
    "        (or ``verbose=True``) and warning (``verbose=False``).\n",
    "    Returns\n",
    "    -------\n",
    "    hypno : array_like\n",
    "        The hypnogram, upsampled to ``sf_data`` and cropped/padded to ``max(data.shape)``.\n",
    "    Warns\n",
    "    -----\n",
    "    UserWarning\n",
    "        If the upsampled ``hypno`` is shorter / longer than ``max(data.shape)``\n",
    "        and therefore needs to be padded/cropped respectively. This output can be disabled by\n",
    "        passing ``verbose='ERROR'``.\n",
    "    \"\"\"\n",
    "    set_log_level(verbose)\n",
    "    if isinstance(data, mne.io.BaseRaw):\n",
    "        sf_data = data.info[\"sfreq\"]\n",
    "        data = data.times\n",
    "\n",
    "    # Upsample the hypnogram to a given sampling frequency\n",
    "    repeats = sf_data / sf_hypno\n",
    "    assert sf_hypno <= sf_data, \"sf_hypno must be less than sf_data.\"\n",
    "    assert repeats.is_integer(), \"sf_hypno / sf_data must be a whole number.\"\n",
    "    assert isinstance(hypno, (list, np.ndarray, pd.Series))\n",
    "    hypno_up = np.repeat(np.asarray(hypno), repeats)\n",
    "\n",
    "    # Crop or pad the hypnogram to fit the length of data.\n",
    "    # Check if data is an MNE raw object\n",
    "    hypno = hypno_up\n",
    "    sf = sf_data\n",
    "    if isinstance(data, mne.io.BaseRaw):\n",
    "        sf = data.info[\"sfreq\"]\n",
    "        data = data.times  # 1D array and does not require to preload data\n",
    "    data = np.asarray(data)\n",
    "    hypno = np.asarray(hypno)\n",
    "    assert hypno.ndim == 1, \"Hypno must be 1D.\"\n",
    "    npts_hyp = hypno.size\n",
    "    npts_data = max(data.shape)  # Support for 2D data\n",
    "    if npts_hyp < npts_data:\n",
    "        # Hypnogram is shorter than data\n",
    "        npts_diff = npts_data - npts_hyp\n",
    "        if sf is not None:\n",
    "            dur_diff = npts_diff / sf\n",
    "            logger.warning(\n",
    "                \"Hypnogram is SHORTER than data by %.2f seconds. \"\n",
    "                \"Padding hypnogram with last value to match data.size.\" % dur_diff\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Hypnogram is SHORTER than data by %i samples. \"\n",
    "                \"Padding hypnogram with last value to match data.size.\" % npts_diff\n",
    "            )\n",
    "        hypno = np.pad(hypno, (0, npts_diff), mode=\"edge\")\n",
    "    elif npts_hyp > npts_data:\n",
    "        # Hypnogram is longer than data\n",
    "        npts_diff = npts_hyp - npts_data\n",
    "        if sf is not None:\n",
    "            dur_diff = npts_diff / sf\n",
    "            logger.warning(\n",
    "                \"Hypnogram is LONGER than data by %.2f seconds. \"\n",
    "                \"Cropping hypnogram to match data.size.\" % dur_diff\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Hypnogram is LONGER than data by %i samples. \"\n",
    "                \"Cropping hypnogram to match data.size.\" % npts_diff\n",
    "            )\n",
    "        hypno = hypno[0:npts_data]\n",
    "\n",
    "    return hypno\n",
    "\n",
    "\n",
    "def transition_matrix(hypno):\n",
    "    \"\"\"Create a state-transition matrix from an hypnogram.\n",
    "    .. versionadded:: 0.1.9\n",
    "    Parameters\n",
    "    ----------\n",
    "    hypno : array_like\n",
    "        Hypnogram. The dtype of ``hypno`` must be integer\n",
    "        (e.g. [0, 2, 2, 1, 1, 1, ...]). The sampling frequency must be the\n",
    "        original one, i.e. 1 value per 30 seconds if the staging was done in\n",
    "        30 seconds epochs. Using an upsampled hypnogram will result in an\n",
    "        incorrect transition matrix.\n",
    "        For best results, we recommend using an hypnogram cropped to\n",
    "        either the time in bed (TIB) or the sleep period time (SPT), without\n",
    "        any artefact / unscored epochs.\n",
    "    Returns\n",
    "    -------\n",
    "    counts : :py:class:`pandas.DataFrame`\n",
    "        Counts transition matrix (number of transitions from stage A to\n",
    "        stage B). The pre-transition states are the rows and the\n",
    "        post-transition states are the columns.\n",
    "    probs : :py:class:`pandas.DataFrame`\n",
    "        Conditional probability transition matrix, i.e.\n",
    "        given that current state is A, what is the probability that\n",
    "        the next state is B.\n",
    "        ``probs`` is a `right stochastic matrix\n",
    "        <https://en.wikipedia.org/wiki/Stochastic_matrix>`_,\n",
    "        i.e. each row sums to 1.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from yasa import transition_matrix\n",
    "    >>> a = [0, 0, 0, 1, 1, 0, 1, 2, 2, 3, 3, 2, 3, 3, 0, 2, 2, 1, 2, 2, 3, 3]\n",
    "    >>> counts, probs = transition_matrix(a)\n",
    "    >>> counts\n",
    "           0  1  2  3\n",
    "    Stage\n",
    "    0      2  2  1  0\n",
    "    1      1  1  2  0\n",
    "    2      0  1  3  3\n",
    "    3      1  0  1  3\n",
    "    >>> probs.round(2)\n",
    "              0     1     2     3\n",
    "    Stage\n",
    "    0      0.40  0.40  0.20  0.00\n",
    "    1      0.25  0.25  0.50  0.00\n",
    "    2      0.00  0.14  0.43  0.43\n",
    "    3      0.20  0.00  0.20  0.60\n",
    "    Several metrics of sleep fragmentation can be calculated from the\n",
    "    probability matrix. For example, the stability of sleep stages can be\n",
    "    calculated by taking the average of the diagonal values (excluding Wake\n",
    "    and N1 sleep):\n",
    "    >>> np.diag(probs.loc[2:, 2:]).mean().round(3)\n",
    "    0.514\n",
    "    Finally, we can plot the transition matrix using :py:func:`seaborn.heatmap`\n",
    "    .. plot::\n",
    "        >>> import numpy as np\n",
    "        >>> import seaborn as sns\n",
    "        >>> import matplotlib.pyplot as plt\n",
    "        >>> from yasa import transition_matrix\n",
    "        >>> # Calculate probability matrix\n",
    "        >>> a = [1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0]\n",
    "        >>> _, probs = transition_matrix(a)\n",
    "        >>> # Start the plot\n",
    "        >>> grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .1}\n",
    "        >>> f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws,\n",
    "        ...                                 figsize=(5, 5))\n",
    "        >>> sns.heatmap(probs, ax=ax, square=False, vmin=0, vmax=1, cbar=True,\n",
    "        ...             cbar_ax=cbar_ax, cmap='YlOrRd', annot=True, fmt='.2f',\n",
    "        ...             cbar_kws={\"orientation\": \"horizontal\", \"fraction\": 0.1,\n",
    "        ...                       \"label\": \"Transition probability\"})\n",
    "        >>> ax.set_xlabel(\"To sleep stage\")\n",
    "        >>> ax.xaxis.tick_top()\n",
    "        >>> ax.set_ylabel(\"From sleep stage\")\n",
    "        >>> ax.xaxis.set_label_position('top')\n",
    "    \"\"\"\n",
    "    x = np.asarray(hypno, dtype=int)\n",
    "    unique, inverse = np.unique(x, return_inverse=True)  # unique is sorted\n",
    "    n = unique.size\n",
    "    # Integer transition counts\n",
    "    counts = np.zeros((n, n), dtype=int)\n",
    "    np.add.at(counts, (inverse[:-1], inverse[1:]), 1)\n",
    "    # Conditional probabilities\n",
    "    probs = counts / counts.sum(axis=-1, keepdims=True)\n",
    "    # Convert to a Pandas DataFrame\n",
    "    counts = pd.DataFrame(counts, index=unique, columns=unique)\n",
    "    probs = pd.DataFrame(probs, index=unique, columns=unique)\n",
    "    counts.index.name = \"From Stage\"\n",
    "    probs.index.name = \"From Stage\"\n",
    "    counts.columns.name = \"To Stage\"\n",
    "    probs.columns.name = \"To Stage\"\n",
    "    return counts, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to load augmented hypno:\n",
    "name = reference_df.iloc[33].name\n",
    "hypno_30s_loc = reference_df.iloc[33].hypno\n",
    "hypno_30s_loc = hypno_30s_loc.split(\".\")[0] + \" aug.txt\"\n",
    "hypno_30s = np.loadtxt(hypno_30s_loc, delimiter=\"\\n\")\n",
    "\n",
    "### to load features for augmented eeg:\n",
    "df_feat_loc = reference_df.iloc[33].df_feat\n",
    "df_feat_loc = df_feat_loc.split(\".\")[0] + \" aug.csv\"\n",
    "df_feat = pd.read_csv(df_feat_loc, index_col=False)\n",
    "\n",
    "### to load augmented eeg:\n",
    "eeg_loc = reference_df.iloc[33].eeg\n",
    "eeg_loc = eeg_loc.split(\".\")[0] + \" aug.txt\"\n",
    "data = np.loadtxt(eeg_loc, delimiter=\",\")  # took ~7 seconds # this is filtered data actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1677,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypno_30s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 50310.0 (sec) OR 13:58:30\n"
     ]
    }
   ],
   "source": [
    "sf = 256\n",
    "\n",
    "print(\n",
    "    f\"Duration: {data.flatten().shape[0]/sf} (sec) OR {format_seconds_to_hhmmss(data.flatten().shape[0]/sf)}\"\n",
    ")\n",
    "\n",
    "fig = plot_spectrogram(data.flatten(), sf, fmax=45)\n",
    "plt.title(\n",
    "    f\"Spectrogram of {name} - {format_seconds_to_hhmmss(data.shape[1]/sf)}\", fontsize=16\n",
    ")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'spectro QS {folder} {LR}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "hypno = hypno_upsample_to_data(\n",
    "    hypno=hypno_30s, sf_hypno=(1 / 30), data=data.flatten(), sf_data=sf\n",
    ")\n",
    "hypno_pred = hypno_upsample_to_data(\n",
    "    hypno=y_pred[: len(hypno_30s)], sf_hypno=(1 / 30), data=data.flatten(), sf_data=sf\n",
    ")\n",
    "\n",
    "fig = plot_spectrogram(\n",
    "    data.flatten(), sf, hypno=hypno, hypno_pred=hypno_pred, fmax=30, trimperc=5\n",
    ")\n",
    "fig.suptitle(\n",
    "    f\"Spectrogram and Hypnogram of {name} - {format_seconds_to_hhmmss(data.shape[1]/sf)}\",\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'spectro-hypno QS {folder} {LR}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_std2, X_test_std2, y_train2, y_test2 = train_test_split(test_prop=0.25, n_feat=60)\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10, random_state=1)\n",
    "svm.fit(X_train_std2, y_train2)\n",
    "y_pred2 = svm.predict(X_test_std2)\n",
    "print(\"Misclassified examples: %d\" % (y_test2 != y_pred2).sum())\n",
    "print(\"Accuracy: %.3f\" % accuracy_score(y_test2, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred)\n",
    "confmat_f(confmat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mne')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9461a3eb0cbef873800a78c94f0b6f375dc71a03d464463d7b611af82b6cd16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
